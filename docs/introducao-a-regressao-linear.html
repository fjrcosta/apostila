<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 13 Introdução à Regressão Linear | apostila.knit</title>
  <meta name="description" content="Apostila com alguns tópicos de estatística e probabilidade." />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 13 Introdução à Regressão Linear | apostila.knit" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://fjrcosta.github.io/apostila/images/logo-uel.png" />
  <meta property="og:description" content="Apostila com alguns tópicos de estatística e probabilidade." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 13 Introdução à Regressão Linear | apostila.knit" />
  
  <meta name="twitter:description" content="Apostila com alguns tópicos de estatística e probabilidade." />
  <meta name="twitter:image" content="https://fjrcosta.github.io/apostila/images/logo-uel.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introducao-a-correlacao-linear-de-pearson.html"/>
<link rel="next" href="introducao-a-modelagem-dados.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Índice</a></li>

<li class="divider"></li>
<li><a href="index.html#section" id="toc-section"></a></li>
<li class="chapter" data-level="1" data-path="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html"><a href="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html"><i class="fa fa-check"></i><b>1</b> Introdução histórica daquilo que veio a se chamar estatística</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html"><a href="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html#filosofia-da-ciencia-teoria-do-conhecimento-epistemologia"><i class="fa fa-check"></i><b>1.1</b> Filosofia da ciencia (teoria do conhecimento, epistemologia)</a></li>
<li class="chapter" data-level="1.2" data-path="13-regressao_linear.html"><a href="#diferentes-usos-relacionados-ao-termo-primeiros-levantamentos-estudos-e-publica%C3%A7%C3%B5es-o-passado-distante"><i class="fa fa-check"></i><b>1.2</b> Diferentes usos relacionados ao termo, primeiros levantamentos, estudos e publicações (o passado distante)</a></li>
<li class="chapter" data-level="1.3" data-path="13-regressao_linear.html"><a href="#visualiza%C3%A7%C3%A3o-de-dados-estudos-e-primeiras-publica%C3%A7%C3%B5es"><i class="fa fa-check"></i><b>1.3</b> Visualização de dados &amp; Estudos e primeiras publicações</a></li>
<li class="chapter" data-level="1.4" data-path="13-regressao_linear.html"><a href="#pesquisadores-cuja-contribui%C3%A7%C3%A3o-foi-fundamental-na-%C3%A1rea"><i class="fa fa-check"></i><b>1.4</b> Pesquisadores cuja contribuição foi fundamental na área</a></li>
<li class="chapter" data-level="1.5" data-path="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html"><a href="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html#revista-biometrika"><i class="fa fa-check"></i><b>1.5</b> Revista Biometrika</a></li>
<li class="chapter" data-level="1.6" data-path="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html"><a href="introducao-historica-daquilo-que-veio-a-se-chamar-estatistica.html#eugenia"><i class="fa fa-check"></i><b>1.6</b> Eugenia</a></li>
<li class="chapter" data-level="1.7" data-path="13-regressao_linear.html"><a href="#estat%C3%ADstica-e-machine-learning-uma-livre-tradu%C3%A7%C3%A3o-deste-link"><i class="fa fa-check"></i><b>1.7</b> Estatística e <em>machine learning</em> : uma livre tradução deste link</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introducao-conceitual-essencial.html"><a href="introducao-conceitual-essencial.html"><i class="fa fa-check"></i><b>2</b> Introdução conceitual essencial</a>
<ul>
<li class="chapter" data-level="2.1" data-path="13-regressao_linear.html"><a href="#estat%C3%ADstica-descritiva"><i class="fa fa-check"></i><b>2.1</b> Estatística descritiva</a></li>
<li class="chapter" data-level="2.2" data-path="13-regressao_linear.html"><a href="#estat%C3%ADstica-inferencial"><i class="fa fa-check"></i><b>2.2</b> Estatística inferencial</a></li>
<li class="chapter" data-level="2.3" data-path="13-regressao_linear.html"><a href="#produ%C3%A7%C3%A3o-de-conhecimento"><i class="fa fa-check"></i><b>2.3</b> Produção de conhecimento</a></li>
<li class="chapter" data-level="2.4" data-path="13-regressao_linear.html"><a href="#popula%C3%A7%C3%A3o-universo-amostra"><i class="fa fa-check"></i><b>2.4</b> População (universo) &amp; amostra</a></li>
<li class="chapter" data-level="2.5" data-path="13-regressao_linear.html"><a href="#par%C3%A2metros-e-estat%C3%ADsticas"><i class="fa fa-check"></i><b>2.5</b> Parâmetros e estatísticas</a></li>
<li class="chapter" data-level="2.6" data-path="13-regressao_linear.html"><a href="#tipos-de-vari%C3%A1veis"><i class="fa fa-check"></i><b>2.6</b> Tipos de variáveis</a></li>
<li class="chapter" data-level="2.7" data-path="13-regressao_linear.html"><a href="#indexa%C3%A7%C3%A3o-de-dados-i"><i class="fa fa-check"></i><b>2.7</b> Indexação de dados (<span class="math inline">\(i\)</span>)</a></li>
<li class="chapter" data-level="2.8" data-path="13-regressao_linear.html"><a href="#no%C3%A7%C3%B5es-b%C3%A1sicas-sobre-somat%C3%B3rios-sigma"><i class="fa fa-check"></i><b>2.8</b> Noções básicas sobre somatórios (<span class="math inline">\(\Sigma\)</span>)</a></li>
<li class="chapter" data-level="2.9" data-path="13-regressao_linear.html"><a href="#an%C3%A1lise-combinat%C3%B3ria-m%C3%A9todos-de-enumera%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>2.9</b> Análise combinatória (métodos de enumeração)</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="13-regressao_linear.html"><a href="#princ%C3%ADpio-b%C3%A1sico-da-contagem-regra-da-multiplica%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>2.9.1</b> Princípio básico da contagem (regra da multiplicação)</a></li>
<li class="chapter" data-level="2.9.2" data-path="13-regressao_linear.html"><a href="#regra-da-adi%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>2.9.2</b> Regra da adição</a></li>
<li class="chapter" data-level="2.9.3" data-path="13-regressao_linear.html"><a href="#permuta%C3%A7%C3%B5es-ordena%C3%A7%C3%A3o-de-elementos"><i class="fa fa-check"></i><b>2.9.3</b> Permutações (ordenação de elementos)</a></li>
<li class="chapter" data-level="2.9.4" data-path="13-regressao_linear.html"><a href="#arranjos-sem-repeti%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>2.9.4</b> Arranjos sem repetição</a></li>
<li class="chapter" data-level="2.9.5" data-path="13-regressao_linear.html"><a href="#arranjos-com-repeti%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>2.9.5</b> Arranjos com repetição</a></li>
<li class="chapter" data-level="2.9.6" data-path="13-regressao_linear.html"><a href="#combina%C3%A7%C3%B5es-sem-repeti%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>2.9.6</b> Combinações sem repetição</a></li>
<li class="chapter" data-level="2.9.7" data-path="13-regressao_linear.html"><a href="#combina%C3%A7%C3%B5es-com-repeti%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>2.9.7</b> Combinações com Repetição</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="introducao-conceitual-essencial.html"><a href="introducao-conceitual-essencial.html#fatoriais"><i class="fa fa-check"></i><b>2.10</b> Fatoriais</a></li>
<li class="chapter" data-level="2.11" data-path="13-regressao_linear.html"><a href="#conectivos-l%C3%B3gicos"><i class="fa fa-check"></i><b>2.11</b> Conectivos lógicos</a></li>
<li class="chapter" data-level="2.12" data-path="introducao-conceitual-essencial.html"><a href="introducao-conceitual-essencial.html#leis-de-de-morgan"><i class="fa fa-check"></i><b>2.12</b> Leis de De Morgan</a></li>
<li class="chapter" data-level="2.13" data-path="13-regressao_linear.html"><a href="#no%C3%A7%C3%B5es-b%C3%A1sicas-para-o-uso-de-calculadora-cassio-fx-82ms"><i class="fa fa-check"></i><b>2.13</b> Noções básicas para o uso de calculadora (Cassio fx-82MS)</a></li>
<li class="chapter" data-level="2.14" data-path="13-regressao_linear.html"><a href="#instala%C3%A7%C3%A3o-do-software-r-em-conjunto-com-a-interface-gr%C3%A1fica-rstudio"><i class="fa fa-check"></i><b>2.14</b> Instalação do software R em conjunto com a interface gráfica RStudio</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="introducao-conceitual-essencial.html"><a href="introducao-conceitual-essencial.html#rstudio"><i class="fa fa-check"></i><b>2.14.1</b> RStudio</a></li>
<li class="chapter" data-level="2.14.2" data-path="introducao-conceitual-essencial.html"><a href="introducao-conceitual-essencial.html#pacotes"><i class="fa fa-check"></i><b>2.14.2</b> Pacotes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html"><i class="fa fa-check"></i><b>3</b> Introdução à estatística descritiva</a>
<ul>
<li class="chapter" data-level="3.1" data-path="13-regressao_linear.html"><a href="#an%C3%A1lise-explorat%C3%B3ria"><i class="fa fa-check"></i><b>3.1</b> Análise exploratória</a></li>
<li class="chapter" data-level="3.2" data-path="13-regressao_linear.html"><a href="#dados-brutos-em-rol-diagrama-de-ramos-folhas-e-de-dispers%C3%A3o-unidimensional"><i class="fa fa-check"></i><b>3.2</b> Dados brutos, em rol, diagrama de ramos &amp; folhas e de dispersão unidimensional</a></li>
<li class="chapter" data-level="3.3" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#apresentacao-descritiva-de-dados-na-forma-de-resumos-numericos"><i class="fa fa-check"></i><b>3.3</b> Apresentacao descritiva de dados na forma de resumos numericos</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#estimadores"><i class="fa fa-check"></i><b>3.3.1</b> Estimadores</a></li>
<li class="chapter" data-level="3.3.2" data-path="13-regressao_linear.html"><a href="#medidas-de-tend%C3%AAncia-central-posi%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>3.3.2</b> Medidas de tendência central (posição)</a></li>
<li class="chapter" data-level="3.3.3" data-path="13-regressao_linear.html"><a href="#medidas-de-dispers%C3%A3o-variabilidade"><i class="fa fa-check"></i><b>3.3.3</b> Medidas de dispersão (variabilidade)</a></li>
<li class="chapter" data-level="3.3.4" data-path="13-regressao_linear.html"><a href="#medidas-de-subdivis%C3%A3o-separatrizes"><i class="fa fa-check"></i><b>3.3.4</b> Medidas de subdivisão (separatrizes)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#padronizacao-de-dados-z-scores"><i class="fa fa-check"></i><b>3.4</b> Padronizacao de dados (<em>z-scores</em>)</a></li>
<li class="chapter" data-level="3.5" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#medidas-de-forma-assimetria-curtose"><i class="fa fa-check"></i><b>3.5</b> Medidas de forma (assimetria &amp; curtose)</a></li>
<li class="chapter" data-level="3.6" data-path="13-regressao_linear.html"><a href="#diferentes-posi%C3%A7%C3%B5es-da-m%C3%A9dia-moda-e-mediana-2o-quartil"><i class="fa fa-check"></i><b>3.6</b> Diferentes posições da média, moda e mediana (2<span class="math inline">\(^{o}\)</span> quartil)</a></li>
<li class="chapter" data-level="3.7" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#apresentacao-descritiva-de-dados-na-forma-tabular"><i class="fa fa-check"></i><b>3.7</b> Apresentacao descritiva de dados na forma tabular</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#apresentacao-tabular-de-dados-qualitativos"><i class="fa fa-check"></i><b>3.7.1</b> Apresentacao tabular de dados qualitativos</a></li>
<li class="chapter" data-level="3.7.2" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#apresentacao-tabular-de-dados-quantitativos"><i class="fa fa-check"></i><b>3.7.2</b> Apresentacao tabular de dados quantitativos</a></li>
<li class="chapter" data-level="3.7.3" data-path="13-regressao_linear.html"><a href="#m%C3%A9dia-1"><i class="fa fa-check"></i><b>3.7.3</b> Média</a></li>
<li class="chapter" data-level="3.7.4" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#moda-1"><i class="fa fa-check"></i><b>3.7.4</b> Moda</a></li>
<li class="chapter" data-level="3.7.5" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#mediana-q_2d_5p_50"><i class="fa fa-check"></i><b>3.7.5</b> Mediana (<span class="math inline">\(=Q_{2}=D_{5}=P_{50}\)</span>)</a></li>
<li class="chapter" data-level="3.7.6" data-path="13-regressao_linear.html"><a href="#vari%C3%A2ncia"><i class="fa fa-check"></i><b>3.7.6</b> Variância</a></li>
<li class="chapter" data-level="3.7.7" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#quartis"><i class="fa fa-check"></i><b>3.7.7</b> Quartis</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="introducao-a-estatistica-descritiva.html"><a href="introducao-a-estatistica-descritiva.html#apresentacao-descritiva-de-dados-na-forma-grafica"><i class="fa fa-check"></i><b>3.8</b> Apresentacao descritiva de dados na forma grafica</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="13-regressao_linear.html"><a href="#gr%C3%A1ficos-para-uma-vari%C3%A1vel-qualitativa"><i class="fa fa-check"></i><b>3.8.1</b> Gráficos para uma variável qualitativa</a></li>
<li class="chapter" data-level="3.8.2" data-path="13-regressao_linear.html"><a href="#gr%C3%A1ficos-para-uma-vari%C3%A1vel-quantitativa"><i class="fa fa-check"></i><b>3.8.2</b> Gráficos para uma variável quantitativa</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html"><i class="fa fa-check"></i><b>4</b> Introdução ao cálculo de probabilidades</a>
<ul>
<li class="chapter" data-level="4.1" data-path="13-regressao_linear.html"><a href="#introdu%C3%A7%C3%A3o-hist%C3%B3rica"><i class="fa fa-check"></i><b>4.1</b> Introdução histórica</a></li>
<li class="chapter" data-level="4.2" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#conceitos-essenciais"><i class="fa fa-check"></i><b>4.2</b> Conceitos essenciais</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="13-regressao_linear.html"><a href="#experimentos-determin%C3%ADsticos-e-experimentos-aleat%C3%B3rios"><i class="fa fa-check"></i><b>4.2.1</b> Experimentos determinísticos e experimentos aleatórios</a></li>
<li class="chapter" data-level="4.2.2" data-path="13-regressao_linear.html"><a href="#o-espa%C3%A7o-amostral"><i class="fa fa-check"></i><b>4.2.2</b> O espaço amostral</a></li>
<li class="chapter" data-level="4.2.3" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#evento"><i class="fa fa-check"></i><b>4.2.3</b> Evento</a></li>
<li class="chapter" data-level="4.2.4" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#probabilidade"><i class="fa fa-check"></i><b>4.2.4</b> Probabilidade</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="13-regressao_linear.html"><a href="#probabilidade-da-uni%C3%A3o-de-eventos"><i class="fa fa-check"></i><b>4.3</b> Probabilidade da união de eventos</a></li>
<li class="chapter" data-level="4.4" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#probabilidade-de-eventos-condicionados"><i class="fa fa-check"></i><b>4.4</b> Probabilidade de eventos condicionados</a></li>
<li class="chapter" data-level="4.5" data-path="13-regressao_linear.html"><a href="#depend%C3%AAncia-e-independ%C3%AAncia-de-eventos"><i class="fa fa-check"></i><b>4.5</b> Dependência e independência de eventos</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="13-regressao_linear.html"><a href="#demonstra%C3%A7%C3%A3o-cl%C3%A1ssica-de-independ%C3%AAncia"><i class="fa fa-check"></i><b>4.5.1</b> Demonstração clássica de independência</a></li>
<li class="chapter" data-level="4.5.2" data-path="13-regressao_linear.html"><a href="#demonstra%C3%A7%C3%A3o-cl%C3%A1ssica-de-depend%C3%AAncia"><i class="fa fa-check"></i><b>4.5.2</b> Demonstração clássica de dependência</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#probabilidade-de-eventos-independentes-regra-da-cadeia"><i class="fa fa-check"></i><b>4.6</b> Probabilidade de eventos independentes (regra da cadeia)</a></li>
<li class="chapter" data-level="4.7" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-de-bayes"><i class="fa fa-check"></i><b>4.7</b> Teorema de Bayes</a></li>
<li class="chapter" data-level="4.8" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teoremas-da-teoria-das-probabilidades"><i class="fa fa-check"></i><b>4.8</b> Teoremas da Teoria das probabilidades</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-01"><i class="fa fa-check"></i><b>4.8.1</b> Teorema 01</a></li>
<li class="chapter" data-level="4.8.2" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-02"><i class="fa fa-check"></i><b>4.8.2</b> Teorema 02</a></li>
<li class="chapter" data-level="4.8.3" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-03"><i class="fa fa-check"></i><b>4.8.3</b> Teorema 03</a></li>
<li class="chapter" data-level="4.8.4" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-04"><i class="fa fa-check"></i><b>4.8.4</b> Teorema 04</a></li>
<li class="chapter" data-level="4.8.5" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-05"><i class="fa fa-check"></i><b>4.8.5</b> Teorema 05</a></li>
<li class="chapter" data-level="4.8.6" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-06"><i class="fa fa-check"></i><b>4.8.6</b> Teorema 06</a></li>
<li class="chapter" data-level="4.8.7" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-07"><i class="fa fa-check"></i><b>4.8.7</b> Teorema 07</a></li>
<li class="chapter" data-level="4.8.8" data-path="introducao-ao-calculo-de-probabilidades.html"><a href="introducao-ao-calculo-de-probabilidades.html#teorema-08"><i class="fa fa-check"></i><b>4.8.8</b> Teorema 08</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introducao-a-variaveis-aleatorias.html"><a href="introducao-a-variaveis-aleatorias.html"><i class="fa fa-check"></i><b>5</b> Introdução a variáveis aleatórias</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introducao-a-variaveis-aleatorias.html"><a href="introducao-a-variaveis-aleatorias.html#variaveis-aleatorias-discretas-continuas"><i class="fa fa-check"></i><b>5.1</b> Variáveis aleatórias discretas e contínuas</a></li>
<li class="chapter" data-level="5.2" data-path="13-regressao_linear.html"><a href="#fun%C3%A7%C3%A3o-massa-de-probabilidade-probability-mass-function---pmf"><i class="fa fa-check"></i><b>5.2</b> Função massa de probabilidade (<em>Probability Mass Function - PMF</em>)</a></li>
<li class="chapter" data-level="5.3" data-path="13-regressao_linear.html"><a href="#fun%C3%A7%C3%A3o-cumulativa-de-probabilidade"><i class="fa fa-check"></i><b>5.3</b> Função cumulativa de probabilidade</a></li>
<li class="chapter" data-level="5.4" data-path="13-regressao_linear.html"><a href="#fun%C3%A7%C3%A3o-de-densidade-de-probabilidade-probability-density-function---pdf"><i class="fa fa-check"></i><b>5.4</b> Função de densidade de probabilidade (<em>Probability Density Function - PDF</em>)</a></li>
<li class="chapter" data-level="5.5" data-path="13-regressao_linear.html"><a href="#esperan%C3%A7a-e-vari%C3%A2ncia-de-uma-vari%C3%A1vel-aleat%C3%B3ria-discreta"><i class="fa fa-check"></i><b>5.5</b> Esperança e variância de uma variável aleatória discreta</a></li>
<li class="chapter" data-level="5.6" data-path="13-regressao_linear.html"><a href="#esperan%C3%A7a-e-vari%C3%A2ncia-de-uma-vari%C3%A1vel-aleat%C3%B3ria-cont%C3%ADnua"><i class="fa fa-check"></i><b>5.6</b> Esperança e variância de uma variável aleatória contínua</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html"><i class="fa fa-check"></i><b>6</b> Introdução a modelos teóricos de probabilidade</a>
<ul>
<li class="chapter" data-level="6.1" data-path="13-regressao_linear.html"><a href="#modelos-te%C3%B3ricos-de-probabilidade-para-vari%C3%A1veis-aleat%C3%B3rias-discretas"><i class="fa fa-check"></i><b>6.1</b> Modelos teóricos de probabilidade para variáveis aleatórias discretas</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#uniforme"><i class="fa fa-check"></i><b>6.1.1</b> Uniforme</a></li>
<li class="chapter" data-level="6.1.2" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#bernoulli"><i class="fa fa-check"></i><b>6.1.2</b> Bernoulli</a></li>
<li class="chapter" data-level="6.1.3" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#binomial"><i class="fa fa-check"></i><b>6.1.3</b> Binomial</a></li>
<li class="chapter" data-level="6.1.4" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#poisson"><i class="fa fa-check"></i><b>6.1.4</b> Poisson</a></li>
<li class="chapter" data-level="6.1.5" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#multinomial"><i class="fa fa-check"></i><b>6.1.5</b> Multinomial</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="13-regressao_linear.html"><a href="#modelos-t%C3%A9oricos-do-tempo-de-espera"><i class="fa fa-check"></i><b>6.2</b> Modelos téoricos do tempo de espera</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="13-regressao_linear.html"><a href="#geom%C3%A9trica"><i class="fa fa-check"></i><b>6.2.1</b> Geométrica</a></li>
<li class="chapter" data-level="6.2.2" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#binomial-negativa"><i class="fa fa-check"></i><b>6.2.2</b> Binomial Negativa</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="13-regressao_linear.html"><a href="#modelos-te%C3%B3ricos-de-probabilidade-para-vari%C3%A1veis-aleat%C3%B3rias-cont%C3%ADnuas"><i class="fa fa-check"></i><b>6.3</b> Modelos teóricos de probabilidade para variáveis aleatórias contínuas</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#uniforme-1"><i class="fa fa-check"></i><b>6.3.1</b> Uniforme</a></li>
<li class="chapter" data-level="6.3.2" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#exponencial"><i class="fa fa-check"></i><b>6.3.2</b> Exponencial</a></li>
<li class="chapter" data-level="6.3.3" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#normal"><i class="fa fa-check"></i><b>6.3.3</b> Normal</a></li>
<li class="chapter" data-level="6.3.4" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#student-t"><i class="fa fa-check"></i><b>6.3.4</b> Student “t”</a></li>
<li class="chapter" data-level="6.3.5" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#qui-quadrado"><i class="fa fa-check"></i><b>6.3.5</b> Qui-Quadrado</a></li>
<li class="chapter" data-level="6.3.6" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#fisher-snedecor-f"><i class="fa fa-check"></i><b>6.3.6</b> Fisher-Snedecor “F”</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introducao-a-modelos-teoricos-de-probabilidade.html"><a href="introducao-a-modelos-teoricos-de-probabilidade.html#tabelas"><i class="fa fa-check"></i><b>6.4</b> Tabelas</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html"><i class="fa fa-check"></i><b>7</b> Introdução ao planejamento de pesquisas</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#planejamento-de-pesquisas"><i class="fa fa-check"></i><b>7.1</b> Planejamento de pesquisas</a></li>
<li class="chapter" data-level="7.2" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#tipos-de-pesquisas"><i class="fa fa-check"></i><b>7.2</b> Tipos de pesquisas</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="13-regressao_linear.html"><a href="#quanto-%C3%A0-finalidade"><i class="fa fa-check"></i><b>7.2.1</b> Quanto à finalidade</a></li>
<li class="chapter" data-level="7.2.2" data-path="13-regressao_linear.html"><a href="#quanto-%C3%A0-forma-de-abordagem"><i class="fa fa-check"></i><b>7.2.2</b> Quanto à forma de abordagem</a></li>
<li class="chapter" data-level="7.2.3" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#quanto-aos-objetivos"><i class="fa fa-check"></i><b>7.2.3</b> Quanto aos objetivos</a></li>
<li class="chapter" data-level="7.2.4" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#quanto-ao-desenvolvimento-no-tempo"><i class="fa fa-check"></i><b>7.2.4</b> Quanto ao desenvolvimento no tempo</a></li>
<li class="chapter" data-level="7.2.5" data-path="13-regressao_linear.html"><a href="#quanto-%C3%A0-natureza"><i class="fa fa-check"></i><b>7.2.5</b> Quanto à natureza</a></li>
<li class="chapter" data-level="7.2.6" data-path="13-regressao_linear.html"><a href="#quanto-%C3%A0-forma-de-obten%C3%A7%C3%A3o-dos-dados"><i class="fa fa-check"></i><b>7.2.6</b> Quanto à forma de obtenção dos dados</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#principais-etapas-de-uma-pesquisa"><i class="fa fa-check"></i><b>7.3</b> Principais etapas de uma pesquisa:</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#objetivo"><i class="fa fa-check"></i><b>7.3.1</b> Objetivo</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="13-regressao_linear.html"><a href="#popula%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>7.4</b> População</a></li>
<li class="chapter" data-level="7.5" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#censo"><i class="fa fa-check"></i><b>7.5</b> Censo</a></li>
<li class="chapter" data-level="7.6" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#amostra"><i class="fa fa-check"></i><b>7.6</b> Amostra</a></li>
<li class="chapter" data-level="7.7" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#planejamento-do-levantamento-amostral"><i class="fa fa-check"></i><b>7.7</b> Planejamento do levantamento amostral</a></li>
<li class="chapter" data-level="7.8" data-path="13-regressao_linear.html"><a href="#elabora%C3%A7%C3%A3o-dos-question%C3%A1rios"><i class="fa fa-check"></i><b>7.8</b> Elaboração dos questionários</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#tipos-de-perguntas"><i class="fa fa-check"></i><b>7.8.1</b> Tipos de perguntas:</a></li>
<li class="chapter" data-level="7.8.2" data-path="13-regressao_linear.html"><a href="#execu%C3%A7%C3%A3o-do-levantamento-amostral"><i class="fa fa-check"></i><b>7.8.2</b> Execução do levantamento amostral</a></li>
<li class="chapter" data-level="7.8.3" data-path="13-regressao_linear.html"><a href="#an%C3%A1lise-explorat%C3%B3ria-dos-dados"><i class="fa fa-check"></i><b>7.8.3</b> Análise exploratória dos dados</a></li>
<li class="chapter" data-level="7.8.4" data-path="13-regressao_linear.html"><a href="#resultados-e-conclus%C3%B5es"><i class="fa fa-check"></i><b>7.8.4</b> Resultados e conclusões</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="13-regressao_linear.html"><a href="#t%C3%A9cnicas-de-amostragem"><i class="fa fa-check"></i><b>7.9</b> Técnicas de amostragem</a></li>
<li class="chapter" data-level="7.10" data-path="13-regressao_linear.html"><a href="#amostragem-probabil%C3%ADstica"><i class="fa fa-check"></i><b>7.10</b> Amostragem probabilística</a>
<ul>
<li class="chapter" data-level="7.10.1" data-path="13-regressao_linear.html"><a href="#amostragem-aleat%C3%B3ria-simples-aas"><i class="fa fa-check"></i><b>7.10.1</b> Amostragem aleatória simples (AAS)</a></li>
<li class="chapter" data-level="7.10.2" data-path="13-regressao_linear.html"><a href="#amostragem-aleat%C3%B3ria-sistem%C3%A1tica"><i class="fa fa-check"></i><b>7.10.2</b> Amostragem aleatória sistemática</a></li>
<li class="chapter" data-level="7.10.3" data-path="13-regressao_linear.html"><a href="#amostragem-aleat%C3%B3ria-estratificada"><i class="fa fa-check"></i><b>7.10.3</b> Amostragem aleatória estratificada</a></li>
<li class="chapter" data-level="7.10.4" data-path="13-regressao_linear.html"><a href="#amostragem-aleat%C3%B3ria-por-conglomerados"><i class="fa fa-check"></i><b>7.10.4</b> Amostragem aleatória por conglomerados</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="13-regressao_linear.html"><a href="#amostragem-n%C3%A3o-probabil%C3%ADstica"><i class="fa fa-check"></i><b>7.11</b> Amostragem não probabilística</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="13-regressao_linear.html"><a href="#amostragem-por-conveni%C3%AAncia"><i class="fa fa-check"></i><b>7.11.1</b> Amostragem por conveniência</a></li>
<li class="chapter" data-level="7.11.2" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#amostragem-por-cotas"><i class="fa fa-check"></i><b>7.11.2</b> Amostragem por cotas</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#dimensionamento-de-amostras"><i class="fa fa-check"></i><b>7.12</b> Dimensionamento de amostras</a>
<ul>
<li class="chapter" data-level="7.12.1" data-path="introducao-ao-planejamento-de-pesquisas.html"><a href="introducao-ao-planejamento-de-pesquisas.html#erros"><i class="fa fa-check"></i><b>7.12.1</b> Erros</a></li>
<li class="chapter" data-level="7.12.2" data-path="13-regressao_linear.html"><a href="#determina%C3%A7%C3%A3o-do-tamanho-de-uma-amostra-para-estima%C3%A7%C3%A3o-da-m%C3%A9dia-populacional"><i class="fa fa-check"></i><b>7.12.2</b> Determinação do tamanho de uma amostra para estimação da média populacional</a></li>
<li class="chapter" data-level="7.12.3" data-path="13-regressao_linear.html"><a href="#determina%C3%A7%C3%A3o-do-tamanho-de-uma-amostra-para-estima%C3%A7%C3%A3o-da-propor%C3%A7%C3%A3o-populacional"><i class="fa fa-check"></i><b>7.12.3</b> Determinação do tamanho de uma amostra para estimação da proporção populacional</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html"><i class="fa fa-check"></i><b>8</b> Introdução às estatísticas epidemiológicas</a>
<ul>
<li class="chapter" data-level="8.1" data-path="13-regressao_linear.html"><a href="#tipos-de-estudos-epidemiol%C3%B3gicos"><i class="fa fa-check"></i><b>8.1</b> Tipos de estudos epidemiológicos</a></li>
<li class="chapter" data-level="8.2" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#estudos-transversais"><i class="fa fa-check"></i><b>8.2</b> Estudos transversais</a></li>
<li class="chapter" data-level="8.3" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#estudos-longitudinais"><i class="fa fa-check"></i><b>8.3</b> Estudos longitudinais</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#estudos-de-casos-e-controles"><i class="fa fa-check"></i><b>8.3.1</b> Estudos de casos e controles</a></li>
<li class="chapter" data-level="8.3.2" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#estudos-de-coorte"><i class="fa fa-check"></i><b>8.3.2</b> Estudos de coorte</a></li>
<li class="chapter" data-level="8.3.3" data-path="13-regressao_linear.html"><a href="#estudos-cl%C3%ADnicos-aleatorizados"><i class="fa fa-check"></i><b>8.3.3</b> Estudos clínicos aleatorizados</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#terminologia"><i class="fa fa-check"></i><b>8.4</b> Terminologia</a></li>
<li class="chapter" data-level="8.5" data-path="13-regressao_linear.html"><a href="#medidas-de-risco-morte-associa%C3%A7%C3%A3o-e-correla%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>8.5</b> Medidas de risco, morte, associação e correlação</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="13-regressao_linear.html"><a href="#incid%C3%AAncia"><i class="fa fa-check"></i><b>8.5.1</b> Incidência</a></li>
<li class="chapter" data-level="8.5.2" data-path="13-regressao_linear.html"><a href="#preval%C3%AAncia"><i class="fa fa-check"></i><b>8.5.2</b> Prevalência</a></li>
<li class="chapter" data-level="8.5.3" data-path="13-regressao_linear.html"><a href="#incid%C3%AAncia-cumulativa---ic-risco"><i class="fa fa-check"></i><b>8.5.3</b> Incidência cumulativa - IC (Risco)</a></li>
<li class="chapter" data-level="8.5.4" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#fatalidade-dos-casos-fc"><i class="fa fa-check"></i><b>8.5.4</b> Fatalidade dos Casos (FC)</a></li>
<li class="chapter" data-level="8.5.5" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#taxas-de-mortalidade-tm"><i class="fa fa-check"></i><b>8.5.5</b> Taxas de mortalidade (TM)</a></li>
<li class="chapter" data-level="8.5.6" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#sobrevida"><i class="fa fa-check"></i><b>8.5.6</b> Sobrevida</a></li>
<li class="chapter" data-level="8.5.7" data-path="13-regressao_linear.html"><a href="#taxas-mais-espec%C3%ADficas"><i class="fa fa-check"></i><b>8.5.7</b> Taxas mais específicas</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="13-regressao_linear.html"><a href="#medidas-de-associa%C3%A7%C3%A3o-em-estudos-de-coorte"><i class="fa fa-check"></i><b>8.6</b> Medidas de associação em estudos de coorte</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="13-regressao_linear.html"><a href="#incid%C3%AAncia-observada-de-nascimentos-com-baixo-peso-entre-m%C3%A3es-expostas-ao-risco-n%C3%A3o-fumantes-i_e"><i class="fa fa-check"></i><b>8.6.1</b> Incidência observada de nascimentos com baixo peso entre mães expostas ao risco (não fumantes): <span class="math inline">\(I_{e}\)</span></a></li>
<li class="chapter" data-level="8.6.2" data-path="13-regressao_linear.html"><a href="#incid%C3%AAncia-observada-de-nascimentos-com-baixo-peso-entre-m%C3%A3es-n%C3%A3o-expostas-ao-risco-n%C3%A3o-fumantes-i_0"><i class="fa fa-check"></i><b>8.6.2</b> Incidência observada de nascimentos com baixo peso entre mães não expostas ao risco (não fumantes): <span class="math inline">\(I_{0}\)</span></a></li>
<li class="chapter" data-level="8.6.3" data-path="13-regressao_linear.html"><a href="#preval%C3%AAncia-de-nascimentos-com-baixo-peso-na-popula%C3%A7%C3%A3o-estudada"><i class="fa fa-check"></i><b>8.6.3</b> Prevalência de nascimentos com baixo peso na população estudada</a></li>
<li class="chapter" data-level="8.6.4" data-path="13-regressao_linear.html"><a href="#diferen%C3%A7a-de-risco-risco-atribu%C3%ADvel---ra"><i class="fa fa-check"></i><b>8.6.4</b> Diferença de risco (Risco atribuível - RA)</a></li>
<li class="chapter" data-level="8.6.5" data-path="13-regressao_linear.html"><a href="#raz%C3%A3o-de-risco-risco-relativo---rr"><i class="fa fa-check"></i><b>8.6.5</b> Razão de risco (Risco relativo - RR)</a></li>
<li class="chapter" data-level="8.6.6" data-path="13-regressao_linear.html"><a href="#risco-atribu%C3%ADvel-proporcional-fra%C3%A7%C3%A3o-etiol%C3%B3gica---fe"><i class="fa fa-check"></i><b>8.6.6</b> Risco atribuível proporcional (Fração etiológica - FE)</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="13-regressao_linear.html"><a href="#odds-ratio-raz%C3%A3o-das-chances-em-studos-de-casos-e-controles"><i class="fa fa-check"></i><b>8.7</b> <em>Odds ratio</em> (Razão das chances) em studos de casos e controles</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#chance-odds-de-observar-um-desfecho-entre-os-casos"><i class="fa fa-check"></i><b>8.7.1</b> Chance (<em>odds</em>) de observar um desfecho entre os casos:</a></li>
<li class="chapter" data-level="8.7.2" data-path="introducao-as-estatisticas-epidemiologicas.html"><a href="introducao-as-estatisticas-epidemiologicas.html#chance-odds-de-observar-um-desfecho-entre-os-controles"><i class="fa fa-check"></i><b>8.7.2</b> Chance (<em>odds</em>) de observar um desfecho entre os controles:</a></li>
<li class="chapter" data-level="8.7.3" data-path="13-regressao_linear.html"><a href="#a-raz%C3%A3o-das-chances-entre-os-casos-e-controle-odds-ratio"><i class="fa fa-check"></i><b>8.7.3</b> A razão das chances entre os casos e controle (<em>odds ratio</em>):</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="13-regressao_linear.html"><a href="#correla%C3%A7%C3%A3o-linear-de-pearson"><i class="fa fa-check"></i><b>8.8</b> Correlação linear de Pearson</a></li>
<li class="chapter" data-level="8.9" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a"><i class="fa fa-check"></i><b>8.9</b> Intervalos de confiança</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="13-regressao_linear.html"><a href="#raz%C3%A3o-de-risco-risco-relativo---rr-1"><i class="fa fa-check"></i><b>8.9.1</b> Razão de risco (Risco relativo - RR)</a></li>
<li class="chapter" data-level="8.9.2" data-path="13-regressao_linear.html"><a href="#raz%C3%A3o-de-chances-odds-ratio---or"><i class="fa fa-check"></i><b>8.9.2</b> Razão de chances ( <em>odds ratio</em> - OR)</a></li>
<li class="chapter" data-level="8.9.3" data-path="13-regressao_linear.html"><a href="#diferen%C3%A7a-de-risco-risco-atribu%C3%ADvel---ra-1"><i class="fa fa-check"></i><b>8.9.3</b> Diferença de risco (Risco atribuível - RA)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="introducao-a-distribuicao-das-medias-e-diferencas-entre-medias-e-seus-intervalos-de-confianca.html"><a href="introducao-a-distribuicao-das-medias-e-diferencas-entre-medias-e-seus-intervalos-de-confianca.html"><i class="fa fa-check"></i><b>9</b> Introdução à distribuição das médias e diferenças entre médias e seus intervalos de confiança</a>
<ul>
<li class="chapter" data-level="9.1" data-path="13-regressao_linear.html"><a href="#distribui%C3%A7%C3%B5es-amostrais"><i class="fa fa-check"></i><b>9.1</b> Distribuições amostrais</a></li>
<li class="chapter" data-level="9.2" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-1"><i class="fa fa-check"></i><b>9.2</b> Intervalos de confiança</a></li>
<li class="chapter" data-level="9.3" data-path="13-regressao_linear.html"><a href="#distribui%C3%A7%C3%A3o-das-m%C3%A9dias-amostrais-e-seus-intervalos-de-confian%C3%A7a"><i class="fa fa-check"></i><b>9.3</b> Distribuição das médias amostrais e seus intervalos de confiança</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="13-regressao_linear.html"><a href="#fator-de-corre%C3%A7%C3%A3o-para-popula%C3%A7%C3%B5es-finitas"><i class="fa fa-check"></i><b>9.3.1</b> Fator de correção para populações finitas</a></li>
<li class="chapter" data-level="9.3.2" data-path="13-regressao_linear.html"><a href="#intervalo-de-confian%C3%A7a-para-a-m%C3%A9dia-de-uma-popula%C3%A7%C3%A3o-normal"><i class="fa fa-check"></i><b>9.3.2</b> Intervalo de confiança para a média de uma população Normal</a></li>
<li class="chapter" data-level="9.3.3" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-m%C3%A9dia-de-uma-popula%C3%A7%C3%A3o-normal-com-vari%C3%A2ncia-conhecida-figura-reffigfig28"><i class="fa fa-check"></i><b>9.3.3</b> Intervalos de confiança para a média de uma população Normal com variância conhecida (Figura @ref(fig:fig28))</a></li>
<li class="chapter" data-level="9.3.4" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-m%C3%A9dia-de-uma-popula%C3%A7%C3%A3o-normal-de-vari%C3%A2ncia-desconhecida-mas-amostra-n%C3%A3o-t%C3%A3o-pequena-n-ge-30-figura-reffigfig55"><i class="fa fa-check"></i><b>9.3.4</b> Intervalos de confiança para a média de uma população Normal de variância desconhecida mas amostra não tão pequena: <span class="math inline">\(n \ge 30\)</span> (Figura @ref(fig:fig55))</a></li>
<li class="chapter" data-level="9.3.5" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-m%C3%A9dia-de-uma-popula%C3%A7%C3%A3o-normal-com-vari%C3%A2ncia-desconhecida-e-amostra-de-qualquer-tamanho-figura-reffigfig58"><i class="fa fa-check"></i><b>9.3.5</b> Intervalos de confiança para a média de uma população Normal com variância desconhecida e amostra de qualquer tamanho (Figura @ref(fig:fig58))</a></li>
<li class="chapter" data-level="9.3.6" data-path="introducao-a-distribuicao-das-medias-e-diferencas-entre-medias-e-seus-intervalos-de-confianca.html"><a href="introducao-a-distribuicao-das-medias-e-diferencas-entre-medias-e-seus-intervalos-de-confianca.html#fluxograma-auxiliar"><i class="fa fa-check"></i><b>9.3.6</b> Fluxograma auxiliar</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="13-regressao_linear.html"><a href="#distribui%C3%A7%C3%A3o-das-diferen%C3%A7as-das-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-e-seus-intervalos-de-confian%C3%A7a"><i class="fa fa-check"></i><b>9.4</b> Distribuição das diferenças das médias de duas populações Normais e seus intervalos de confiança</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-diferen%C3%A7a-das-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-nomrais-de-vari%C3%A2ncias-conhecidas-e-amostras-independentes-de-qualquer-tamanho"><i class="fa fa-check"></i><b>9.4.1</b> Intervalos de confiança para a diferença das médias de duas populações Nomrais de variâncias conhecidas e amostras independentes de qualquer tamanho</a></li>
<li class="chapter" data-level="9.4.2" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-diferen%C3%A7a-das-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-de-vari%C3%A2ncias-desconhecidas-e-amostras-independentes-grandes"><i class="fa fa-check"></i><b>9.4.2</b> Intervalos de confiança para a diferença das médias de duas populações Normais de variâncias desconhecidas e amostras independentes grandes</a></li>
<li class="chapter" data-level="9.4.3" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-diferen%C3%A7a-das-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-de-vari%C3%A2ncias-desconhecidas-mas-iguais"><i class="fa fa-check"></i><b>9.4.3</b> Intervalos de confiança para a diferença das médias de duas populações Normais de variâncias desconhecidas mas iguais</a></li>
<li class="chapter" data-level="9.4.4" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-diferen%C3%A7a-das-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-de-vari%C3%A2ncias-desconhecidas-e-desiguais"><i class="fa fa-check"></i><b>9.4.4</b> Intervalos de confiança para a diferença das médias de duas populações Normais de variâncias desconhecidas e desiguais</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="13-regressao_linear.html"><a href="#distribui%C3%A7%C3%A3o-das-diferen%C3%A7as-das-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-com-m%C3%A9dias-n%C3%A3o-independentes-e-seus-intervalos-de-confian%C3%A7a"><i class="fa fa-check"></i><b>9.5</b> Distribuição das diferenças das médias de duas populações Normais com médias não independentes e seus intervalos de confiança</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="introducao-a-distribuicao-das-medias-e-diferencas-entre-medias-e-seus-intervalos-de-confianca.html"><a href="introducao-a-distribuicao-das-medias-e-diferencas-entre-medias-e-seus-intervalos-de-confianca.html#fluxograma-auxiliar-1"><i class="fa fa-check"></i><b>9.5.1</b> Fluxograma auxiliar</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introducao-a-distribuicao-das-proporcoes-e-seus-intervalos-de-confianca.html"><a href="introducao-a-distribuicao-das-proporcoes-e-seus-intervalos-de-confianca.html"><i class="fa fa-check"></i><b>10</b> Introdução à distribuição das proporções e seus intervalos de confiança</a>
<ul>
<li class="chapter" data-level="10.1" data-path="13-regressao_linear.html"><a href="#conceito-elementar-de-uma-propor%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>10.1</b> Conceito elementar de uma proporção</a></li>
<li class="chapter" data-level="10.2" data-path="13-regressao_linear.html"><a href="#distribui%C3%A7%C3%A3o-das-propor%C3%A7%C3%B5es-amostrais"><i class="fa fa-check"></i><b>10.2</b> Distribuição das proporções amostrais</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="13-regressao_linear.html"><a href="#simula%C3%A7%C3%B5es-ilustrativas-da-aproxima%C3%A7%C3%A3o-da-distribui%C3%A7%C3%A3o-das-propor%C3%A7%C3%B5es-amostrais-pela-distribui%C3%A7%C3%A3o-normal"><i class="fa fa-check"></i><b>10.2.1</b> Simulações ilustrativas da aproximação da distribuição das proporções amostrais pela distribuição Normal</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="13-regressao_linear.html"><a href="#a-aleatoriedade-das-propor%C3%A7%C3%B5es-amostrais-e-o-tamanho-amostral"><i class="fa fa-check"></i><b>10.3</b> A aleatoriedade das proporções amostrais e o tamanho amostral</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="13-regressao_linear.html"><a href="#simula%C3%A7%C3%B5es-ilustrativas-sobre-as-flutua%C3%A7%C3%B5es-das-propor%C3%A7%C3%B5es-amostrais-e-o-erro-amostral-fixado"><i class="fa fa-check"></i><b>10.3.1</b> Simulações ilustrativas sobre as flutuações das proporções amostrais e o erro amostral fixado</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-uma-propor%C3%A7%C3%A3o-pi-pela-aproxima%C3%A7%C3%A3o-da-binomial-pela-normal"><i class="fa fa-check"></i><b>10.4</b> Intervalos de confiança para uma proporção <span class="math inline">\(\Pi\)</span> pela aproximação da Binomial pela Normal</a></li>
<li class="chapter" data-level="10.5" data-path="13-regressao_linear.html"><a href="#intervalo-de-confian%C3%A7a-para-pi-diretamente-da-distribui%C3%A7%C3%A3o-binomial"><i class="fa fa-check"></i><b>10.5</b> Intervalo de confiança para <span class="math inline">\(\Pi\)</span> diretamente da Distribuição Binomial</a></li>
<li class="chapter" data-level="10.6" data-path="13-regressao_linear.html"><a href="#pobabilidades-associadas-%C3%A0-observa%C3%A7%C3%A3o-de-uma-propor%C3%A7%C3%A3o-amostral-hatp"><i class="fa fa-check"></i><b>10.6</b> Pobabilidades associadas à observação de uma proporção amostral <span class="math inline">\(\hat{p}\)</span></a></li>
<li class="chapter" data-level="10.7" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-a-diferen%C3%A7a-entre-duas-propor%C3%A7%C3%B5es-pi_x-pi_ypela-aproxima%C3%A7%C3%A3o-da-binomial-pela-normal"><i class="fa fa-check"></i><b>10.7</b> Intervalos de confiança para a diferença entre duas proporções (<span class="math inline">\(\Pi_{X}-\Pi_{Y}\)</span>)pela aproximação da Binomial pela Normal</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introducao-a-testes-de-hipoteses.html"><a href="introducao-a-testes-de-hipoteses.html"><i class="fa fa-check"></i><b>11</b> Introdução a testes de hipóteses</a>
<ul>
<li class="chapter" data-level="11.1" data-path="13-regressao_linear.html"><a href="#filosofia-da-ci%C3%AAncia"><i class="fa fa-check"></i><b>11.1</b> Filosofia da ciência</a></li>
<li class="chapter" data-level="11.2" data-path="13-regressao_linear.html"><a href="#hist%C3%B3ria"><i class="fa fa-check"></i><b>11.2</b> História</a></li>
<li class="chapter" data-level="11.3" data-path="introducao-a-testes-de-hipoteses.html"><a href="introducao-a-testes-de-hipoteses.html#conceitos"><i class="fa fa-check"></i><b>11.3</b> Conceitos</a></li>
<li class="chapter" data-level="11.4" data-path="introducao-a-testes-de-hipoteses.html"><a href="introducao-a-testes-de-hipoteses.html#natureza-dos-erros"><i class="fa fa-check"></i><b>11.4</b> Natureza dos erros</a></li>
<li class="chapter" data-level="11.5" data-path="13-regressao_linear.html"><a href="#recomenda%C3%A7%C3%B5es-gerais"><i class="fa fa-check"></i><b>11.5</b> Recomendações gerais</a></li>
<li class="chapter" data-level="11.6" data-path="introducao-a-testes-de-hipoteses.html"><a href="introducao-a-testes-de-hipoteses.html#efeito-do-limite-central"><i class="fa fa-check"></i><b>11.6</b> Efeito do limite central</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="introducao-a-testes-de-hipoteses.html"><a href="introducao-a-testes-de-hipoteses.html#erro-global"><i class="fa fa-check"></i><b>11.6.1</b> Erro global</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="13-regressao_linear.html"><a href="#estruturas-das-hip%C3%B3teses"><i class="fa fa-check"></i><b>11.7</b> Estruturas das hipóteses</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="13-regressao_linear.html"><a href="#interpreta%C3%A7%C3%A3o-gr%C3%A1fica-dos-n%C3%ADveis-de-signific%C3%A2nciaconfian%C3%A7a"><i class="fa fa-check"></i><b>11.7.1</b> Interpretação gráfica dos níveis de significância/confiança</a></li>
<li class="chapter" data-level="11.7.2" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-bilateral"><i class="fa fa-check"></i><b>11.7.2</b> Teste de hipóteses Bilateral</a></li>
<li class="chapter" data-level="11.7.3" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-unilateral-%C3%A0-esquerda"><i class="fa fa-check"></i><b>11.7.3</b> Teste de hipóteses Unilateral à esquerda</a></li>
<li class="chapter" data-level="11.7.4" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-unilateral-%C3%A0-direita"><i class="fa fa-check"></i><b>11.7.4</b> Teste de hipóteses Unilateral à direita</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-para-a-m%C3%A9dia-mu-de-uma-popula%C3%A7%C3%A3o-normal"><i class="fa fa-check"></i><b>11.8</b> Teste de hipóteses para a média <span class="math inline">\(\mu\)</span> de uma população Normal</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="13-regressao_linear.html"><a href="#cen%C3%A1rios-poss%C3%ADveis"><i class="fa fa-check"></i><b>11.8.1</b> Cenários possíveis</a></li>
<li class="chapter" data-level="11.8.2" data-path="introducao-a-testes-de-hipoteses.html"><a href="introducao-a-testes-de-hipoteses.html#roteiro-geral"><i class="fa fa-check"></i><b>11.8.2</b> Roteiro geral</a></li>
<li class="chapter" data-level="11.8.3" data-path="13-regressao_linear.html"><a href="#probabilidade-dos-intervalos-de-confian%C3%A7a-para-os-testes-de-hip%C3%B3teses-com-o-uso-da-estat%C3%ADstica-z-z-sim-mathcaln01"><i class="fa fa-check"></i><b>11.8.3</b> Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística Z (<span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>):</a></li>
<li class="chapter" data-level="11.8.4" data-path="13-regressao_linear.html"><a href="#probabilidade-dos-intervalos-de-confian%C3%A7a-para-os-testes-de-hip%C3%B3teses-com-o-uso-da-estat%C3%ADstica-t-tsim-t_n-1"><i class="fa fa-check"></i><b>11.8.4</b> Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística T (<span class="math inline">\(T\sim t_{(n-1)}\)</span>):</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-para-a-raz%C3%A3o-de-duas-vari%C3%A2ncias-fracsigma_12sigma_22"><i class="fa fa-check"></i><b>11.9</b> Teste de hipóteses para a razão de duas variâncias (<span class="math inline">\(\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\)</span>)</a></li>
<li class="chapter" data-level="11.10" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-para-as-m%C3%A9dias-mu_1mu_2-de-duas-popula%C3%A7%C3%B5es-normais-independentes"><i class="fa fa-check"></i><b>11.10</b> Teste de hipóteses para as médias (<span class="math inline">\(\mu_{1};\mu_{2}\)</span>) de duas populações Normais independentes</a>
<ul>
<li class="chapter" data-level="11.10.1" data-path="13-regressao_linear.html"><a href="#as-estruturas-poss%C3%ADveis-dos-testes-de-hip%C3%B3teses-relacionados-%C3%A0s-suas-m%C3%A9dias-ser%C3%A3o"><i class="fa fa-check"></i><b>11.10.1</b> As estruturas possíveis dos testes de hipóteses relacionados às suas médias serão:</a></li>
<li class="chapter" data-level="11.10.2" data-path="13-regressao_linear.html"><a href="#testes-de-hip%C3%B3teses-para-as-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-com-vari%C3%A2ncias-conhecidas-ou-n%C3%A3o-conhecidas-mas-o-tamanho-das-amostras-%C3%A9-grande-nge30-s2approxsigma2"><i class="fa fa-check"></i><b>11.10.2</b> Testes de hipóteses para as médias de duas populações Normais com variâncias conhecidas (ou não conhecidas mas o tamanho das amostras é grande: <span class="math inline">\(n\ge30\)</span>: <span class="math inline">\(S^{2}\approx\sigma^{2}\)</span>)</a></li>
<li class="chapter" data-level="11.10.3" data-path="13-regressao_linear.html"><a href="#testes-de-hip%C3%B3teses-para-as-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-com-vari%C3%A2ncias-desconhecidas-mas-iguais-teste-t-homoced%C3%A1stico-sigma_12sigma_22"><i class="fa fa-check"></i><b>11.10.3</b> Testes de hipóteses para as médias de duas populações Normais com variâncias desconhecidas mas iguais: teste “t’’ homocedástico (<span class="math inline">\(\sigma_{1}^{2}=\sigma_{2}^{2}=?\)</span>)</a></li>
<li class="chapter" data-level="11.10.4" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-para-as-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-normais-com-vari%C3%A2ncias-desconhecidas-e-desiguais-teste-t-heteroced%C3%A1stico-sigma_12-neq-sigma_22"><i class="fa fa-check"></i><b>11.10.4</b> Teste de hipóteses para as médias de duas populações Normais com variâncias desconhecidas e desiguais: teste “``t’’ heterocedástico (<span class="math inline">\(\sigma_{1}^{2} \neq \sigma_{2}^{2}=?\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-para-uma-propor%C3%A7%C3%A3o-pi-de-uma-popula%C3%A7%C3%A3o-binomial"><i class="fa fa-check"></i><b>11.11</b> Teste de hipóteses para uma proporção <span class="math inline">\(\Pi\)</span> de uma população binomial</a>
<ul>
<li class="chapter" data-level="11.11.1" data-path="13-regressao_linear.html"><a href="#estruturas-poss%C3%ADveis-para-as-hip%C3%B3teses"><i class="fa fa-check"></i><b>11.11.1</b> Estruturas possíveis para as hipóteses</a></li>
<li class="chapter" data-level="11.11.2" data-path="13-regressao_linear.html"><a href="#probabilidade-dos-intervalos-de-confian%C3%A7a-para-os-testes-de-hip%C3%B3teses-com-o-uso-da-estat%C3%ADstica-z-z-sim-mathcaln01-1"><i class="fa fa-check"></i><b>11.11.2</b> Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística Z (<span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>):</a></li>
</ul></li>
<li class="chapter" data-level="11.12" data-path="13-regressao_linear.html"><a href="#testes-n%C3%A3o-param%C3%A9tricos"><i class="fa fa-check"></i><b>11.12</b> Testes não paramétricos</a>
<ul>
<li class="chapter" data-level="11.12.1" data-path="13-regressao_linear.html"><a href="#teste-qui-quadrado-para-verifica%C3%A7%C3%A3o-da-independ%C3%AAncia-homogeneidade"><i class="fa fa-check"></i><b>11.12.1</b> Teste Qui-quadrado para verificação da independência (homogeneidade)</a></li>
<li class="chapter" data-level="11.12.2" data-path="13-regressao_linear.html"><a href="#corre%C3%A7%C3%A3o-de-continuidade-em-tabelas-2x2"><i class="fa fa-check"></i><b>11.12.2</b> Correção de continuidade em tabelas 2x2</a></li>
<li class="chapter" data-level="11.12.3" data-path="13-regressao_linear.html"><a href="#coeficiente-de-conting%C3%AAncia-de-pearson-modificado-c"><i class="fa fa-check"></i><b>11.12.3</b> Coeficiente de contingência de Pearson (modificado: <span class="math inline">\(C^{*})\)</span> }</a></li>
<li class="chapter" data-level="11.12.4" data-path="13-regressao_linear.html"><a href="#teste-qui-quadrado-para-verifica%C3%A7%C3%A3o-da-qualidade-do-ajuste-a-uma-distribui%C3%A7%C3%A3o-te%C3%B3rica-de-probabilidade"><i class="fa fa-check"></i><b>11.12.4</b> Teste Qui-quadrado para verificação da qualidade do ajuste a uma distribuição teórica de probabilidade</a></li>
<li class="chapter" data-level="11.12.5" data-path="13-regressao_linear.html"><a href="#teste-de-signific%C3%A2ncia-para-as-m%C3%A9dias-de-duas-popula%C3%A7%C3%B5es-dependentes"><i class="fa fa-check"></i><b>11.12.5</b> Teste de significância para as médias de duas populações dependentes</a></li>
</ul></li>
<li class="chapter" data-level="11.13" data-path="13-regressao_linear.html"><a href="#fluxograma-auxiliar-para-escolha-da-estat%C3%ADstica-do-teste-de-hip%C3%B3teses"><i class="fa fa-check"></i><b>11.13</b> Fluxograma auxiliar para escolha da estatística do teste de hipóteses</a></li>
<li class="chapter" data-level="11.14" data-path="introducao-a-testes-de-hipoteses.html"><a href="introducao-a-testes-de-hipoteses.html#tabelas-1"><i class="fa fa-check"></i><b>11.14</b> Tabelas</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="introducao-a-correlacao-linear-de-pearson.html"><a href="introducao-a-correlacao-linear-de-pearson.html"><i class="fa fa-check"></i><b>12</b> Introdução à Correlação Linear de Pearson</a>
<ul>
<li class="chapter" data-level="12.1" data-path="13-regressao_linear.html"><a href="#contexto-hist%C3%B3rico"><i class="fa fa-check"></i><b>12.1</b> Contexto histórico</a></li>
<li class="chapter" data-level="12.2" data-path="introducao-a-correlacao-linear-de-pearson.html"><a href="introducao-a-correlacao-linear-de-pearson.html#conceitos-1"><i class="fa fa-check"></i><b>12.2</b> Conceitos</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="13-regressao_linear.html"><a href="#correla%C3%A7%C3%A3o-linear-versus-regress%C3%A3o"><i class="fa fa-check"></i><b>12.2.1</b> Correlação linear <em>versus</em> regressão</a></li>
<li class="chapter" data-level="12.2.2" data-path="13-regressao_linear.html"><a href="#correla%C3%A7%C3%A3o-versus-causa%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>12.2.2</b> Correlação <em>versus</em> causação</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="13-regressao_linear.html"><a href="#diagrama-de-dispers%C3%A3o"><i class="fa fa-check"></i><b>12.3</b> Diagrama de dispersão</a></li>
<li class="chapter" data-level="12.4" data-path="13-regressao_linear.html"><a href="#coeficiente-de-correla%C3%A7%C3%A3o-linear-de-pearson"><i class="fa fa-check"></i><b>12.4</b> Coeficiente de correlação linear de Pearson</a></li>
<li class="chapter" data-level="12.5" data-path="13-regressao_linear.html"><a href="#teste-de-hip%C3%B3teses-para-a-correla%C3%A7%C3%A3o-linear-na-popula%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>12.5</b> Teste de hipóteses para a correlação linear na população</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="13-regressao_linear.html"><a href="#outros-testes-de-hip%C3%B3teses-sobre-a-correla%C3%A7%C3%A3o-linear-na-popula%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>12.5.1</b> Outros testes de hipóteses sobre a correlação linear na população</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html"><i class="fa fa-check"></i><b>13</b> Introdução à Regressão Linear</a>
<ul>
<li class="chapter" data-level="13.1" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html#contexto"><i class="fa fa-check"></i><b>13.1</b> Contexto</a></li>
<li class="chapter" data-level="13.2" data-path="13-regressao_linear.html"><a href="#simples-e-m%C3%BAltipla"><i class="fa fa-check"></i><b>13.2</b> Simples e Múltipla</a></li>
<li class="chapter" data-level="13.3" data-path="13-regressao_linear.html"><a href="#m%C3%A9todo-dos-m%C3%ADnimos-quadrados"><i class="fa fa-check"></i><b>13.3</b> Método dos Mínimos Quadrados</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html#contexto-1"><i class="fa fa-check"></i><b>13.3.1</b> Contexto</a></li>
<li class="chapter" data-level="13.3.2" data-path="13-regressao_linear.html"><a href="#dedu%C3%A7%C3%A3o-para-uma-regress%C3%A3o-linear-simples"><i class="fa fa-check"></i><b>13.3.2</b> Dedução para uma Regressão Linear Simples</a></li>
<li class="chapter" data-level="13.3.3" data-path="13-regressao_linear.html"><a href="#express%C3%A3o-matricial"><i class="fa fa-check"></i><b>13.3.3</b> Expressão Matricial</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html#o-teorema-de-gauss-markov"><i class="fa fa-check"></i><b>13.4</b> O Teorema de Gauss-Markov</a></li>
<li class="chapter" data-level="13.5" data-path="13-regressao_linear.html"><a href="#modelo-de-regress%C3%A3o-linear-com-erros-normais"><i class="fa fa-check"></i><b>13.5</b> Modelo de Regressão Linear com Erros Normais</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html#propriedades-dos-estimadores-sob-erro-normal"><i class="fa fa-check"></i><b>13.5.1</b> Propriedades dos Estimadores sob Erro Normal</a></li>
<li class="chapter" data-level="13.5.2" data-path="13-regressao_linear.html"><a href="#consequ%C3%AAncias-da-normalidade"><i class="fa fa-check"></i><b>13.5.2</b> Consequências da Normalidade</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13-regressao_linear.html"><a href="#an%C3%A1lise-diagn%C3%B3stica-do-modelo-de-regress%C3%A3o-linear"><i class="fa fa-check"></i><b>13.6</b> Análise Diagnóstica do Modelo de Regressão Linear</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="13-regressao_linear.html"><a href="#tipos-de-res%C3%ADduos"><i class="fa fa-check"></i><b>13.6.1</b> Tipos de Resíduos</a></li>
<li class="chapter" data-level="13.6.2" data-path="13-regressao_linear.html"><a href="#linearidade-na-rela%C3%A7%C3%A3o-entre-a-vari%C3%A1vel-preditora-x-e-a-vari%C3%A1vel-resposta-y"><i class="fa fa-check"></i><b>13.6.2</b> Linearidade na relação entre a variável preditora <span class="math inline">\(X\)</span> e a variável resposta <span class="math inline">\(Y\)</span>:</a></li>
<li class="chapter" data-level="13.6.3" data-path="13-regressao_linear.html"><a href="#homogeneidade-da-vari%C3%A2ncia-dos-res%C3%ADduos-varepsilon_i-homocedasticidade"><i class="fa fa-check"></i><b>13.6.3</b> Homogeneidade da variância dos resíduos <span class="math inline">\(\varepsilon_i\)</span> (homocedasticidade):</a></li>
<li class="chapter" data-level="13.6.4" data-path="13-regressao_linear.html"><a href="#independ%C3%AAncia"><i class="fa fa-check"></i><b>13.6.4</b> Independência</a></li>
<li class="chapter" data-level="13.6.5" data-path="13-regressao_linear.html"><a href="#normalidade-dos-res%C3%ADduos"><i class="fa fa-check"></i><b>13.6.5</b> Normalidade dos Resíduos</a></li>
<li class="chapter" data-level="13.6.6" data-path="13-regressao_linear.html"><a href="#observa%C3%A7%C3%B5es-inconsistentes"><i class="fa fa-check"></i><b>13.6.6</b> Observações Inconsistentes</a></li>
<li class="chapter" data-level="13.6.7" data-path="13-regressao_linear.html"><a href="#observa%C3%A7%C3%B5es-influentes"><i class="fa fa-check"></i><b>13.6.7</b> Observações Influentes</a></li>
<li class="chapter" data-level="13.6.8" data-path="13-regressao_linear.html"><a href="#multicolinearidade-fator-de-infla%C3%A7%C3%A3o-da-vari%C3%A2ncia-vif"><i class="fa fa-check"></i><b>13.6.8</b> Multicolinearidade: Fator de Inflação da Variância (VIF)</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13-regressao_linear.html"><a href="#m%C3%A9tricas-de-aferi%C3%A7%C3%A3o-da-qualidade-de-ajuste-de-uma-regress%C3%A3o-linear"><i class="fa fa-check"></i><b>13.7</b> Métricas de Aferição da Qualidade de Ajuste de uma Regressão Linear</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="13-regressao_linear.html"><a href="#coeficiente-de-determina%C3%A7%C3%A3o-r2-da-regress%C3%A3o-linear"><i class="fa fa-check"></i><b>13.7.1</b> Coeficiente de determinação <span class="math inline">\(R^{2}\)</span> da Regressão Linear</a></li>
<li class="chapter" data-level="13.7.2" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html#aic-akaike-information-criterion-e-rmse-root-mean-squared-error"><i class="fa fa-check"></i><b>13.7.2</b> AIC (Akaike <em>Information Criterion</em>) e RMSE (<em>Root Mean Squared Error</em>)</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="13-regressao_linear.html"><a href="#testes-de-hip%C3%B3teses-da-regress%C3%A3o-linear"><i class="fa fa-check"></i><b>13.8</b> Testes de Hipóteses da Regressão Linear</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="13-regressao_linear.html"><a href="#teste-f-an%C3%A1lise-de-vari%C3%A2ncia-da-regress%C3%A3o"><i class="fa fa-check"></i><b>13.8.1</b> Teste F (Análise de Variância da Regressão)</a></li>
<li class="chapter" data-level="13.8.2" data-path="13-regressao_linear.html"><a href="#testes-t-para-os-par%C3%A2metros-da-regress%C3%A3o"><i class="fa fa-check"></i><b>13.8.2</b> Testes ``t’’ para os Parâmetros da Regressão</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="13-regressao_linear.html"><a href="#intervalos-de-confian%C3%A7a-para-os-par%C3%A2metros-da-regress%C3%A3o-linear"><i class="fa fa-check"></i><b>13.9</b> Intervalos de Confiança para os Parâmetros da Regressão Linear</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html#coeficiente-linear-alpha-1"><i class="fa fa-check"></i><b>13.9.1</b> Coeficiente Linear (<span class="math inline">\(\alpha\)</span>)</a></li>
<li class="chapter" data-level="13.9.2" data-path="introducao-a-regressao-linear.html"><a href="introducao-a-regressao-linear.html#coeficiente-angular-beta_j-1"><i class="fa fa-check"></i><b>13.9.2</b> Coeficiente Angular (<span class="math inline">\(\beta_j\)</span>)</a></li>
<li class="chapter" data-level="13.9.3" data-path="13-regressao_linear.html"><a href="#vari%C3%A2ncia-sigma2"><i class="fa fa-check"></i><b>13.9.3</b> Variância (<span class="math inline">\(\sigma^2\)</span>)</a></li>
<li class="chapter" data-level="13.9.4" data-path="13-regressao_linear.html"><a href="#intervalo-de-confian%C3%A7a-para-eymathbfx"><i class="fa fa-check"></i><b>13.9.4</b> Intervalo de Confiança para <span class="math inline">\(E[Y|\mathbf{X}]\)</span></a></li>
<li class="chapter" data-level="13.9.5" data-path="13-regressao_linear.html"><a href="#intervalo-de-predi%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>13.9.5</b> Intervalo de Predição</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="introducao-a-modelagem-dados.html"><a href="introducao-a-modelagem-dados.html"><i class="fa fa-check"></i><b>14</b> Introdução à Modelagem de Dados</a>
<ul>
<li class="chapter" data-level="14.1" data-path="13-regressao_linear.html"><a href="#introdu%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>14.1</b> Introdução</a></li>
<li class="chapter" data-level="14.2" data-path="introducao-a-modelagem-dados.html"><a href="introducao-a-modelagem-dados.html#tipos-de-aprendizado"><i class="fa fa-check"></i><b>14.2</b> Tipos de Aprendizado</a></li>
<li class="chapter" data-level="14.3" data-path="13-regressao_linear.html"><a href="#classifica%C3%A7%C3%A3o-e-regress%C3%A3o"><i class="fa fa-check"></i><b>14.3</b> Classificação e Regressão</a></li>
<li class="chapter" data-level="14.4" data-path="introducao-a-modelagem-dados.html"><a href="introducao-a-modelagem-dados.html#dados"><i class="fa fa-check"></i><b>14.4</b> Dados</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="13-regressao_linear.html"><a href="#importa%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>14.4.1</b> Importação</a></li>
<li class="chapter" data-level="14.4.2" data-path="introducao-a-modelagem-dados.html"><a href="introducao-a-modelagem-dados.html#estrutura"><i class="fa fa-check"></i><b>14.4.2</b> Estrutura</a></li>
<li class="chapter" data-level="14.4.3" data-path="13-regressao_linear.html"><a href="#tipo-das-vari%C3%A1veis"><i class="fa fa-check"></i><b>14.4.3</b> Tipo das Variáveis</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="introducao-a-modelagem-dados.html"><a href="introducao-a-modelagem-dados.html#engenharia-de-features"><i class="fa fa-check"></i><b>14.5</b> Engenharia de <em>features</em></a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="13-regressao_linear.html"><a href="#depura%C3%A7%C3%A3o-de-dados-data-cleaning"><i class="fa fa-check"></i><b>14.5.1</b> Depuração de Dados (<em>Data Cleaning</em>)</a></li>
<li class="chapter" data-level="14.5.2" data-path="13-regressao_linear.html"><a href="#atribui%C3%A7%C3%A3o-de-valores-imputation"><i class="fa fa-check"></i><b>14.5.2</b> Atribuição de Valores (<em>Imputation</em>)</a></li>
<li class="chapter" data-level="14.5.3" data-path="13-regressao_linear.html"><a href="#recodifica%C3%A7%C3%A3o-de-vari%C3%A1veis-quantitativas"><i class="fa fa-check"></i><b>14.5.3</b> Recodificação de Variáveis Quantitativas</a></li>
<li class="chapter" data-level="14.5.4" data-path="13-regressao_linear.html"><a href="#codifica%C3%A7%C3%A3o-de-vari%C3%A1veis-qualitativas"><i class="fa fa-check"></i><b>14.5.4</b> Codificação de Variáveis Qualitativas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="13-regressao_linear.html"><a href="#reparti%C3%A7%C3%B5es-em-treino-e-valida%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>14.6</b> Repartições em Treino e Validação</a></li>
<li class="chapter" data-level="14.7" data-path="13-regressao_linear.html"><a href="#padroniza%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>14.7</b> Padronização</a></li>
<li class="chapter" data-level="14.8" data-path="13-regressao_linear.html"><a href="#an%C3%A1lise-explorat%C3%B3ria-1"><i class="fa fa-check"></i><b>14.8</b> Análise Exploratória</a>
<ul>
<li class="chapter" data-level="14.8.1" data-path="13-regressao_linear.html"><a href="#gr%C3%A1ficos-explorat%C3%B3rios"><i class="fa fa-check"></i><b>14.8.1</b> Gráficos Exploratórios</a></li>
</ul></li>
<li class="chapter" data-level="14.9" data-path="13-regressao_linear.html"><a href="#sele%C3%A7%C3%A3o-de-vari%C3%A1veis-regressoras"><i class="fa fa-check"></i><b>14.9</b> Seleção de Variáveis Regressoras</a>
<ul>
<li class="chapter" data-level="14.9.1" data-path="13-regressao_linear.html"><a href="#todas-as-regress%C3%B5es-poss%C3%ADveis"><i class="fa fa-check"></i><b>14.9.1</b> Todas as Regressões Possíveis</a></li>
<li class="chapter" data-level="14.9.2" data-path="13-regressao_linear.html"><a href="#m%C3%A9todo-passo-atr%C3%A1s-backward"><i class="fa fa-check"></i><b>14.9.2</b> Método “Passo Atrás” (Backward)</a></li>
<li class="chapter" data-level="14.9.3" data-path="13-regressao_linear.html"><a href="#m%C3%A9todo-passo-a-frente-forward"><i class="fa fa-check"></i><b>14.9.3</b> Método “Passo a Frente” (Forward)</a></li>
<li class="chapter" data-level="14.9.4" data-path="13-regressao_linear.html"><a href="#m%C3%A9todo-passo-a-passo-stepwise"><i class="fa fa-check"></i><b>14.9.4</b> Método “Passo a Passo” (Stepwise)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="introducao-a-modelagem-de-processos-estocasticos.html"><a href="introducao-a-modelagem-de-processos-estocasticos.html"><i class="fa fa-check"></i><b>15</b> Introdução à modelagem de processos estocásticos</a>
<ul>
<li class="chapter" data-level="15.1" data-path="13-regressao_linear.html"><a href="#modelos-determin%C3%ADsticos-e-estoc%C3%A1sticos"><i class="fa fa-check"></i><b>15.1</b> Modelos determinísticos e estocásticos</a></li>
<li class="chapter" data-level="15.2" data-path="13-regressao_linear.html"><a href="#dedu%C3%A7%C3%A3o-e-indu%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>15.2</b> Dedução e indução</a></li>
<li class="chapter" data-level="15.3" data-path="13-regressao_linear.html"><a href="#processos-estoc%C3%A1sticos-temporais-espaciais-e-espa%C3%A7otemporais"><i class="fa fa-check"></i><b>15.3</b> Processos estocásticos temporais, espaciais e espaçotemporais</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="13-regressao_linear.html"><a href="#processos-estoc%C3%A1sticos-temporais"><i class="fa fa-check"></i><b>15.3.1</b> Processos Estocásticos Temporais</a></li>
<li class="chapter" data-level="15.3.2" data-path="13-regressao_linear.html"><a href="#processos-estoc%C3%A1sticos-espaciais"><i class="fa fa-check"></i><b>15.3.2</b> Processos Estocásticos Espaciais</a></li>
<li class="chapter" data-level="15.3.3" data-path="13-regressao_linear.html"><a href="#processos-estoc%C3%A1sticos-espa%C3%A7otemporais"><i class="fa fa-check"></i><b>15.3.3</b> Processos Estocásticos Espaçotemporais</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="introducao-a-modelagem-de-processos-estocasticos.html"><a href="introducao-a-modelagem-de-processos-estocasticos.html#processo-de-poisson"><i class="fa fa-check"></i><b>15.4</b> Processo de Poisson</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="introducao-a-modelagem-de-processos-estocasticos.html"><a href="introducao-a-modelagem-de-processos-estocasticos.html#natureza"><i class="fa fa-check"></i><b>15.4.1</b> Natureza</a></li>
<li class="chapter" data-level="15.4.2" data-path="13-regressao_linear.html"><a href="#processo-de-poisson-com-classifica%C3%A7%C3%A3o-de-eventos"><i class="fa fa-check"></i><b>15.4.2</b> Processo de Poisson com classificação de eventos</a></li>
<li class="chapter" data-level="15.4.3" data-path="13-regressao_linear.html"><a href="#processos-de-poisson-n%C3%A3o-homog%C3%AAneos"><i class="fa fa-check"></i><b>15.4.3</b> Processos de Poisson não homogêneos</a></li>
<li class="chapter" data-level="15.4.4" data-path="introducao-a-modelagem-de-processos-estocasticos.html"><a href="introducao-a-modelagem-de-processos-estocasticos.html#tempo-de-espera-em-um-processo-de-poisson"><i class="fa fa-check"></i><b>15.4.4</b> Tempo de espera em um processo de Poisson</a></li>
<li class="chapter" data-level="15.4.5" data-path="13-regressao_linear.html"><a href="#distribui%C3%A7%C3%A3o-condicional-dos-tempos-de-chegada"><i class="fa fa-check"></i><b>15.4.5</b> Distribuição condicional dos tempos de chegada</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="13-regressao_linear.html"><a href="#simula%C3%A7%C3%B5es-monte-carlo"><i class="fa fa-check"></i><b>15.5</b> Simulações Monte Carlo</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="13-regressao_linear.html"><a href="#introdu%C3%A7%C3%A3o-1"><i class="fa fa-check"></i><b>15.5.1</b> Introdução</a></li>
<li class="chapter" data-level="15.5.2" data-path="13-regressao_linear.html"><a href="#fundamenta%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>15.5.2</b> Fundamentação</a></li>
<li class="chapter" data-level="15.5.3" data-path="13-regressao_linear.html"><a href="#n%C3%BAmeros-aleat%C3%B3rios-e-pseudoaleat%C3%B3rios"><i class="fa fa-check"></i><b>15.5.3</b> Números Aleatórios e Pseudoaleatórios</a></li>
<li class="chapter" data-level="15.5.4" data-path="13-regressao_linear.html"><a href="#gera%C3%A7%C3%A3o-de-amostras-aleat%C3%B3rias-de-distribui%C3%A7%C3%B5es-de-probabilidade"><i class="fa fa-check"></i><b>15.5.4</b> Geração de amostras aleatórias de distribuições de probabilidade</a></li>
<li class="chapter" data-level="15.5.5" data-path="introducao-a-modelagem-de-processos-estocasticos.html"><a href="introducao-a-modelagem-de-processos-estocasticos.html#exemplo-1-goodwin-e-wright-2009"><i class="fa fa-check"></i><b>15.5.5</b> Exemplo 1 (Goodwin e Wright, 2009)</a></li>
<li class="chapter" data-level="15.5.6" data-path="introducao-a-modelagem-de-processos-estocasticos.html"><a href="introducao-a-modelagem-de-processos-estocasticos.html#exemplo-2-the-elite-pottery-company-goodwin-e-wright-2009"><i class="fa fa-check"></i><b>15.5.6</b> Exemplo 2: The Elite Pottery Company (Goodwin e Wright, 2009)</a></li>
<li class="chapter" data-level="15.5.7" data-path="13-regressao_linear.html"><a href="#exemplo-3-integra%C3%A7%C3%A3o-num%C3%A9rica-usando-o-m%C3%A9todo-de-monte-carlo"><i class="fa fa-check"></i><b>15.5.7</b> Exemplo 3: Integração Numérica Usando o Método de Monte Carlo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="numeros-indices.html"><a href="numeros-indices.html"><i class="fa fa-check"></i><b>16</b> Introdução a números índices</a>
<ul>
<li class="chapter" data-level="16.1" data-path="13-regressao_linear.html"><a href="#defini%C3%A7%C3%A3o"><i class="fa fa-check"></i><b>16.1</b> Definição</a></li>
<li class="chapter" data-level="16.2" data-path="13-regressao_linear.html"><a href="#caracter%C3%ADsticas-principais"><i class="fa fa-check"></i><b>16.2</b> Características principais</a></li>
<li class="chapter" data-level="16.3" data-path="13-regressao_linear.html"><a href="#aplica%C3%A7%C3%B5es"><i class="fa fa-check"></i><b>16.3</b> Aplicações</a></li>
<li class="chapter" data-level="16.4" data-path="13-regressao_linear.html"><a href="#n%C3%BAmeros-%C3%ADndice-relativos-simples-apenas-um-produto-est%C3%A1-sendo-analisado."><i class="fa fa-check"></i><b>16.4</b> Números índice relativos simples: apenas um produto está sendo analisado.</a></li>
<li class="chapter" data-level="16.5" data-path="13-regressao_linear.html"><a href="#n%C3%BAmeros-%C3%ADndice-compostos-mais-de-um-produto-est%C3%A1-sendo-analisado-cesta-de-produtos."><i class="fa fa-check"></i><b>16.5</b> Números índice compostos: mais de um produto está sendo analisado (cesta de produtos).</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="13-regressao_linear.html"><a href="#m%C3%A9todo-dos-agregados-simples"><i class="fa fa-check"></i><b>16.5.1</b> Método dos agregados simples</a></li>
<li class="chapter" data-level="16.5.2" data-path="13-regressao_linear.html"><a href="#m%C3%A9todo-dos-agregados-ponderados"><i class="fa fa-check"></i><b>16.5.2</b> Método dos agregados ponderados</a></li>
<li class="chapter" data-level="16.5.3" data-path="13-regressao_linear.html"><a href="#%C3%ADndice-de-fisher-m%C3%A9dia-geom%C3%A9trica"><i class="fa fa-check"></i><b>16.5.3</b> Índice de Fisher (Média geométrica)</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="numeros-indices.html"><a href="numeros-indices.html#exemplo-completo"><i class="fa fa-check"></i><b>16.6</b> Exemplo completo</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="13-regressao_linear.html"><a href="#%C3%ADndices-simples-pre%C3%A7os-quantidades-e-valoresreceitas"><i class="fa fa-check"></i><b>16.6.1</b> Índices Simples (preços, quantidades e valores/receitas)</a></li>
<li class="chapter" data-level="16.6.2" data-path="13-regressao_linear.html"><a href="#%C3%ADndices-agregativos"><i class="fa fa-check"></i><b>16.6.2</b> Índices Agregativos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="orientacoes-gerais.html"><a href="orientacoes-gerais.html"><i class="fa fa-check"></i><b>17</b> Orientações Gerais</a>
<ul>
<li class="chapter" data-level="17.1" data-path="13-regressao_linear.html"><a href="#informa%C3%A7%C3%B5es-administrativas"><i class="fa fa-check"></i><b>17.1</b> Informações administrativas</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="orientacoes-gerais.html"><a href="orientacoes-gerais.html#regimento-geral-da-uel"><i class="fa fa-check"></i><b>17.1.1</b> Regimento geral da UEL</a></li>
<li class="chapter" data-level="17.1.2" data-path="orientacoes-gerais.html"><a href="orientacoes-gerais.html#amparos-e-apoios-na-uel"><i class="fa fa-check"></i><b>17.1.2</b> Amparos e apoios na UEL</a></li>
<li class="chapter" data-level="17.1.3" data-path="13-regressao_linear.html"><a href="#tutoriais-para-os-estudantes-da-gradua%C3%A7%C3%A3o-da-uel"><i class="fa fa-check"></i><b>17.1.3</b> Tutoriais para os estudantes da graduação da UEL</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="13-regressao_linear.html"><a href="#programas-de-atividade-acad%C3%AAmica"><i class="fa fa-check"></i><b>17.2</b> Programas de atividade acadêmica</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="13-regressao_linear.html"><a href="#geografia-1sta004---estat%C3%ADstica-aplicada-%C3%A0-geografia"><i class="fa fa-check"></i><b>17.2.1</b> Geografia: 1STA004 - Estatística Aplicada à Geografia</a></li>
<li class="chapter" data-level="17.2.2" data-path="13-regressao_linear.html"><a href="#qu%C3%ADmica-2sta032---estat%C3%ADstica"><i class="fa fa-check"></i><b>17.2.2</b> Química: 2STA032 - Estatística</a></li>
<li class="chapter" data-level="17.2.3" data-path="13-regressao_linear.html"><a href="#farm%C3%A1cia-2sta010---elementos-de-bioestat%C3%ADstica"><i class="fa fa-check"></i><b>17.2.3</b> Farmácia: 2STA010 - Elementos de bioestatística</a></li>
<li class="chapter" data-level="17.2.4" data-path="13-regressao_linear.html"><a href="#computa%C3%A7%C3%A3o-2sta030---estat%C3%ADstica"><i class="fa fa-check"></i><b>17.2.4</b> Computação: 2STA030 - Estatística</a></li>
<li class="chapter" data-level="17.2.5" data-path="13-regressao_linear.html"><a href="#engenharia-civil-2sta016---estat%C3%ADstica-e-probabilidades-202502"><i class="fa fa-check"></i><b>17.2.5</b> Engenharia Civil: 2STA016 - Estatística e probabilidades (2025/02)</a></li>
<li class="chapter" data-level="17.2.6" data-path="13-regressao_linear.html"><a href="#ci%C3%AAncia-de-dados-e-intelig%C3%AAncia-artifical-2sta011---probabilidade"><i class="fa fa-check"></i><b>17.2.6</b> Ciência de dados e Inteligência Artifical: 2STA011 - Probabilidade</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://fjrcosta.github.io/apostila/" target="blank">FJCosta Github</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">UNIVERSIDADE ESTADUAL DE LONDRINA<br />
CCE - Centro de Ciências Exatas<br />
DSTA - Departamento de Estatística (sala 11)<br />
Apostila com alguns tópicos de estatística, probabilidade e inferência<br />
Prof. M.e Eng.<span class="math inline">\(^{o}\)</span> Felinto Junior Da Costa<br />
contato (sugestões): <a href="mailto:fjcosta@uel.br" class="email">fjcosta@uel.br</a></div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introducao-a-regressao-linear" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number">Capítulo 13</span> Introdução à Regressão Linear<a href="introducao-a-regressao-linear.html#introducao-a-regressao-linear" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><br></p>
<hr />
<figure class="image">
<img src="images13/teaser.jpg" alt="Figure" width="auto" height="auto" style="display: block; margin:auto">
<figcaption style="font-size: 20px;">
<center>
“Essentially, all models are wrong,but some are useful […]” (George Edward Pelham Box, 1919 - 2013)
<center>
</figcaption>
</figure>
<hr />
<div id="contexto" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> Contexto<a href="introducao-a-regressao-linear.html#contexto" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<p>Diversos fenômenos observados no mundo real podem ser descritos por meio de uma relação entre duas ou mais variáveis. Essa relação pode ser representada por um modelo matemático, que associa a <em>variável dependente</em> com a(s) <em>variável(is) independente(s)</em>.</p>
<p><br></p>
<blockquote>
<p>Natureza das variáveis (dados):</p>
</blockquote>
<p><br></p>
<ul>
<li>Variáveis qualitativas:
<ul>
<li>ordinais (nível de escolaridade, classe de rendimento, padrão construtivo)<br />
</li>
<li>nominais (cor dos olhos, sexo, naturalidade)</li>
</ul></li>
<li>Variáveis Quantitativas.
<ul>
<li>discretas (anos completos de idade/escolaridade, degraus de uma escada, andares de um edifício).</li>
<li>contínuas (pesos, velocidades, comprimentos, áreas, volumes, tempo?)</li>
</ul></li>
</ul>
<p><br></p>
<figure class="image">
<img src="images13/tipos_variaveis.jpg" alt="Figure" width="auto" height="auto" style="display: block; margin:auto">
<figcaption style="font-size: 20px;">
<center>
Quadro esquemático sobre a natureza das variáveis
<center>
</figcaption>
</figure>
<p><br></p>
<blockquote>
<p>Relação das variáveis com o fenômeno estudado:</p>
</blockquote>
<p><br></p>
<ul>
<li><p>Variável dependente (explicada, prevista, regressando, resposta, endógena, saída, controlada)</p></li>
<li><p>Variável(is) independente(s) (explicativa, previsora, regressor, estímulo, exógena, entrada, controle)</p></li>
</ul>
<p><br></p>
<p>Em geral estudamos a relação de uma única variável dependente <span class="math inline">\(Y\)</span> com <span class="math inline">\(k\)</span> variáveis independentes <span class="math inline">\(X_1, X_2, \ldots, X_k\)</span> por meio de modelos.</p>
<p><br></p>
<p>Em epistemologia, um modelo é uma <strong>simplificação</strong> de um sistema (conjunto de elementos ou componentes inter-relacionados) utilizado para compreender a realidade.</p>
<p><br></p>
<p>Ao focar em certos aspectos (variáveis analisadas) enquanto omite outros, permitem um raciocínio substitutivo (aprender a partir do modelo) para obter insights sobre o sistema real. Modelos funcionam como ferramentas epistêmicas para a investigação, sendo distintos de teorias completas.</p>
<p><br></p>
<figure class="image">
<img src="images13/modelos_regressao_timeline.png" alt="Figure" width="auto" height="auto" style="display: block; margin:auto">
<figcaption style="font-size: 20px;">
<center>
Alguns modelos estatísticos de regressão/classificação
<center>
</figcaption>
</figure>
<p><br></p>
<p>A teoria de Regressão teve origem no século XIX com Galton. Em um de seus trabalhos, estudou a relação entre a altura dos pais e dos filhos <span class="math inline">\((X_i, Y_i)\)</span>, procurando saber como a altura do pai se relacionava com a altura do filho.</p>
<p><br></p>
<p>Um modelo de regressão pode ser linear nas <strong>variáveis</strong> ou nos <strong>parâmetros</strong>. A linearidade nos parâmetros é a relevante para a formulação da teoria da regressão.</p>
<p><br></p>
<p>Uma função é <strong>linear nos parâmetros</strong> se pode ser expressa como uma combinação linear dos parâmetros, onde nenhum parâmetro está elevado a uma potência ou multiplicado por outro parâmetro.</p>
<p><br></p>
<blockquote>
<p>Exemplos de modelos lineares nos parâmetros:</p>
</blockquote>
<p><br></p>
<ul>
<li><span class="math inline">\(Y = \alpha + \beta X^2\)</span> (linear nos parâmetros, não linear em <span class="math inline">\(X\)</span>)</li>
<li><span class="math inline">\(Y = \alpha + \beta \ln(X)\)</span></li>
<li><span class="math inline">\(Y = \alpha + \beta_1 X + \beta_2 X^2\)</span></li>
</ul>
<blockquote>
<p>Exemplos de modelos não lineares nos parâmetros:</p>
</blockquote>
<p><br></p>
<ul>
<li><span class="math inline">\(Y = \alpha e^{\beta X}\)</span></li>
<li><span class="math inline">\(Y = \alpha X^{\beta}\)</span></li>
</ul>
<hr />
</div>
<div id="simples-e-múltipla" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> Simples e Múltipla<a href="#simples-e-m%C3%BAltipla" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<p>Na regressão linear simples, objetiva-se determinar a relação entre uma única variável preditora <span class="math inline">\(X\)</span> e uma variável resposta <span class="math inline">\(Y\)</span> mediante a observação do fenômeno de interesse. Em experimentação (experimento planejado), o pesquisador define os valores de <span class="math inline">\(X\)</span> e observa os valores correspondentes de <span class="math inline">\(Y\)</span>.</p>
<p>No entanto, fenômenos reais frequentemente dependem de múltiplos fatores simultaneamente. A <strong>regressão linear múltipla</strong> estende o conceito da regressão simples para incorporar <span class="math inline">\(p\)</span> variáveis preditoras <span class="math inline">\((X_1, X_2, \ldots, X_p)\)</span>, permitindo modelar a relação entre múltiplas variáveis independentes e a variável resposta <span class="math inline">\(Y\)</span>. O modelo de regressão linear múltipla é expresso como:</p>
<p><span class="math display">\[Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \varepsilon\]</span></p>
<p>em que <span class="math inline">\(\alpha\)</span> é o intercepto, <span class="math inline">\(\beta_j\)</span> são os coeficientes angulares (ou coeficientes parciais de regressão), e <span class="math inline">\(\varepsilon\)</span> é o erro aleatório.</p>
<p><strong>Vantagens da Regressão Múltipla:</strong></p>
<ul>
<li>Modela simultaneamente o efeito de múltiplas variáveis sobre a resposta</li>
<li>Permite controlar o efeito de variáveis confundidoras</li>
<li>Aumenta o poder explicativo do modelo (maior <span class="math inline">\(R^2\)</span>)</li>
<li>Possibilita análise da contribuição individual de cada preditor mantendo os demais fixos</li>
</ul>
<p><strong>Interpretação dos Coeficientes:</strong></p>
<p>Na regressão múltipla, o coeficiente <span class="math inline">\(\beta_j\)</span> representa a variação esperada em <span class="math inline">\(Y\)</span> quando <span class="math inline">\(X_j\)</span> aumenta em uma unidade, <strong>mantendo todas as outras variáveis constantes</strong> (coeficiente parcial). Esta interpretação difere da regressão simples, onde o coeficiente representa o efeito total da variável sem controlar por outros fatores.</p>
<p><br></p>
<p>Considerem a proposição de John Maynard Keynes para a relação linear simples entre o consumo e a renda, onde ele postulava haver uma relação positiva entre ambos: uma mudança em uma das variáveis iria alterar a outra. Seu modelo funcional para essa relação, com <span class="math inline">\(Y\)</span> sendo as despesas de consumo e <span class="math inline">\(X\)</span> a renda, é:</p>
<p><br></p>
<p><span class="math display">\[
Y = \alpha + \beta \cdot X
\]</span></p>
<p><br></p>
<p>Esse modelo admite que a verdadeira relação entre <span class="math inline">\(Y\)</span> e <span class="math inline">\(X\)</span> seja uma linha reta e que a observação <span class="math inline">\(Y\)</span> para cada nível de <span class="math inline">\(X\)</span> seja uma variável aleatória. Assim, o valor esperado de <span class="math inline">\(Y\)</span> para cada valor de <span class="math inline">\(X\)</span> é:</p>
<p><br></p>
<p><span class="math display">\[
E(Y | X) = \alpha + \beta \cdot X
\]</span></p>
<p><br></p>
<p>É um modelo puramente teórico, de limitada aplicabilidade prática, pois pretende exprimir por uma relação exata (<em>determinística</em>) o consumo e a renda, quando se sabe que grande parte das relações entre duas variáveis não são exatas.</p>
<p><br></p>
<p>Ao se fixar a variável explicativa <span class="math inline">\(X\)</span> observa-se que há flutuações nos valores observados da variável explicada <span class="math inline">\(Y\)</span>. Essa inexatidão, esse desvio da cada valor observado <span class="math inline">\(Y_i\)</span> em relação ao seu valor esperado pode ser expresso da seguinte maneira:</p>
<p><br></p>
<p><span class="math display">\[
\varepsilon_i = Y_i - E(Y | X_i).
\]</span></p>
<p><br></p>
<p>Assim, o modelo completo deve ser expresso como:</p>
<p><br></p>
<p><span class="math display">\[
Y_i = E(Y | X_i) + \varepsilon_i \\
Y_i = \alpha + \beta \cdot X_i + \varepsilon_i
\]</span></p>
<p><br></p>
<p>Os componentes desse modelo de regressão com uma variável independente são:</p>
<p><br></p>
<ul>
<li><span class="math inline">\(\alpha\)</span>: intercepto da reta representando o valor esperado de <span class="math inline">\(Y\)</span> quando <span class="math inline">\(X = 0\)</span>; ou seja, <span class="math inline">\(E(Y|X=0)\)</span>. No contexto de Keynes, um consumo mínimo observado mesmo quando a renda é nula, em razão de programas de assistência governamental.</li>
<li><span class="math inline">\(\beta\)</span>: inclinação da reta, representando a variação esperada de <span class="math inline">\(Y\)</span> para um aumento unitário em <span class="math inline">\(X\)</span> (a propensão marginal a consumir: <span class="math inline">\(\frac{\Delta Y}{\Delta X}\)</span>).</li>
<li><span class="math inline">\(E(Y | X_i) = \alpha + \beta \cdot X_i\)</span>: componente sistemático ou determinístico, representando o gasto médio de todas as famílias com um mesmo nível de renda.</li>
<li><span class="math inline">\(\varepsilon_i\)</span>: termo de erro ou distúrbio estocástico, admitido como substituto para todas as <strong>demais variáveis omitidas (negligenciadas)</strong> no modelo e que podem afetar <span class="math inline">\(Y\)</span>.</li>
</ul>
<p><br></p>
<p>Nessa função:</p>
<p><br></p>
<ul>
<li><span class="math inline">\(Y\)</span>: variável dependente (consumo).</li>
<li><span class="math inline">\(X\)</span>: variável independente (renda).</li>
</ul>
<p><br></p>
<p>Se o termo de erro <span class="math inline">\(\varepsilon_i\)</span> representa todas aquelas variáveis omitidas no modelo (mas que, coletivamente, afetam <span class="math inline">\(Y\)</span>), <strong>por que não formular um modelo de regressão com o máximo de variáveis possíveis?</strong></p>
<p><br></p>
<ul>
<li>Embasamento teórico vago: A teoria existente suporta com certeza apenas algumas variáveis; o termo de erro <span class="math inline">\(\varepsilon_i\)</span> serve como um substituto para todas as variáveis excluídas no modelo.</li>
<li>Princípio da parcimônia: Um modelo mais simples que explique bem a relação é preferível.</li>
<li>Forma funcional equivocada: Em gráficos de dispersão, é mais fácil inferir a relação entre duas variáveis do que com muitas.</li>
<li>Limitação na quantidade de observações: Muitas variáveis exigem mais observações para garantir a precisão do modelo.</li>
</ul>
<p><br></p>
<p>Sendo inviável (muitas vezes impossível) impossível observar toda a população e construir um <em>modelo populacional</em>, focamos o estudo em uma parte observada dessa população: uma <em>amostra</em>.</p>
<p><br></p>
<figure class="image">
<img src="images13/amostragem2.jpg" alt="Figure" width="auto" height="auto" style="display: block; margin:auto">
<figcaption style="font-size: 20px;">
<center>
Universo (parâmetros e amostras (estimativa/estatísticas
<center>
</figcaption>
</figure>
<p><br></p>
<p>Um modelo funcional estimado com base em uma <em>amostra</em> apresenta <strong>estimativas</strong> dos <strong>parâmetros</strong> da função que o descreve na população. Por isso, adota-se uma notação diferente para a <strong>função de regressão amostral</strong>:</p>
<p><br></p>
<p><span class="math display">\[
\hat{Y} = a + b \cdot X
\]</span></p>
<p><br></p>
<p>em que&gt;</p>
<ul>
<li><span class="math inline">\(\hat{Y}\)</span> é um estimador de <span class="math inline">\(E(Y | X)\)</span><br />
</li>
<li><span class="math inline">\(a\)</span> é uma estimativa do parâmetro <span class="math inline">\(\alpha\)</span>.</li>
<li><span class="math inline">\(b\)</span> é uma estimativa do parâmetro <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p><br></p>
<p>Frequentemente são vistas notações alternativas para as estimativas de <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>, como <span class="math inline">\(\hat{\alpha}\)</span> e <span class="math inline">\(\hat{\beta}\)</span> (ou ainda <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span>).</p>
<p><br></p>
<p>Para cada valor <span class="math inline">\(x_i\)</span> presente na amostra temos o valor observado <span class="math inline">\(y_i\)</span> e a <strong>função de regressão amostral</strong> nos retorna um valor estimado <span class="math inline">\(\hat{y}_i = a + b \cdot x_i\)</span>. Como o valor observado <span class="math inline">\(y_{i}\)</span> pode ser decomposto em <span class="math inline">\(y_i = \hat{y}_i + e_i\)</span>, o modelo pode ser expresso como:.</p>
<p><br></p>
<p><span class="math display">\[
y_i = a + b \cdot x_i + e_i
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(e_i=y_i - \hat{y}_i\)</span> é o resíduo de ajuste para a <span class="math inline">\(i\text{-ésima}\)</span> observação.</p>
<p><br></p>
<blockquote>
<p><strong>Mas, como estimar <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>?</strong></p>
</blockquote>
<hr />
</div>
<div id="método-dos-mínimos-quadrados" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> Método dos Mínimos Quadrados<a href="#m%C3%A9todo-dos-m%C3%ADnimos-quadrados" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<p>Na literatura estatística há vários métodos de estimação dos parâmetros de um modelo de regressão linear, dentre os quais:</p>
<p><br></p>
<ul>
<li>Método dos mínimos quadrados: desenvolvido por Carl Friedrich Gauss (1795), publicado por Adrien-Marie Legendre (1805), Friedrich Robert Helmert (1872).<br />
</li>
<li>Método dos momentos: introduzido por Pafnuty Chebyshev (1887), desenvolvido por Karl Pearson (1894-1895), e posteriormente generalizado por Lars Peter Hansen (Método Generalizado dos Momentos - GMM, 1982).<br />
</li>
<li>Método da máxima verossimilhança: formas rudimentares foram utilizadas por Carl Friedrich Gauss, Pierre-Simon Laplace, Thorvald N. Thiele e Francis Ysidro Edgeworth; a versão moderna foi criada e popularizada por Ronald Aylmer Fisher (1912-1922).</li>
</ul>
<hr />
<div id="contexto-1" class="section level3 hasAnchor" number="13.3.1">
<h3><span class="header-section-number">13.3.1</span> Contexto<a href="introducao-a-regressao-linear.html#contexto-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Desde tempos remotos as pessoas têm se interessado pelo problema de escolher o melhor valor único (médio) para resumir as informações fornecidas por várias observações, cada uma sujeita a erro.</p>
<p><br></p>
<p>O problema de se estimar as constantes na equação da linha reta que melhor se ajusta a três ou mais pontos não colineares no plano (x, y) cujas coordenadas são pares de valores associados de duas variáveis relacionadas: <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> remonta a Galileu Galilei (1632).</p>
<p><br></p>
<p>Credita-se Johann Carl Friedrich Gauss como o desenvolvedor das bases fundamentais do Método dos mínimos quadrados, em 1795, quando Gauss tinha apenas dezoito anos.</p>
<p><br></p>
<p>Mas o Método dos mínimos quadrados foi publicado pela primeira vez por por Adrien-Marie Legendre (1752-1833) em 1805: <em>Nouvelles méthodes pour la détermination des orbites des comètes</em>.</p>
<p><br></p>
<p>Alguns demonstradores:</p>
<p><br></p>
<ul>
<li>Robert Adrain (1775-1843) em 1808: <em>Research concerning the probabilities of the errors which happen in making observations</em><br />
</li>
<li>Johann Carl Friedrich Gauss (1777-1855) em 1809: <em>Theoria motus corporum coelestium</em></li>
<li>Pierre-Simon Laplace (1749-1827) em 1810: <em>Theorie analytique des Probabilite</em> - Johann Carl Friedrich Gauss (1777-1855) em 1823: <em>Theoria combinationis observationum erroribus obnoxiae</em></li>
<li>James Ivory (1765-1842) em 1825: <em>On the Method of the Least Squares</em>.</li>
</ul>
<p><br></p>
<p>Para a função de regressão amostral <span class="math inline">\(\hat{y}_{i}= a + b.x_{i}\)</span> - em que <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> é a diferença entre o valor observado e o valor estimado - a função procurada pode ser escrita como <span class="math inline">\(y_i=a + bx_i + e_i\)</span> e o o <strong>problema</strong> se resume a determinar as constantes <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> da equação de uma linha reta que melhor se ajusta a três ou mais pontos não colineares.</p>
<p><br></p>
<p>A <strong>solução</strong> é minimizar a soma dos quadrados dos resíduos:</p>
<p><br></p>
<p><span class="math display">\[
min\left(\sum _{i=1}^{n}{e}_{i}^{2}\right).
\]</span></p>
<hr />
</div>
<div id="dedução-para-uma-regressão-linear-simples" class="section level3 hasAnchor" number="13.3.2">
<h3><span class="header-section-number">13.3.2</span> Dedução para uma Regressão Linear Simples<a href="#dedu%C3%A7%C3%A3o-para-uma-regress%C3%A3o-linear-simples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>O método dos mínimos quadrados (MMQ) é puramente geométrico: não faz nenhuma suposição sobre a distribuição dos dados ou dos erros (resíduos). Em outras palavras, ele é aplicado sem se preocupar com a natureza probabilística dos erros (resíduos). (SIMULADOR 3)</p>
<p><br></p>
<p>O objetivo é apenas ajustar a melhor reta possível para um conjunto de pontos de dados. A partir da igualdade:</p>
<p><br></p>
<p><span class="math display">\[
\sum _{i=1}^{n} [ y_{i} - \hat{y}_i ]^{2} = \sum _{i=1}^{n}{\left[y_i-\left(a+b{x}_{i}\right)\right]}^{2}
\]</span></p>
<p><br></p>
<p>a solução passa por derivar-se em relação a <span class="math inline">\(a\)</span> e em relação a <span class="math inline">\(b\)</span>, igualando-se a <em>zero</em>:</p>
<p><br></p>
<p><span class="math display">\[
\frac{\partial }{\partial a}\sum _{i=1}^{n}{\left[y_i-\left(a + b \cdot x_{i}\right)\right]}^{2}= 2 \cdot \sum _{i=1}^{n}\left(y_{i} - a - b \cdot x_{i}\right)\left(-1\right)=0 \\
\frac{\partial }{\partial b}\sum _{i=1}^{n}{\left[y_i-\left(a + b \cdot x_{i}\right)\right]}^{2}= 2\cdot \sum _{i=1}^{n}\left(y_{i} - a - b \cdot x_{i}\right)\left(-x_i\right)=0
\]</span></p>
<p><br></p>
<p>Expandindo as derivadas e simplificando (dividindo por 2 e removendo os sinais negativos):</p>
<p><br></p>
<p><span class="math display">\[
\sum _{i=1}^{n}\left(y_{i}-a-b \cdot x_{i}\right) = 0 \\
\sum _{i=1}^{n}\left(y_{i}-a-b \cdot x_{i}\right) \cdot x_i = 0
\]</span></p>
<p><br></p>
<p>Desenvolvendo a primeira equação:</p>
<p><br></p>
<p><span class="math display">\[
\sum _{i=1}^{n}y_{i} - \sum _{i=1}^{n}a - \sum _{i=1}^{n}b \cdot x_{i} = 0 \\
\sum _{i=1}^{n}y_{i} - n \cdot a - b \cdot \sum _{i=1}^{n}x_{i} = 0
\]</span></p>
<p><br></p>
<p>Desenvolvendo a segunda equação:</p>
<p><br></p>
<p><span class="math display">\[
\sum _{i=1}^{n}y_{i} \cdot x_i - \sum _{i=1}^{n}a \cdot x_i - \sum _{i=1}^{n}b \cdot x_{i}^2 = 0 \\
\sum _{i=1}^{n}y_{i} \cdot x_i - a \cdot \sum _{i=1}^{n}x_i - b \cdot \sum _{i=1}^{n}x_{i}^2 = 0
\]</span></p>
<p><br></p>
<p>Rearranjando ambas as equações, obtemos o <strong>sistema de equações normais</strong>:</p>
<p><br></p>
<p><span class="math display">\[
\begin{cases}
a \cdot n + \left(\sum_{i=1}^{n}x_{i}\right) \cdot b = \sum_{i=1}^{n}y_{i} \\
\left(\sum_{i=1}^{n}x_{i}\right) \cdot a + \left(\sum_{i=1}^{n}x_{i}^{2}\right) \cdot b = \sum_{i=1}^{n}x_{i} \cdot y_{i}
\end{cases}
\]</span></p>
<p><br></p>
<p>ou, equivalentemente:</p>
<p><br></p>
<p><span class="math display">\[
\begin{cases}
a \cdot n + b \cdot \sum_{i=1}^{n}x_{i} = \sum_{i=1}^{n}y_{i} \\
a \cdot \sum_{i=1}^{n}x_{i} + b \cdot \sum_{i=1}^{n}x_{i}^{2} = \sum_{i=1}^{n}x_{i} \cdot y_{i}
\end{cases}
\]</span></p>
<p><br></p>
<p>Após algumas manipulações algébricas obtemos as seguintes expressões para as estimativas: <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>:</p>
<p><br></p>
<p><span class="math display">\[
a\cdot n + b\cdot \sum _{i=1}^{n}{x}_{i}=\sum _{i=1}^{n}{y}_{i}
\]</span>
<br></p>
<p><span class="math display">\[
a\cdot \sum _{i=1}^{n}{x}_{i}+b\cdot \sum _{i=1}^{n}{x}_{i}^{2}=\sum _{i=1}^{n}{x}_{i}\cdot {y}_{i}
\]</span></p>
<p><br></p>
<p>chegando-se à <strong>estimativa</strong> <span class="math inline">\(b\)</span> do parâmetro <span class="math inline">\(\beta\)</span>, dada pelo <strong>estimador de mínimos quadrados</strong>:</p>
<p><br></p>
<p><span class="math display">\[
b=\frac{n\cdot \left(\sum _{i=1}^{n}{x}_{i}{y}_{i}\right)-\sum _{i=1}^{n}{x}_{i}\sum _{i=1}^{n}{y}_{i}}{n\cdot \sum _{i=1}^{n}{x}_{i}^{2}-{\left(\sum _{i=1}^{n}{x}_{i}\right)}^{2}}
\]</span></p>
<p><br></p>
<p>e à <strong>estimativa</strong> <span class="math inline">\(a\)</span> do parâmetro <span class="math inline">\(\alpha\)</span>, dada pelo <strong>estimador de mínimos quadrados</strong>:</p>
<p><span class="math display">\[
a=\frac{\left(\sum _{i=1}^{n}{x}_{i}^{2}\right)\cdot \left(\sum _{i=1}^{n}{y}_{i}\right)-\left(\sum _{i=1}^{n}{x}_{i}{y}_{i}\right)\cdot \left(\sum _{i=1}^{n}{x}_{i}\right)}{n\cdot \left(\sum _{i=1}^{n}{x}_i^{2}\right)-{\left(\sum _{i=1}^{n}{x}_{i}\right)}^{2}}
\]</span></p>
<p><br></p>
<p>Se definirmos <span class="math inline">\(S_{xy}\)</span> e <span class="math inline">\(S_{xx}\)</span> como sendo:</p>
<p><br></p>
<p><span class="math display">\[
S_{xy} = \sum _{i=1}^{n} x_{i}y_{i} - \frac{\sum _{i=1}^{n}x_{i}\cdot\sum _{i=1}^{n}y_{i}}{n}
\]</span></p>
<p><br></p>
<p>e</p>
<p><br></p>
<p><span class="math display">\[
S_{xx} = \sum _{i=1}^{n} x_{i}^{2} - \frac{(\sum _{i=1}^{n} x_{i})^{2}}{n}
\]</span></p>
<p><br></p>
<p>então podemos escrever:</p>
<p><br></p>
<p><span class="math display">\[
b = \frac{S_{xy}}{S_ {xx}}\\
\text{e} \\
a = \stackrel{-}{y} - b\cdot\stackrel{-}{x}.
\]</span></p>
<p><br></p>
<p>Uma vez que</p>
<p><br></p>
<p><span class="math display">\[
\stackrel{-}{y}=\frac{\sum _{i=1}^{n}{y}_{i}}{n}\\
\text{e}\\
\stackrel{-}{x}=\frac{\sum _{i=1}^{n}{x}_{i}}{n}
\]</span></p>
<p><br></p>
<p>o estimador <strong><span class="math inline">\(a\)</span></strong> pode ser reescrito na forma:</p>
<p><br></p>
<p><span class="math display">\[
a = \frac{\sum _{i=1}^{n}{y}_{i} - b . \sum _{i=1}^{n}{x}_{i}}{n}
\]</span></p>
<p><br></p>
<p>Portanto, <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> são os estimadores de mínimos quadrados do intercepto e inclinação, respectivamente. O modelo de regressão linear simples ajustado é dado por:</p>
<p><br></p>
<p><span class="math display">\[
\hat{y}_i = a  + b  x_i
\]</span></p>
<p><br></p>
<p>que dá uma estimativa pontual da média de <span class="math inline">\(Y\)</span> para cada valor de <span class="math inline">\(X\)</span>.</p>
<hr />
</div>
<div id="expressão-matricial" class="section level3 hasAnchor" number="13.3.3">
<h3><span class="header-section-number">13.3.3</span> Expressão Matricial<a href="#express%C3%A3o-matricial" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Na forma matricial para um modelo de regressão linear múltipla com <span class="math inline">\(p\)</span> variáveis preditoras, sejam os vetores e matrizes:</p>
<p><br></p>
<p><span class="math display">\[
Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}, \quad
X = \begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix}, \quad
\boldsymbol{\varepsilon} = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{bmatrix}
\]</span></p>
<p><br></p>
<p>em que:</p>
<ul>
<li><span class="math inline">\(Y\)</span> é o vetor <span class="math inline">\(n \times 1\)</span> das observações da variável resposta</li>
<li><span class="math inline">\(X\)</span> é a matriz <span class="math inline">\(n \times (p+1)\)</span> de planejamento (ou matriz de delineamento)</li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span> é o vetor <span class="math inline">\((p+1) \times 1\)</span> dos parâmetros do modelo</li>
<li><span class="math inline">\(\boldsymbol{\varepsilon}\)</span> é o vetor <span class="math inline">\(n \times 1\)</span> dos erros aleatórios</li>
</ul>
<p><br></p>
<p>Note que o número de colunas de <span class="math inline">\(X\)</span> é <span class="math inline">\((p+1)\)</span>, igual ao número de parâmetros a serem estimados, e o número de linhas de <span class="math inline">\(X\)</span> é <span class="math inline">\(n\)</span>, o tamanho da amostra. A primeira coluna de <span class="math inline">\(X\)</span> é um vetor com elementos iguais a 1, correspondente ao intercepto <span class="math inline">\(\beta_0\)</span>. As demais colunas contêm os valores das <span class="math inline">\(p\)</span> variáveis preditoras.</p>
<p><br></p>
<p>O vetor aleatório <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> é composto de variáveis independentes, com distribuição normal <span class="math inline">\(\varepsilon \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I})\)</span>. O vetor esperança dos elementos de <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> é o vetor nulo de dimensão <span class="math inline">\(n\)</span>. A matriz de variância-covariância de <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> possui na diagonal principal as variâncias <span class="math inline">\(\sigma^{2}\)</span> e, nos demais elementosm, as covariâncias <span class="math inline">\(Cov[\varepsilon_i, \varepsilon_j] = 0\)</span> para <span class="math inline">\(i \neq j\)</span>, com <span class="math inline">\(i, j = 1, 2, \ldots, n\)</span>.</p>
<p><br></p>
<p>Assim, pode-se assumir o modelo de regressão linear amostral da forma:</p>
<p><br></p>
<p><span class="math display">\[
\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\]</span>
<br></p>
<p>Para regressão linear simples (<span class="math inline">\(p=1\)</span>), a notação se reduz a:</p>
<p><br></p>
<p><span class="math display">\[
Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}, \quad
X = \begin{bmatrix} 1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots &amp; \vdots \\ 1 &amp; x_n \end{bmatrix}, \quad
\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}, \quad
\boldsymbol{\varepsilon} = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{bmatrix}
\]</span></p>
<p><br></p>
<p>em que a matriz <span class="math inline">\(\boldsymbol{X}\)</span> possui dimensão <span class="math inline">\(n \times 2\)</span>, com a primeira coluna de 1’s e a segunda coluna contendo os valores <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> da única variável preditora.</p>
<p><br></p>
<p>O estimador de mínimos quadrados é:</p>
<p><br></p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = (X&#39;X)^{-1}X&#39;Y
\]</span></p>
<p><br></p>
<p>Os valores ajustados (preditos) podem ser expressos como:</p>
<p><br></p>
<p><span class="math display">\[
\boldsymbol{\hat{Y}} = \boldsymbol{X}\hat{\boldsymbol{\beta}} = \boldsymbol{X}(\boldsymbol{X}&#39;\boldsymbol{X})^{-1}\boldsymbol{X}&#39;\boldsymbol{Y} = \boldsymbol{HY}
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(\mathbf{H=X(X&#39;X)^{-1}X&#39;}\)</span> é a chamada matriz <span class="math inline">\(hat\)</span> (chapéu). Propriedades da matriz chapéu:</p>
<p><br></p>
<ul>
<li><span class="math inline">\(\boldsymbol{H}\)</span> é simétrica: <span class="math inline">\(\boldsymbol{H&#39; = H}\)</span></li>
<li><span class="math inline">\(\boldsymbol{H}\)</span> é idempotente: <span class="math inline">\(\boldsymbol{HH = H}\)</span></li>
</ul>
<p><br></p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="introducao-a-regressao-linear.html#cb225-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="dv">1</span>, x))  <span class="co"># Matriz X</span></span>
<span id="cb225-2"><a href="introducao-a-regressao-linear.html#cb225-2" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(y))      <span class="co"># Matriz Y</span></span>
<span id="cb225-3"><a href="introducao-a-regressao-linear.html#cb225-3" tabindex="-1"></a>XtX <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X             <span class="co"># (X&#39;X)</span></span>
<span id="cb225-4"><a href="introducao-a-regressao-linear.html#cb225-4" tabindex="-1"></a>XtY <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> Y             <span class="co"># X&#39;Y</span></span>
<span id="cb225-5"><a href="introducao-a-regressao-linear.html#cb225-5" tabindex="-1"></a>INV <span class="ot">&lt;-</span> <span class="fu">solve</span>(XtX)             <span class="co"># Inversa da matriz (X&#39;X)</span></span>
<span id="cb225-6"><a href="introducao-a-regressao-linear.html#cb225-6" tabindex="-1"></a>Betas <span class="ot">&lt;-</span> INV <span class="sc">%*%</span> XtY          <span class="co"># Coeficientes</span></span>
<span id="cb225-7"><a href="introducao-a-regressao-linear.html#cb225-7" tabindex="-1"></a></span>
<span id="cb225-8"><a href="introducao-a-regressao-linear.html#cb225-8" tabindex="-1"></a><span class="co"># Matriz Hat</span></span>
<span id="cb225-9"><a href="introducao-a-regressao-linear.html#cb225-9" tabindex="-1"></a>H <span class="ot">&lt;-</span> X <span class="sc">%*%</span> INV <span class="sc">%*%</span> <span class="fu">t</span>(X)       <span class="co"># Matriz hat H = X(X&#39;X)^{-1}X&#39;</span></span>
<span id="cb225-10"><a href="introducao-a-regressao-linear.html#cb225-10" tabindex="-1"></a>h_ii <span class="ot">&lt;-</span> <span class="fu">diag</span>(H)               <span class="co"># Elementos da diagonal (alavancagem)</span></span>
<span id="cb225-11"><a href="introducao-a-regressao-linear.html#cb225-11" tabindex="-1"></a></span>
<span id="cb225-12"><a href="introducao-a-regressao-linear.html#cb225-12" tabindex="-1"></a><span class="co"># Valores ajustados e resíduos</span></span>
<span id="cb225-13"><a href="introducao-a-regressao-linear.html#cb225-13" tabindex="-1"></a>Y_hat <span class="ot">&lt;-</span> H <span class="sc">%*%</span> Y              <span class="co"># Valores ajustados</span></span>
<span id="cb225-14"><a href="introducao-a-regressao-linear.html#cb225-14" tabindex="-1"></a>residuos <span class="ot">&lt;-</span> Y <span class="sc">-</span> Y_hat         <span class="co"># Resíduos</span></span></code></pre></div>
<hr />
<p><br></p>
<blockquote>
<p>Exemplo 3: Um jornal deseja verificar a eficácia de seus anúncios na venda de carros usados e para isso realizou um levantamento de todos os seus anúncios e informações dos resultados obtidos pelas empresas que o contrataram e dele extraiu uma pequena amostra. A tabela abaixo mostra o número de anúncios e o correspondente número de veículos vendidos por 6 companhias que usaram apenas este jornal como veículo de propaganda. Obtenha a equação de regressão linear simples e estime o número de carros vendidos para um volume de 70 anúncios?</p>
</blockquote>
<p><br></p>
<center>
<div class="small-equation80">
<table>
<caption>
Quadro de dados da quantidade de carros vendidos por 6 empresas
distintas em função da quantidade de anúncios feitos
</caption>
<thead>
<tr>
<th style="text-align: center;">
Companhia
</th>
<th style="text-align: center;">
Anúncios feitos (X)
</th>
<th style="text-align: center;">
Carros vendidos (Y)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">
A
</td>
<td style="text-align: center;">
74
</td>
<td style="text-align: center;">
139
</td>
</tr>
<tr>
<td style="text-align: center;">
B
</td>
<td style="text-align: center;">
45
</td>
<td style="text-align: center;">
108
</td>
</tr>
<tr>
<td style="text-align: center;">
C
</td>
<td style="text-align: center;">
48
</td>
<td style="text-align: center;">
98
</td>
</tr>
<tr>
<td style="text-align: center;">
D
</td>
<td style="text-align: center;">
36
</td>
<td style="text-align: center;">
76
</td>
</tr>
<tr>
<td style="text-align: center;">
E
</td>
<td style="text-align: center;">
27
</td>
<td style="text-align: center;">
62
</td>
</tr>
<tr>
<td style="text-align: center;">
F
</td>
<td style="text-align: center;">
16
</td>
<td style="text-align: center;">
57
</td>
</tr>
</tbody>
</table>
</div>
</center>
<hr />
<p><br></p>
<table>
<caption>
Quadro para cálculo das estimativas <span class="math inline"><em>a</em></span> e <span class="math inline"><em>b</em></span> dos parâmetros do modelo
</caption>
<thead>
<tr>
<th style="text-align: center;">
Companhia
</th>
<th style="text-align: center;">
Anúncios (<span class="math inline"><em>x</em></span>)
</th>
<th style="text-align: center;">
Carros vendidos (<span class="math inline"><em>y</em></span>)
</th>
<th style="text-align: center;">
<span class="math inline"><em>x</em><sub><em>i</em></sub>.<em>y</em><sub><em>i</em></sub></span>
</th>
<th style="text-align: center;">
<span class="math inline"><em>x</em><sub><em>i</em></sub><sup>2</sup></span>
</th>
<th style="text-align: center;">
<span class="math inline"><em>y</em><sub><em>i</em></sub><sup>2</sup></span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">
A
</td>
<td style="text-align: center;">
74
</td>
<td style="text-align: center;">
139
</td>
<td style="text-align: center;">
10286
</td>
<td style="text-align: center;">
5476
</td>
<td style="text-align: center;">
19321
</td>
</tr>
<tr>
<td style="text-align: center;">
B
</td>
<td style="text-align: center;">
45
</td>
<td style="text-align: center;">
108
</td>
<td style="text-align: center;">
4860
</td>
<td style="text-align: center;">
2025
</td>
<td style="text-align: center;">
11664
</td>
</tr>
<tr>
<td style="text-align: center;">
C
</td>
<td style="text-align: center;">
48
</td>
<td style="text-align: center;">
98
</td>
<td style="text-align: center;">
4704
</td>
<td style="text-align: center;">
2304
</td>
<td style="text-align: center;">
9604
</td>
</tr>
<tr>
<td style="text-align: center;">
D
</td>
<td style="text-align: center;">
36
</td>
<td style="text-align: center;">
76
</td>
<td style="text-align: center;">
2736
</td>
<td style="text-align: center;">
1296
</td>
<td style="text-align: center;">
5776
</td>
</tr>
<tr>
<td style="text-align: center;">
E
</td>
<td style="text-align: center;">
27
</td>
<td style="text-align: center;">
62
</td>
<td style="text-align: center;">
1674
</td>
<td style="text-align: center;">
729
</td>
<td style="text-align: center;">
3844
</td>
</tr>
<tr>
<td style="text-align: center;">
F
</td>
<td style="text-align: center;">
16
</td>
<td style="text-align: center;">
57
</td>
<td style="text-align: center;">
912
</td>
<td style="text-align: center;">
256
</td>
<td style="text-align: center;">
3249
</td>
</tr>
<tr>
<td style="text-align: center;">
Totais
</td>
<td style="text-align: center;">
246
</td>
<td style="text-align: center;">
540
</td>
<td style="text-align: center;">
25172
</td>
<td style="text-align: center;">
12086
</td>
<td style="text-align: center;">
53458
</td>
</tr>
<tr>
<td style="text-align: center;">
Valor médio
</td>
<td style="text-align: center;">
41
</td>
<td style="text-align: center;">
90
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
</tr>
</tbody>
</table>
</center>
<hr />
<p><br></p>
<p>Sendo <span class="math inline">\(n= 6\)</span>, <span class="math inline">\(\stackrel{-}{y}= 90\)</span> e <span class="math inline">\(\stackrel{-}{x} = 41\)</span>:</p>
<p><span class="math display">\[
S_{xy} = \sum _{i=1}^{n} x_{i}y_{i} - \frac{\sum _{i=1}^{n}x_{i}\cdot\sum _{i=1}^{n}y_{i}}{n} = 25172 - \frac{246 \cdot 540}{6} = 3032 \\
S_{xx} = \sum _{i=1}^{n} x_{i}^{2} - \frac{(\sum _{i=1}^{n} x_{i})^{2}}{n} = 12086 - \frac{246^2}{6} = 2000 \\
{S}_{yy} = \sum _{i=1}^{n}y_{i}^{2} - \frac{(\sum _{i=1}^{n} y_{i})^{2}}{n}=   53458 - \frac{540^2}{6} = 4858
\]</span></p>
<p><br></p>
<p>As estimativas dos parâmetros do modelo serão:</p>
<p><span class="math display">\[
b = \frac{S_{xy}}{S_ {xx}} = \frac{3032}{2000} = 1,5160
\]</span></p>
<p>e</p>
<p><span class="math display">\[
a = \stackrel{-}{y} - b\cdot\stackrel{-}{x} = 90 - 1,5160 \cdot 41 = 27,844
\]</span></p>
<hr />
<p><br></p>
<p>e o modelo toma a seguinte forma <span class="math inline">\(\hat{y} = 27,844 + 1,5160 \cdot x\)</span>. Para um volume de anúncios de 70 veiculações teremos, em média, 134 carros vendidos.</p>
<hr />
<p><br></p>
<pre><code>## Equação: Y = 27.844 + 1.5160 * X</code></pre>
<p><br></p>
<pre><code>## Equação: Y = 27.844 + 1.5160 * X
## Predição para 70 anúncios: 134 carros</code></pre>
<p><br></p>
<p>O método dos mínimos quadrados fornece estimativas para os parâmetros (<span class="math inline">\(\boldsymbol{\beta}=\beta_0, \beta_1, \dots, \beta_p\)</span>) para uma amostra específica. Contudo, diferentes amostras do mesmo fenômeno produzirão estimativas distintas. Como quantificar a variabilidade dessas estimativas e modo a ser possível estabelecer intervalos para cada uma? Como associar alguma medida de certeza para essas medidas? Para responder a essa questão pressuposições sobre a distribuição dos erros devem ser estabelecidas conduzindo ao arcabouço da inferência estatística em regressão.</p>
<p><br></p>
<hr />
</div>
</div>
<div id="o-teorema-de-gauss-markov" class="section level2 hasAnchor" number="13.4">
<h2><span class="header-section-number">13.4</span> O Teorema de Gauss-Markov<a href="introducao-a-regressao-linear.html#o-teorema-de-gauss-markov" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<p>O método dos mínimos quadrados (MMQ) é um procedimento puramente algébrico e geométrico que fornece estimativas para <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>. Para responder às questões propostas devemos antes estabelecer certas propriedades estatísticas para essas estimativas.</p>
<p><br></p>
<p>O Teorema de Gauss-Markov estabelece que, sob certos pressupostos, os estimadores de MQO são os BLUE (<em>Best Linear Unbiased Estimators</em>) ou seja, possuem a <strong>menor variância</strong> possível dentro da classe de todos os estimadores lineares e não-enviesados.</p>
<p><br></p>
<figure class="image">
<img src="images13/bias_variance.jpg" alt="Figure" width="auto" height="auto" style="display: block; margin:auto">
<figcaption style="font-size: 20px;">
<center>
Precisão e enviesamento
<center>
</figcaption>
</figure>
<p><br></p>
<p>Pressupostos de Gauss-Markov:</p>
<p><br></p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Linearidade nos parâmetros: O modelo deve ser linear em relação aos parâmetros <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>, ou seja, a variável dependente <span class="math inline">\(Y\)</span> pode ser expressa como uma combinação linear desses parâmetros: <span class="math inline">\(Y_i = \alpha + \beta X_i + \varepsilon_i\)</span>.</li>
</ol>
</blockquote>
<p><strong>Importante:</strong> A linearidade refere-se aos <strong>parâmetros</strong>, não necessariamente à variável <span class="math inline">\(X\)</span>. Por exemplo, os modelos <span class="math inline">\(Y = \alpha + \beta X^2 + \varepsilon\)</span> ou <span class="math inline">\(Y = \alpha + \beta \ln(X) + \varepsilon\)</span> ainda são <strong>lineares nos parâmetros</strong> <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>, mesmo que a relação com <span class="math inline">\(X\)</span> seja não-linear. Por outro lado, modelos como <span class="math inline">\(Y = \alpha \cdot e^{\beta X} \cdot \varepsilon\)</span> ou <span class="math inline">\(Y = \alpha \cdot X^{\beta} \cdot \varepsilon\)</span> <strong>não são lineares</strong> nos parâmetros (embora possam ser linearizados por transformações logarítmicas).</p>
<hr />
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>Exogeneidade (ou Independência Média): A média condicional dos erros é zero dado qualquer valor de <span class="math inline">\(X\)</span>: <span class="math inline">\(E[\varepsilon_i | X_i] = 0\)</span>. Isto implica que não há correlação sistemática entre a variável explicativa <span class="math inline">\(X\)</span> e o termo de erro <span class="math inline">\(\varepsilon\)</span>.</li>
</ol>
</blockquote>
<p><strong>Importante:</strong> Este pressuposto garante que <span class="math inline">\(X\)</span> não carrega informação sobre o erro médio, ou seja, conhecer o valor de <span class="math inline">\(X_i\)</span> não nos ajuda a prever <span class="math inline">\(\varepsilon_i\)</span>. Violações deste pressuposto ocorrem quando há variáveis omitidas relevantes (que estão em <span class="math inline">\(\varepsilon\)</span> mas correlacionadas com <span class="math inline">\(X\)</span>), erros de medida em <span class="math inline">\(X\)</span>, ou simultaneidade (quando <span class="math inline">\(Y\)</span> também influencia <span class="math inline">\(X\)</span>). Quando este pressuposto é violado, os estimadores de MQO são <strong>enviesados</strong>.</p>
<hr />
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Homocedasticidade (Variância Constante): A variância do erro é constante para todas as observações, independentemente do valor de <span class="math inline">\(X\)</span>: <span class="math inline">\(Var(\varepsilon_i | X_i) = \sigma^2\)</span> para todo <span class="math inline">\(i\)</span>.</li>
</ol>
</blockquote>
<p><strong>Importante:</strong> Este pressuposto estabelece que a dispersão dos erros ao redor da linha de regressão é a mesma para todos os níveis de <span class="math inline">\(X\)</span>. A violação deste pressuposto — <strong>heterocedasticidade</strong> — ocorre quando <span class="math inline">\(Var(\varepsilon_i | X_i) = \sigma_i^2\)</span> varia com <span class="math inline">\(X_i\)</span>. Por exemplo, em dados de renda familiar versus gastos com alimentação, a variabilidade dos gastos tende a aumentar com a renda. Sob heterocedasticidade, os estimadores de MQO permanecem não-enviesados, mas <strong>perdem eficiência</strong> (não são mais BLUE) e os <strong>erros padrão</strong> usuais ficam incorretos, comprometendo testes de hipóteses e intervalos de confiança.</p>
<hr />
<blockquote>
<ol start="4" style="list-style-type: decimal">
<li>Ausência de Autocorrelação: Os erros de observações diferentes não estão correlacionados entre si: <span class="math inline">\(Cov(\varepsilon_i, \varepsilon_j) = 0\)</span> para todo <span class="math inline">\(i \neq j\)</span>, ou equivalentemente, <span class="math inline">\(E[\varepsilon_i \cdot \varepsilon_j] = 0\)</span> para <span class="math inline">\(i \neq j\)</span>.</li>
</ol>
</blockquote>
<p><strong>Importante:</strong> Este pressuposto é especialmente relevante para dados de <strong>séries temporais</strong> ou dados com estrutura <strong>espacial</strong>. Em séries temporais, a autocorrelação ocorre quando o erro no período <span class="math inline">\(t\)</span> está correlacionado com o erro no período <span class="math inline">\(t-1, t-2\)</span>, etc. Por exemplo, em dados mensais de vendas, choques positivos tendem a persistir por vários meses. Em dados espaciais, propriedades vizinhas tendem a ter erros correlacionados. Sob autocorrelação, os estimadores de MQO permanecem não-enviesados, mas <strong>perdem eficiência</strong> e os <strong>erros padrão usuais</strong> tornam-se enviesados (sendo frequentemente subestimados em casos de autocorrelação positiva, comum em séries temporais).</p>
<hr />
<p>O Teorema de Gauss-Markov não <strong>exige</strong> normalidade dos erros: sob esses quatro pressupostos, os estimadores de MQO já são os melhores estimadores lineares não-enviesados, mesmo que os erros não sejam normalmente distribuídos. Todavia, mesmo sendo os <em>BLUE</em>, ele não nos permite fazer as inferências estatísticas que antes estabelecemos, tais como:</p>
<ul>
<li>construir intervalos de confiança para as estimativas</li>
<li>realizar testes de hipóteses sobre seus valores</li>
</ul>
<p>Para realizar <strong>inferências estatísticas</strong>, devemos fazer uma <strong>suposição adicional</strong>.</p>
<p><br></p>
<p>Para tanto precisamos garantir que os erros (<span class="math inline">\(\varepsilon_i\)</span>) do modelo de regressão linear sejam variáveis aleatórias <strong>Normalmente distribuídas</strong> com média zero e variância constante:</p>
<p><br></p>
<p><span class="math display">\[
\varepsilon_i \sim N(0, \sigma^2)
\]</span></p>
<hr />
</div>
<div id="modelo-de-regressão-linear-com-erros-normais" class="section level2 hasAnchor" number="13.5">
<h2><span class="header-section-number">13.5</span> Modelo de Regressão Linear com Erros Normais<a href="#modelo-de-regress%C3%A3o-linear-com-erros-normais" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<p>Consolidando os pressupostos de Gauss-Markov com a normalidade, temos que as premissas do modelo de regressão linear podem ser classificadas em quatro categorias:</p>
<p><br></p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Linearidade: A relação entre a variável preditora <span class="math inline">\(X\)</span> e a variável resposta <span class="math inline">\(Y\)</span> é linear nos parâmetros; o valor esperado da variável resposta é uma combinação linear dos parãmetros</li>
</ol>
</blockquote>
<hr />
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>Normalidade: <span class="math inline">\(\varepsilon_i \sim N(0, \sigma^2)\)</span> — os erros são normalmente distribuídos com média zero e variância constante.</li>
</ol>
</blockquote>
<p><strong>Importante:</strong> Este pressuposto é <strong>adicional</strong> aos pressupostos de Gauss-Markov e não é necessário para garantir que os estimadores de MQO sejam BLUE. A normalidade dos erros é fundamental para: (i) garantir que os estimadores <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> sejam também normalmente distribuídos, permitindo o uso das distribuições <span class="math inline">\(t\)</span> e <span class="math inline">\(F\)</span> para inferência; (ii) construir intervalos de confiança válidos; (iii) realizar testes de hipóteses em <strong>amostras pequenas</strong>. Para <strong>amostras grandes</strong>, o Teorema Central do Limite garante que os estimadores sejam aproximadamente normais mesmo sem este pressuposto. A violação da normalidade compromete principalmente a validade de intervalos de confiança e testes em amostras pequenas (<span class="math inline">\(n &lt; 30\)</span>).</p>
<hr />
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Independência estatística dos resíduos: <span class="math inline">\(Cov(\varepsilon_i, \varepsilon_j) = E(\varepsilon_i \cdot \varepsilon_j) = 0\)</span> para <span class="math inline">\(i \neq j\)</span>, e, em particular, nenhuma correlação entre erros de observações sucessivas no caso de dados provenientes de uma série.</li>
</ol>
</blockquote>
<p><strong>Importante:</strong> Este pressuposto estabelece que o erro associado a uma observação não fornece informação sobre o erro de outra observação. A violação deste pressuposto — <strong>autocorrelação</strong> ou <strong>correlação serial</strong> — é particularmente comum em: (i) <strong>séries temporais</strong>, onde choques em <span class="math inline">\(t\)</span> persistem em períodos subsequentes <span class="math inline">\(t+1, t+2, ...\)</span>; (ii) <strong>dados espaciais</strong>, onde observações geograficamente próximas tendem a ter erros correlacionados; (iii) <strong>dados em painel</strong>, com observações repetidas da mesma unidade ao longo do tempo. A presença de autocorrelação não enviesa os estimadores de MQO, mas os torna <strong>ineficientes</strong> (não são mais BLUE), e os <strong>erros padrão usuais</strong> ficam <strong>enviesados</strong>, levando a intervalos de confiança artificialmente estreitos e rejeição excessiva de <span class="math inline">\(H_0\)</span> em testes. A detecção pode ser feita pelo teste de Durbin-Watson ou análise gráfica dos resíduos ao longo do tempo.</p>
<hr />
<blockquote>
<ol start="4" style="list-style-type: decimal">
<li>Homocedasticidade: <span class="math inline">\(Var(\varepsilon_i) = E(\varepsilon_i^2) = \sigma^2\)</span> — a variância dos erros é constante quando analisada frente aos valores estimados pelo modelo (<span class="math inline">\(\hat{Y}\)</span>), a variável preditora (<span class="math inline">\(X\)</span>) ou o tempo de coleta nos casos de dados provenientes de uma série.</li>
</ol>
</blockquote>
<p><strong>Importante:</strong> Este pressuposto requer que a dispersão dos erros seja <strong>uniforme</strong> ao longo de toda a amplitude de <span class="math inline">\(X\)</span> ou <span class="math inline">\(\hat{Y}\)</span>. A violação deste pressuposto — <strong>heterocedasticidade</strong> — ocorre quando <span class="math inline">\(Var(\varepsilon_i) = \sigma_i^2\)</span> varia sistematicamente, sendo comum em: (i) dados de <strong>corte transversal</strong> com unidades de diferentes tamanhos (ex: renda e gastos de famílias, onde famílias ricas têm maior variabilidade); (ii) dados onde a <strong>precisão da medição</strong> varia entre observações; (iii) modelos onde a variável dependente cresce exponencialmente. Sob heterocedasticidade, os estimadores de MQO permanecem <strong>não-enviesados</strong> e <strong>consistentes</strong>, mas perdem <strong>eficiência</strong> (não são BLUE), e os <strong>erros padrão</strong> calculados pela fórmula usual ficam <strong>incorretos</strong>, comprometendo testes de hipóteses e intervalos de confiança. A detecção pode ser feita por gráficos de resíduos versus valores ajustados ou testes formais (Breusch-Pagan, White). Correções incluem transformações (Box-Cox, logarítmica) ou uso de erros padrão robustos (White, HC).—</p>
<hr />
<div id="propriedades-dos-estimadores-sob-erro-normal" class="section level3 hasAnchor" number="13.5.1">
<h3><span class="header-section-number">13.5.1</span> Propriedades dos Estimadores sob Erro Normal<a href="introducao-a-regressao-linear.html#propriedades-dos-estimadores-sob-erro-normal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Demonstra-se que, para um modelo <span class="math inline">\(Y_{i}=\alpha+\beta\cdot X_{i}+\varepsilon_{i}\)</span> que:</p>
<p><br></p>
<ul>
<li><span class="math inline">\(b\)</span> é um estimador não tendencioso do parâmetro <span class="math inline">\(\beta\)</span> com:</li>
</ul>
<p><br></p>
<p><span class="math display">\[
E\left(b\right)=\beta \\
\text{e} \\
Var\left(b\right)=\frac{{\sigma }^{2}}{{S}_{xx}}\\
\text{em que}\\
S_{xx}=\sum(x_{i}-\bar{x})^{2}
\]</span></p>
<p><br></p>
<ul>
<li><span class="math inline">\(a\)</span> é um estimador não tendencioso do parâmetro <span class="math inline">\(\alpha\)</span> com:</li>
</ul>
<p><br></p>
<p><span class="math display">\[
E\left(a\right)=\alpha\\
\text{e} \\
Var\left(a\right)={\sigma }^{2}\cdot \left(\frac{1}{n}+\frac{{\stackrel{-}{x}}^{2}}{{S}_{xx}}\right)
\]</span></p>
<p><br></p>
<ul>
<li><span class="math inline">\(\hat{\sigma^{2}}\)</span> é um estimador não tendencioso de <span class="math inline">\(\sigma^{2}\)</span>:</li>
</ul>
<p><br></p>
<p><span class="math display">\[
S^{2} =  \text{QMRes} = \frac{S_{yy} -b  \cdot S_{xy}}{n-2}
\]</span></p>
<p><br></p>
<p>Assim os desvios padrão dos estimadores <span class="math inline">\(a\)</span> (<span class="math inline">\(S_{a}\)</span>) e <span class="math inline">\(b\)</span> (<span class="math inline">\(S_{b}\)</span>) serão</p>
<p><br></p>
<p><span class="math display">\[
S_{b} = \sqrt{\frac{{\hat{\sigma}}^{2}}{{S}_{xx}}}  = \sqrt{\frac{\text{QMRes}}{S_{xx}}}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
S_{a} = \sqrt{{\hat{\sigma} }^{2}\cdot \left(\frac{1}{n}+\frac{{\stackrel{-}{x}}^{2}}{{S}_{xx}}\right)} = \sqrt{\text{QMRes} \cdot \left(\frac{1}{n}+\frac{{\stackrel{-}{x}}^{2}}{{S}_{xx}}\right)}
\]</span></p>
<p><br></p>
<p>lembrando que:</p>
<p><br></p>
<p><span class="math display">\[
S_{yy} = \sum (Y_i - \bar{Y})^2 \\
S_{xy} = \sum (X_i - \bar{X})(Y_i - \bar{Y}),
\]</span></p>
<p><br></p>
<p>e <span class="math inline">\(n - 2\)</span> representa os graus de liberdade, já que são dois parâmetros (<span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>) sendo estimados.</p>
<hr />
</div>
<div id="consequências-da-normalidade" class="section level3 hasAnchor" number="13.5.2">
<h3><span class="header-section-number">13.5.2</span> Consequências da Normalidade<a href="#consequ%C3%AAncias-da-normalidade" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>A normalidade dos resíduos <span class="math inline">\(\varepsilon_i\)</span> garante que os estimadores <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> também sejam Normalmente distribuídos, o que é fundamental para realizar variadas inferências sobre o fenômeno estudado, na forma de testes de hipóteses e intervalos de confiança dos modelos de regressão linear ajustados.</p>
<p><br></p>
<p>Isso permite o uso de distribuições de referência, como as distribuições <span class="math inline">\(t\)</span> e <span class="math inline">\(F\)</span>, especialmente em amostras pequenas, onde a variância dos estimadores não pode ser assumida como conhecida com precisão.</p>
<p><br></p>
<p>Na estimação de um modelo de regressão linear simples com erro Normal (na forma <span class="math inline">\(Y=\beta_{0}+\beta_{1}X+ \varepsilon\)</span>) muitas premissas preliminarmente como válidas deverão ser efetivamente verificadas a posteriori, na chamada etapa de diagnóstico do modelo, de modo a que a condução de inferências com esse modelo sejam dotada de razoável segurança.</p>
<p><br></p>
<p>Se qualquer uma dessas premissas for violada então uma conclusão científica baseada em resultados advindos desse modelo de regressão poderá estar seriamente comprometida. As violações desses pressupostos não podem ser detectadas pelas estatísticas de resumo do modelo que usualmente se dispõe logo após sua estimação: estatísticas <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span> dos testes de significância ou então o coeficiente de determinação <span class="math inline">\(R^{2}\)</span>.</p>
<hr />
<p><br></p>
<p>Assim, é sobretudo fundamental examinar mais aprofundadamente o modelo de modo a se assegurar com razoável confiança de sua adequação aos dados antes de se avançar com seu uso. A esse exame denominamos análise diagnóstica do modelo.</p>
<hr />
</div>
</div>
<div id="análise-diagnóstica-do-modelo-de-regressão-linear" class="section level2 hasAnchor" number="13.6">
<h2><span class="header-section-number">13.6</span> Análise Diagnóstica do Modelo de Regressão Linear<a href="#an%C3%A1lise-diagn%C3%B3stica-do-modelo-de-regress%C3%A3o-linear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<hr />
<div id="tipos-de-resíduos" class="section level3 hasAnchor" number="13.6.1">
<h3><span class="header-section-number">13.6.1</span> Tipos de Resíduos<a href="#tipos-de-res%C3%ADduos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p><strong>Resíduos Padronizados</strong> são escalonados para que possuam variância aproximadamente unitária, mantendo a esperança nula inerente aos resíduos brutos. Consequentemente <span class="math inline">\(|d_i| &gt; 3\)</span> indica <em>outliers</em>.</p>
<p><br></p>
<p><span class="math display">\[
d_i = \frac{e_i}{S} =  \frac{e_i}{\sqrt{QMRes}}
\]</span></p>
<p><br></p>
<p>com <span class="math inline">\(i = 1, 2, \ldots, n\)</span>.</p>
<p><br></p>
<pre><code>## Os resíduos padronizados são: { -0.1271 1.476 -0.3231 -0.794 -0.8381 0.606 }</code></pre>
<p><br></p>
<p><strong>Resíduo na forma de Student (Studentized)</strong> – os resíduos padronizados e estudentizados são parecidos, mas em algumas situações os resíduos estudentizados são mais sensíveis para detectar <em>outliers</em>, dados por:</p>
<p><br></p>
<p><span class="math display">\[
r_i = \frac{e_i}{\sqrt{S^2(1-h_{ii})}}
\]</span></p>
<p><br></p>
<p>com <span class="math inline">\(i = 1, 2, \ldots, n\)</span> e, caso para <span class="math inline">\(|r_i| &gt; 3\)</span>, há uma indicação de se tratarem de <em>outliers</em>.</p>
<p><br></p>
<hr />
</div>
<div id="linearidade-na-relação-entre-a-variável-preditora-x-e-a-variável-resposta-y" class="section level3 hasAnchor" number="13.6.2">
<h3><span class="header-section-number">13.6.2</span> Linearidade na relação entre a variável preditora <span class="math inline">\(X\)</span> e a variável resposta <span class="math inline">\(Y\)</span>:<a href="#linearidade-na-rela%C3%A7%C3%A3o-entre-a-vari%C3%A1vel-preditora-x-e-a-vari%C3%A1vel-resposta-y" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>A violação da linearidade é extremamente grave pois um modelo ajustado a dados não lineares leva a previsões equivocadas não somente para valores situados além das fronteiras amostrais (como se usualmente observa) mas também para valores próximos ao seu centro.</p>
<p>Uma técnica para se verificar a linearidade da relação é através de dois gráficos:</p>
<ul>
<li>valores observados <em>versus</em> valores estimados; ou/e,</li>
<li>resíduos obtidos <em>versus</em> valores estimados.</li>
</ul>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-229-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Os padrões desejados nos gráficos acima deve assemelhar-se a:</p>
<ul>
<li>pontos dispersos de modo aproximadamente simétrico em torno de uma linha diagonal; e,</li>
<li>pontos dispersos de modo aproximadamente simétrico em torno de uma linha horizontal, com uma variância aproximadamente homogênea.</li>
</ul>
<p>Relações não lineares devem ser tratadas por meio da aplicação de uma transformação não linear adequada ao padrão da relação na variável resposta ou no variável preditora.</p>
<p>Para dados estritamente positivos com uma relação não linear a transformação com a função logaritmo pode ser uma opção. Se uma a transformação com o uso da função logaritmo é aplicada apenas à variável resposta isso equivalente a assumir que ela cresce (ou decai) exponencialmente como uma função da variável preditora.</p>
<p>Outra possibilidade a considerar é adicionar outra variável preditora na forma de uma função não linear como, por exepmplo, nos padrões de dispersão que mostrem uma curva parabólica onde pode fazer sentido regredir <span class="math inline">\(Y\)</span> em função de <span class="math inline">\(X\)</span> e <span class="math inline">\(X^{2}\)</span>.</p>
<p>Finalmente, a relação não linear observada pode decorrer da omissão de outra(s) variáveis importantes que explicam ou corrigem o padrão não linear quando então modelos de regressão linear múltipla devem ser estudados.</p>
<hr />
</div>
<div id="homogeneidade-da-variância-dos-resíduos-varepsilon_i-homocedasticidade" class="section level3 hasAnchor" number="13.6.3">
<h3><span class="header-section-number">13.6.3</span> Homogeneidade da variância dos resíduos <span class="math inline">\(\varepsilon_i\)</span> (homocedasticidade):<a href="#homogeneidade-da-vari%C3%A2ncia-dos-res%C3%ADduos-varepsilon_i-homocedasticidade" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>A violação da homogeneidade de variância dos resíduos (heterocedasticidade) resulta numa estimação imprecisa do verdadeiro desvio padrão dos erros das estimativas e acarreta em intervalos de confiança irreais: são mais amplos ou mais estreitos do que deveriam ser, e resultam em elevada imprecisão nas inferências feitas com estatísticas baseadas na variância (<span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>).</p>
<p><br></p>
<p>Com variância constante (homocedasticidade) temos que <span class="math inline">\(Var(\varepsilon|X_{i})=\sigma^{2}\)</span>; todavia o que se observa em muitas situações é que a variância está relacionada de algum modo funcional com a variável preditora (<span class="math inline">\(\sigma^{2}=\mathcal{f}(X)\)</span>) e, assim:</p>
<p><br></p>
<p><span class="math display">\[
\begin{aligned}
Var(\varepsilon_{i}|X_{i})=\sigma^{2}_{i} \\
E(\varepsilon_{i}^{2})=\sigma^{2}_{i}
\end{aligned}
\]</span></p>
<p><br></p>
<p>Na presença de heterocedasticidade nos resíduos, os estimadores de mínimos quadrados continuam sendo não viesados e consistentes, mas perdem eficiência. Equivale a dizer que haverá um outro estimador para os parâmetros do modelo que terá uma variância menor mantendo propriedade de não viés:</p>
<p><br></p>
<p><span class="math display">\[
\begin{aligned}
Var(b^{*}) &lt; Var(b)
\end{aligned}
\]</span></p>
<p><br></p>
<p>Uma técnica para se verificar a homocedasticidade dos resíduos é através dos gráficos:</p>
<ul>
<li>resíduos <em>versus</em> valores estimados; ou,</li>
<li>resíduos <em>versus</em> a variável preditora</li>
</ul>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-230-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<p>Os padrões desejados nos gráficos acima devem assemelhar-se a pontos dispersos de modo aproximadamente simétrico em torno de um eixo horizontal e que não exibam, sistematicamente, nenhum padrão de crescimento ou decaimento na amplitude visual de sua dispersão como nas imagens acima.</p>
<p><br></p>
<p>Gráficos dos valores absolutos dos resíduos (ou do quadrado dos resíduos pois os sinais dos resíduos não são significativos para o propósito desse exame) contra a variável preditora <span class="math inline">\(X\)</span> ou em relação aos valores ajustados também são úteis para o diagnóstico da heterocedasticidade da variância dos resíduos.</p>
<p><br></p>
<p>Esses gráficos são recomendados quando não há muitas observações no conjunto de dados pois a plotagem dos resíduos absolutos ou seus quadrados coloca as informações sobre a alteração das suas magnitudes acima da linha horizontal do zero o que facilita a inspeção visual de possíveis alterações de sua magnitude em relação a outra variável adotada no gráfico.</p>
<p><br></p>
<p>A heterocedasticidade pode ser um subproduto de uma violação significativa das premissas de linearidade e/ou independência, caso em que todas essas violações podem ser conjuntamente corrigidas com a aplicação de uma transformação de potência na variável dependente que terá como objetivos:</p>
<ul>
<li>linearizar o ajuste tanto quanto possível; e/ou,</li>
<li>estabilizar a variância dos resíduos.</li>
</ul>
<p><br></p>
<p>Algum cuidado e discernimento é requerido pois esses dois objetivos podem conflitar entre si. Geralmente opta-se em estabilizar a variância dos resíduos primeiramente para, só então analisar a linearização das relações.</p>
<p><br></p>
<p>As transformações sugeridas pela família Box-Cox (1964) em função do valor que maximiza a verossimilhança perfilada são:</p>
<ul>
<li>se <span class="math inline">\(\lambda\)</span>=-2 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\frac{1}{Y^{2}}\)</span></li>
<li>se <span class="math inline">\(\lambda\)</span>=-1 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\frac{1}{Y}\)</span></li>
<li>se <span class="math inline">\(\lambda\)</span>=-0,5 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\frac{1}{\sqrt{Y}}\)</span></li>
<li>se <span class="math inline">\(\lambda\)</span>=0 <span class="math inline">\(\rightarrow\)</span> log(Y)</li>
<li>se <span class="math inline">\(\lambda\)</span>=0,50 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\sqrt{Y}\)</span></li>
<li>se <span class="math inline">\(\lambda\)</span>=1 <span class="math inline">\(\rightarrow\)</span> Y</li>
<li>se <span class="math inline">\(\lambda\)</span>=2 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(Y^{2}\)</span></li>
</ul>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-231-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## 
## Lambda ótimo da transformação Box-Cox: 0.8283</code></pre>
<p><br></p>
<p>Testes de hipóteses para verificação da homogeneidade da variância em regressão:</p>
<ul>
<li>teste de Breusch-Pagan</li>
<li>teste de Park</li>
</ul>
<p><br></p>
<p><span class="math display">\[
\begin{cases}
H_{0}: Var(\varepsilon|X_{i}) = \sigma^{2} \text{ ou seja, constante para todo } i  \\
H_{1}: Var(\varepsilon|X_{i}) = \sigma^{2}_{i} \text{,  varia com  } i
\end{cases}
\]</span></p>
<p><br></p>
<pre><code>## 
## Teste de Breusch-Pagan para heterocedasticidade:
## 
##  studentized Breusch-Pagan test
## 
## data:  modelo
## BP = 0.14, df = 1, p-value = 0.7
## 
## 
## Call:
## lm(formula = ln_residuos_sq ~ ln_anuncios)
## 
## Residuals:
##      1      2      3      4      5      6 
## -1.696  2.359 -0.570  0.738  0.355 -1.186 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)     9.10       5.08    1.79     0.15
## ln_anuncios    -1.71       1.40   -1.22     0.29
## 
## Residual standard error: 1.65 on 4 degrees of freedom
## Multiple R-squared:  0.272,  Adjusted R-squared:  0.0896 
## F-statistic: 1.49 on 1 and 4 DF,  p-value: 0.289</code></pre>
<hr />
</div>
<div id="independência" class="section level3 hasAnchor" number="13.6.4">
<h3><span class="header-section-number">13.6.4</span> Independência<a href="#independ%C3%AAncia" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Quando as observações da amostra são independentes o que se espera é que seus resíduos apresentem-se aleatoriamente dispersos em torno da linha horizontal (zero) quando dispostos na sequência em que foram coletadas. O que se pretende aqui é verificar se há correlação serial entre os resíduos.</p>
<p><br></p>
<p>A autocorrelação pode ser definida como a correlação entre integrantes de séries de observações ordenadas no tempo (como as séries temporais) ou no espaço (como nos dados de corte transversal) quando então os resíduos de duas observações guardam correlação diferente de zero entre si:</p>
<p><br></p>
<p><span class="math display">\[
\begin{aligned}
cov(e_{i}, e_{j}|x_{i}, x_{j}) \neq 0  \\
i \neq j
\end{aligned}
\]</span></p>
<p><br></p>
<p>A correlação serial pode decorrer:</p>
<ul>
<li>inércia: quando os efeitos na alteração da variável <span class="math inline">\(X\)</span> demoram a se manifestar na variável <span class="math inline">\(Y\)</span> (muito comum em dados econômicos);<br />
</li>
<li>forma funcional do modelo incorreta;<br />
</li>
<li>variáveis importantes foram omitidas.</li>
</ul>
<p><br></p>
<p>Uma técnica para se verificar a ausência de correlação serial (independência dos resíduos <span class="math inline">\(\hat{\varepsilon}\)</span>) é através do gráfico dos resíduos versus o tempo ou ordem no qual as observações foram realizadas.</p>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-233-1.png" width="672" style="display: block; margin: auto;" /></p>
<hr />
</div>
<div id="normalidade-dos-resíduos" class="section level3 hasAnchor" number="13.6.5">
<h3><span class="header-section-number">13.6.5</span> Normalidade dos Resíduos<a href="#normalidade-dos-res%C3%ADduos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>A normalidade dos resíduos é essencial para a validade dos testes de hipóteses e intervalos de confiança em regressão. Embora pequenos afastamentos da normalidade não comprometam seriamente as inferências, desvios substanciais podem invalidar conclusões estatísticas, especialmente em amostras pequenas.</p>
<p><br></p>
<p>A análise gráfica constitui o primeiro passo para avaliar a normalidade dos resíduos. Os principais métodos incluem:</p>
<ul>
<li><p><strong>Comparação com frequências esperadas</strong>: Sob normalidade, espera-se que 68% dos resíduos padronizados estejam em <span class="math inline">\(\pm 1\)</span> desvio padrão, 90% em <span class="math inline">\(\pm 1{,}65\)</span> desvios padrão e 95% em <span class="math inline">\(\pm 1{,}96\)</span> desvios padrão</p></li>
<li><p><strong>Histograma</strong>: Permite visualizar a forma da distribuição e compará-la com a curva normal teórica</p></li>
<li><p><strong>Gráfico de caixa</strong>: Identifica assimetria e presença de valores extremos</p></li>
<li><p><strong>QQ-plot (gráfico quantil-quantil)</strong>: Compara os quantis amostrais dos resíduos padronizados com os quantis teóricos da distribuição Normal padrão. Se os resíduos provêm de uma distribuição Normal, os pontos devem alinhar-se aproximadamente sobre uma reta</p></li>
<li><p><strong>Gráfico com envoltória simulada</strong>: Proposto por Ripley (1977), fornece bandas de confiança para avaliar desvios da normalidade</p></li>
</ul>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-234-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<pre><code>## 
## Frequências observadas vs esperadas (sob normalidade):
## Dentro de ±1,00 DP:  83.3% (esperado: 68,0%)
## Dentro de ±1,65 DP: 100.0% (esperado: 90,0%)
## Dentro de ±1,96 DP: 100.0% (esperado: 95,0%)</code></pre>
<p><br></p>
<blockquote>
<p>Interpretação do QQ-plot</p>
</blockquote>
<p><br></p>
<p>O gráfico quantil-quantil compara os quantis amostrais ordenados dos resíduos padronizados com os quantis teóricos da distribuição <span class="math inline">\(N(0,1)\)</span>. Sob normalidade, os pontos devem dispor-se aproximadamente sobre uma linha reta. Padrões sistemáticos de afastamento indicam violações específicas:</p>
<ul>
<li><p><strong>Padrão “S”</strong>: Distribuição com caudas curtas (platicúrtica), indicando concentração excessiva de valores próximos à média.</p></li>
<li><p><strong>Padrão “S invertido”</strong>: Distribuição com caudas longas (leptocúrtica), indicando presença de valores extremos além do esperado sob normalidade.</p></li>
<li><p><strong>Padrão “J”</strong>: Assimetria positiva (cauda direita mais longa).</p></li>
<li><p><strong>Padrão “J invertido”</strong>: Assimetria negativa (cauda esquerda mais longa).</p></li>
</ul>
<p><br></p>
<blockquote>
<p>Considerações sobre diagnóstico de normalidade</p>
</blockquote>
<p><br></p>
<p>A avaliação da normalidade dos resíduos apresenta desafios práticos:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Tamanho amostral</strong>: Em amostras pequenas (<span class="math inline">\(n &lt; 30\)</span>), a variação aleatória dificulta a distinção entre flutuações amostrais e desvios reais da normalidade. Para amostras muito grandes (<span class="math inline">\(n \gtrsim 300\)</span>, as inferências tornam-se aproximadamente válidas mesmo com desvios moderados da normalidade devido à normalidade assintótica dos estimadores dos coeficientes de regressão.</p></li>
<li><p><strong>Outros desvios</strong>: Inadequação da forma funcional ou heterocedasticidade podem distorcer a distribuição dos resíduos, confundindo o diagnóstico de normalidade.</p></li>
<li><p><strong>Tolerância a desvios</strong>: Pequenos afastamentos da normalidade são aceitáveis e não comprometem seriamente as inferências, especialmente em amostras moderadas a grandes.</p></li>
</ol>
<p><br></p>
<blockquote>
<p>Testes formais de normalidade</p>
</blockquote>
<p><br></p>
<p>Embora a análise gráfica seja fundamental, testes de hipóteses formais complementam o diagnóstico:</p>
<p><strong>Hipóteses:</strong></p>
<p><span class="math display">\[
\begin{cases}
H_0: \text{os resíduos seguem distribuição Normal} \\
H_1: \text{os resíduos não seguem distribuição Normal}
\end{cases}
\]</span></p>
<p><br></p>
<p><strong>Testes mais utilizados:</strong></p>
<ul>
<li><strong>Shapiro-Wilk</strong> (Shapiro e Wilk, 1965): Recomendado para amostras pequenas a moderadas (<span class="math inline">\(n \lesssim 200\)</span>)</li>
<li><strong>Kolmogorov-Smirnov</strong> (Kolmogorov, 1933; Smirnov, 1948): Baseado na máxima diferença entre funções de distribuição empírica e teórica</li>
<li><strong>Lilliefors</strong> (Lilliefors, 1967): Modificação do teste de Kolmogorov-Smirnov quando parâmetros são estimados</li>
<li><strong>Anderson-Darling</strong> (Anderson e Darling, 1954): Dá maior peso às caudas da distribuição</li>
<li><strong>Jarque-Bera</strong> (Jarque e Bera, 1987): Baseado em assimetria e curtose amostrais</li>
</ul>
<p><strong>Outros testes:</strong></p>
<ul>
<li><strong>Teste <span class="math inline">\(K^2\)</span> de D’Agostino</strong> (D’Agostino, 1970)</li>
<li><strong>Teste de Cramér-von Mises</strong> (Cramér, 1946; von Mises, 1928)</li>
<li><strong>Teste de Shapiro-Francia</strong> (Shapiro e Francia, 1972)</li>
<li><strong>Teste <span class="math inline">\(\chi^2\)</span> de Pearson</strong> (Pearson, 1900)</li>
<li>Teste de correlação linear entre resíduos ordenados e quantis teóricos</li>
</ul>
<p><br></p>
<pre><code>## 
## Testes formais de normalidade:
## 
## Teste de Shapiro-Wilk:
##   Estatística W = 0.9009 
##   p-valor = 0.3795 
## 
## Teste de Lilliefors (Kolmogorov-Smirnov):
##   Estatística D = 0.2232 
##   p-valor = 0.4584 
## 
## Teste de Anderson-Darling:
##   Não aplicável (requer n &gt; 7)
## 
## Interpretação: p-valor &gt; 0,05 indica não rejeição de H₀ (normalidade)</code></pre>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-237-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuos
## W = 0.9, p-value = 0.4
## 
## 
##  Lilliefors (Kolmogorov-Smirnov) normality test
## 
## data:  residuos
## D = 0.22, p-value = 0.5
## 
## 
## Teste de Anderson-Darling: não aplicável (requer n &gt; 7)</code></pre>
<hr />
</div>
<div id="observações-inconsistentes" class="section level3 hasAnchor" number="13.6.6">
<h3><span class="header-section-number">13.6.6</span> Observações Inconsistentes<a href="#observa%C3%A7%C3%B5es-inconsistentes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p><strong>Outliers</strong> são observações que se afastam substancialmente do padrão geral dos dados. Contudo, nem toda observação distante do centroide dos dados necessariamente é uma observação problemática: é fundamental distinguir observações <strong>naturalmente extremas</strong> de erros que comprometam a validade do modelo.</p>
<p><br></p>
<p>Observações extremas podem originar-se de:</p>
<ul>
<li>Erros não amostrais: falhas na coleta, registro ou processamento dos dados (instrumentos descalibrados, erros de transcrição, observadores diferentes, equipamentos diversos)</li>
<li>Variabilidade natural do fenômeno: observações legítimas que refletem eventos raros, mas genuínos, do processo estudado</li>
</ul>
<p><br></p>
<p>A distinção entre essas situações é crucial: enquanto <strong>erros não amostrais</strong> devem ser corrigidos ou excluídos quando identificados, <strong>observações extremas legítimas</strong> fornecem informação valiosa sobre a variabilidade do fenômeno e não devem ser descartadas arbitrariamente.</p>
<p><br></p>
<p>Embora a identificação de observações inconsistentes deva iniciar na análise exploratória dos dados (EDA), muitos erros só se tornam evidentes <strong>após</strong> o ajuste do modelo, quando a análise dos resíduos revela padrões anômalos.</p>
<p><br></p>
<p><em>Outliers</em> devem ser removidos da análise <strong>apenas</strong> quando há evidência objetiva de que representam erros de coleta, registro, cálculo ou circunstâncias análogas.</p>
<p><br></p>
<p>Observações extremas que refletem variabilidade genuína do fenômeno <strong>não devem</strong> ser descartadas, mesmo que exerçam influência no modelo, pois sua remoção resultaria em perda de informação relevante sobre o processo estudado.</p>
<p><br></p>
<p>Uma técnica para se verificar a presença de observações inconsistentes é através dos seguintes gráficos:</p>
<ul>
<li>Gráfico de caixa ( <em>box-plot</em>) dos resíduos padronizados: identifica observações afastadas da distribuição central por meio de análise univariada<br />
</li>
<li>Resíduos padronizados <em>versus</em> valores ajustados: permite identificar simultaneamente outliers (<span class="math inline">\(|d_i| &gt; 3\)</span>) e avaliar homocedasticidade<br />
</li>
<li>Resíduos padronizados <em>versus</em> variável preditora: complementa a análise anterior, revelando se outliers concentram-se em regiões específicas de <span class="math inline">\(X\)</span><br />
</li>
<li>Resíduos estudentizados <em>versus</em> valores ajustados: mais sensíveis para detectar pontos de alta alavancagem e influência (<span class="math inline">\(∣r_i∣&gt;3\)</span>)</li>
</ul>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-238-1.png" width="672" style="display: block; margin: auto;" /></p>
<hr />
</div>
<div id="observações-influentes" class="section level3 hasAnchor" number="13.6.7">
<h3><span class="header-section-number">13.6.7</span> Observações Influentes<a href="#observa%C3%A7%C3%B5es-influentes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Uma observação é considerada <strong>influente</strong> quando sua presença ou ausência no conjunto de dados altera substancialmente as estimativas dos parâmetros do modelo. A detecção de pontos influentes é crucial devido ao impacto que causam no ajuste do modelo e nos pressupostos, devendo ser analisada em conjunto com as análises de resíduo. A presença de outliers pode:</p>
<p><br></p>
<ul>
<li>Distorcer as estimativas dos coeficientes<br />
</li>
<li>Inflar a variância residual<br />
</li>
<li>Violar pressupostos do modelo (normalidade, homocedasticidade)</li>
</ul>
<p><br></p>
<p>A matriz <span class="math inline">\(\boldsymbol{H}\)</span> (matriz Hat ou matriz chapéu) tem importante papel no diagnóstico de pontos influentes, determinando as variâncias e covariâncias do valor estimado <span class="math inline">\((\hat{Y})\)</span> e dos erros. Os elementos <span class="math inline">\(h_{ii}\)</span> da diagonal da matriz de projeção <span class="math inline">\(H\)</span> são escritos como:</p>
<p><br></p>
<p><span class="math display">\[
h_{ii} = \boldsymbol{x_i&#39;(X&#39;X})^{-1}x_i
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(\boldsymbol{x&#39;_i}\)</span> é o vetor transposto da <span class="math inline">\(i\)</span>-ésima observação do centro no espaço das variáveis preditoras.</p>
<p><br></p>
<p>Há diferentes opiniões sobre os valores críticos para essa medida:</p>
<p><br></p>
<ul>
<li><span class="math inline">\(h_{ii}&gt;2\frac{p}{n}\)</span> (Hoaglin, D. C. and Welsch, R. E, 1978. The hat matrix in regression and ANOVA)</li>
<li><span class="math inline">\(h_{ii}&gt; 3\frac{p}{n}\)</span></li>
</ul>
<p><br></p>
<p>em que <span class="math inline">\(p\)</span> é o número de parâmetros estimados no modelo (<span class="math inline">\(\hat{\beta_{0}}\)</span> e <span class="math inline">\(\hat{\beta_{1}}\)</span>: 2 para uma regressão linear simples). Tradicionalmente assume-se que valores de <span class="math inline">\(h_{ii}\)</span> que excedem <span class="math inline">\((2p/n)\)</span> indicam pontos influentes que merecem investigação mais detalhada.</p>
<p><br></p>
<p>David Sam Jayakumar e A. Sulthan (Exact distribution of Hat Values and Identification of Leverage Points, 2014) propuseram a distribuição teóricas exata para os valores da diagonal da matriz de projeção <a href="http://www.researchgate.net/publication/265423863">link de acesso ao recurso</a>.</p>
<p><img src="apostila_files/figure-html/unnamed-chunk-239-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## 
## Pontos de alavanca (leverage):
## Limite de Hoaglin (2p/n): 0.6667 
## Limite alternativo (3p/n): 1 
## 
## Valores de leverage:
##      1      2      3      4      5      6 
## 0.7112 0.1747 0.1912 0.1792 0.2647 0.4792 
## 
## Pontos com leverage alto (&gt; 0.6667 ):
## 1 
## 1</code></pre>
<hr />
<div id="dfbetas-difference-in-betas" class="section level4 hasAnchor" number="13.6.7.1">
<h4><span class="header-section-number">13.6.7.1</span> DFBetas (<em>Difference in Betas</em>)<a href="introducao-a-regressao-linear.html#dfbetas-difference-in-betas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>A estatística DFBetas indica o quanto cada coeficiente de regressão <span class="math inline">\(\hat{\beta}_j\)</span> se modifica, em unidades de desvio padrão, se a <span class="math inline">\(i\)</span>-ésima observação for removida, refletindo a taxa de mudança provocada em <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> pela retirada da observação <span class="math inline">\(i\)</span>:</p>
<p><br></p>
<p><span class="math display">\[
DFBeta_{j,i} = \frac{\hat{\beta}_j - \hat{\beta}_{j(i)}}{\sqrt{S_{(i)}^2 C_{jj}}}
\]</span></p>
<p><br></p>
<p>com <span class="math inline">\(i = 1, 2, \ldots, n\)</span>, sendo <span class="math inline">\(C_{jj}\)</span> o <span class="math inline">\(j\)</span>-ésimo elemento da diagonal da matriz <span class="math inline">\(\boldsymbol{C = (X&#39;X)}^{-1}\)</span> e:</p>
<p><br></p>
<p><span class="math display">\[
S_{(i)}^2 = \frac{(n-p-1)QMRes - e_i^2(1-h_{ii})}{(n-p-1)}
\]</span></p>
<p><br></p>
<p>onde <span class="math inline">\(h_{ii}\)</span> é o <span class="math inline">\(i\)</span>-ésimo componente da diagonal da matriz <span class="math inline">\(H\)</span>.</p>
<p><br></p>
<p>A observação <span class="math inline">\(i\)</span> tem influência no <span class="math inline">\(j\)</span>-ésimo coeficiente de regressão quando <span class="math inline">\(|DFBeta_{j,i}| &gt; \frac{2}{\sqrt{n}}\)</span>, valores esses que requerem exame mais detalhado.</p>
<p><br></p>
<pre><code>## 
## ========================================
## ANÁLISE DE DFBetas
## ========================================
## Limite crítico (2/√n): 0.8165 
## Número de observações: 6 
## Número de parâmetros: 2 
## Observações influentes: 0 
## 
## Nenhuma observação excede o limite crítico.</code></pre>
<hr />
</div>
<div id="dffits-difference-in-fits" class="section level4 hasAnchor" number="13.6.7.2">
<h4><span class="header-section-number">13.6.7.2</span> DFFits (<em>Difference in fits</em>)<a href="introducao-a-regressao-linear.html#dffits-difference-in-fits" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>A estatística DFFits investiga a influência da <span class="math inline">\(i\)</span>-ésima observação nos valores ajustados (preditos), refletindo o quanto o valor ajustado se modifica, em unidades de desvio padrão, se a <span class="math inline">\(i\)</span>-ésima observação for removida:</p>
<p><br></p>
<p><span class="math display">\[
DFFits_i = \frac{\hat{Y}_i - \hat{Y}_{(i)}}{\sqrt{S_{(i)}^2 h_{ii}}} = \left(\frac{h_{ii}}{1-h_{ii}}\right)^{1/2} r_i
\]</span></p>
<p><br></p>
<p>com <span class="math inline">\(i = 1, 2, \ldots, n\)</span>, em que <span class="math inline">\(r_i\)</span> são os resíduos estudentizados, <span class="math inline">\(\hat{Y}_i\)</span> é o valor predito e <span class="math inline">\(\hat{Y}_{(i)}\)</span> é o valor predito sem o uso da <span class="math inline">\(i\)</span>-ésima observação.</p>
<p><br></p>
<p>Observações com <span class="math inline">\(|DFFits_i| &gt; 2\sqrt{\frac{p}{n}}\)</span> requerem exame mais detalhado.</p>
<p><br></p>
<pre><code>## 
## ========================================
## ANÁLISE DE DFFits
## ========================================
## Limite crítico (2√(p/n)): 1.155 
## Número de observações: 6 
## Número de parâmetros: 2 
## Observações influentes: 0 
## 
## Nenhuma observação excede o limite crítico.</code></pre>
<hr />
</div>
<div id="distância-de-cook" class="section level4 hasAnchor" number="13.6.7.3">
<h4><span class="header-section-number">13.6.7.3</span> Distância de Cook:<a href="#dist%C3%A2ncia-de-cook" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>A estatística proposta por Dennis R. Cook mede a influência de uma observação sobre as estimativas dos parâmetros do modelo, quantificando o quanto a linha de regressão se alteraria caso esse dado fosse removido da análise. É uma medida da distância ao quadrado entre a estimativa usual de mínimos quadrados de <span class="math inline">\(\boldsymbol{\beta}\)</span>, baseada em todas observações, e a estimativa obtida quando o <span class="math inline">\(i\)</span>-ésimo ponto for removido, <span class="math inline">\(\hat{\boldsymbol{\beta}}_{(i)}\)</span>:</p>
<p><br></p>
<p><span class="math display">\[
D_i = \frac{(\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_{(i)})&#39;(X&#39;X)(\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_{(i)})}{p\hat{\sigma}^2}
\]</span></p>
<p><br></p>
<p>com <span class="math inline">\(i = 1, 2, \ldots, n\)</span>.</p>
<p><br></p>
<p>A medida da distância de Cook pode também ser expressa por:</p>
<p><br></p>
<p><span class="math display">\[
D_i = \frac{r_i^2}{p} \cdot \frac{h_{ii}}{(1-h_{ii})}
\]</span></p>
<p><br></p>
<p>com <span class="math inline">\(i = 1, 2, \ldots, n\)</span>, onde <span class="math inline">\(D_i\)</span> consiste no quadrado do resíduo estudentizado, refletindo quão bem o modelo se ajusta à <span class="math inline">\(i\)</span>-ésima observação <span class="math inline">\(Y_i\)</span>, e o termo <span class="math inline">\((h_{ii}/(1-h_{ii}))\)</span> é uma medida de distância do <span class="math inline">\(i\)</span>-ésimo ponto do centroide dos <span class="math inline">\((n-1)\)</span> pontos restantes.</p>
<p><br></p>
<p>Na regressão múltipla pode ocorrer de algum subconjunto de observações influenciarem por estarem longe da vizinhança onde o restante dos dados foi coletado, afetando a determinação do <span class="math inline">\(R^2\)</span>, as estimativas dos coeficientes de regressão e a magnitude da média quadrática dos erros. A influência na locação pode ser investigada pelo gráfico das distâncias de Cook contra os valores ajustados.</p>
<p><br></p>
<p>Há vários critérios para se definir um valor limite para a estatística de Cook:</p>
<ul>
<li><span class="math inline">\(D_{i}&gt;1\)</span>: Cook e Weisberg, 1982 e Chatterjee, Hadi e Price, 2000<br />
</li>
<li>Duas vezes a média das distâncias de Cook<br />
</li>
<li><span class="math inline">\(D_{i}&gt; \frac{4}{n}\)</span>: Bollen et al, 1990<br />
</li>
<li>Valor crítico do quantil da distribuição F para uma significância igual a 0.5 com df1=p e df2=n-p</li>
</ul>
<p>Quando <span class="math inline">\(D_i\)</span> for alto, indica que o <span class="math inline">\(i\)</span>-ésimo ponto exerce influência; se <span class="math inline">\(D_i &gt; 1\)</span>, indica que o ponto exerce grande influência.</p>
<p><br></p>
<pre><code>## 
## ========================================
## ANÁLISE DE DISTÂNCIA DE COOK
## ========================================
## Limite crítico (D_i &gt; 1): 1 
## Número de observações: 6 
## Número de parâmetros: 2 
## Observações influentes: 0 
## 
## Nenhuma observação excede o limite crítico.</code></pre>
<hr />
</div>
<div id="influência-na-precisão-da-estimação" class="section level4 hasAnchor" number="13.6.7.4">
<h4><span class="header-section-number">13.6.7.4</span> Influência na Precisão da Estimação<a href="#influ%C3%AAncia-na-precis%C3%A3o-da-estima%C3%A7%C3%A3o" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>As medidas <span class="math inline">\(D_i\)</span>, <span class="math inline">\(DFBetas_{j,i}\)</span> e <span class="math inline">\(DFFits_i\)</span> fornecem uma visão do efeito de cada observação nos coeficientes estimados e nos valores ajustados. Não fornecem, contudo, qualquer informação sobre a precisão geral da estimação. Para expressar o papel da <span class="math inline">\(i\)</span>-ésima observação na precisão da estimação, utiliza-se a medida:</p>
<p><br></p>
<p><span class="math display">\[
COVRATIO_i = \frac{|(\boldsymbol{X&#39;_{(i)}X_{(i)}})^{-1}S_{(i)}^2|}{|(\boldsymbol{X&#39;X})^{-1}QMRes|} = \left(\frac{S_i^2}{QMres}\right)^p \cdot \frac{1}{1-h_{ii}}
\]</span></p>
<p><br></p>
<p><span class="math inline">\(i = 1, 2, \ldots, n\)</span> em que <span class="math inline">\(h_{ii}\)</span> é o <span class="math inline">\(i\)</span>-ésimo componente da diagonal da matriz <span class="math inline">\(\boldsymbol{H}\)</span>.</p>
<p><br></p>
<p>Belsley et al. (1980) sugeriram que se: <span class="math inline">\(COVRATIO_i &gt; 1 + 3p/n\)</span> ou se <span class="math inline">\(COVRATIO_i &lt; 1 - 3p/n\)</span>, então, o <span class="math inline">\(i\)</span>-ésimo ponto é um possível ponto influente. Em geral, esses pontos de corte são mais apropriados para amostras grandes.</p>
<p><br></p>
<pre><code>## 
## ========================================
## ANÁLISE DE COVRATIO
## ========================================
## Limite superior (1 + 3p/n): 2 
## Limite inferior (1 - 3p/n): 0 
## Número de observações: 6 
## Número de parâmetros: 2 
## Observações influentes: 3 
## 
## Observações que excedem os limites:
##   Obs 1: COVRATIO = 5.9840
##   Obs 3: COVRATIO = 2.0584
##   Obs 6: COVRATIO = 2.3159</code></pre>
<hr />
</div>
</div>
<div id="multicolinearidade-fator-de-inflação-da-variância-vif" class="section level3 hasAnchor" number="13.6.8">
<h3><span class="header-section-number">13.6.8</span> Multicolinearidade: Fator de Inflação da Variância (VIF)<a href="#multicolinearidade-fator-de-infla%C3%A7%C3%A3o-da-vari%C3%A2ncia-vif" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Na <strong>regressão linear múltipla</strong>, além das dependências entre a variável resposta <span class="math inline">\(Y\)</span> e os regressores <span class="math inline">\(X_j\)</span>, frequentemente ocorrem dependências entre os próprios regressores. Quando duas variáveis preditoras apresentam correlação perfeita, a matriz <span class="math inline">\((X^TX)\)</span> torna-se singular (determinante = 0), impedindo a estimação dos coeficientes por mínimos quadrados.</p>
<p><br></p>
<pre><code>## A correlação entre as variáveis é: 1 
## No ajuste do modelo, o R remove automaticamente variáveis redundantes (nota `NA` no seu coeficiente)</code></pre>
<p><br></p>
<p>Consequências</p>
<p><br></p>
<ul>
<li>Coeficientes indeterminados (infinitas soluções)</li>
<li>Variância infinita dos estimadores</li>
<li>Inferência estatística impossível</li>
</ul>
<p><br></p>
<p>A presença da multicolinearidade pode ser detectada de várias maneiras:</p>
<p><br></p>
<ul>
<li><strong>Critério do VIF</strong>: quanto maior o VIF, mais severa é a multicolinearidade. Valores de <span class="math inline">\(VIF_j &gt; 10\)</span> indicam multicolinearidade problemática, enquanto <span class="math inline">\(VIF_j &gt; 5\)</span> já sugere atenção.<br />
</li>
<li><strong>Inconsistência nos testes</strong>: se o teste F para significância global da regressão for significativo, mas os testes t individuais dos coeficientes não forem significativos, então a multicolinearidade está presente.<br />
</li>
<li><strong>Sinais dos coeficientes</strong>: coeficientes com sinais contrários à expectativa teórica podem indicar multicolinearidade.</li>
</ul>
<p><br></p>
<p>Soluções:</p>
<p><br></p>
<ul>
<li>Aumentar o tamanho amostral com novas observações especificamente projetadas para fragmentar as dependências lineares</li>
<li>Remover variáveis altamente correlacionadas do modelo (com a desvantagem de perder informação)</li>
<li>Utilizar métodos alternativos de estimação menos sensíveis à multicolinearidade, como Regressão Ridge ou Regressão de Componentes Principais</li>
</ul>
<p><br></p>
<p>Não situações onde a multicolinearidade não é perfeita, uma medida de sua intensidade é dada pelo Fator de Inflação da Variância (Variance Inflation Factor-VIF). O VIF quantifica a multicolinearidade de cada preditor:</p>
<p><br></p>
<p><span class="math display">\[
VIF_j = \frac{1}{1 - R_j^2}
\]</span></p>
<p><br></p>
<p>onde <span class="math inline">\(R_j^2\)</span> é o coeficiente de determinação da regressão de <span class="math inline">\(x_j\)</span> contra os demais preditores.</p>
<p><br></p>
<p>Interpretação</p>
<ul>
<li>VIF = 1: Sem correlação</li>
<li>VIF &lt; 5: Aceitável</li>
<li>VIF ≥ 10: Multicolinearidade problemática</li>
</ul>
<p><br></p>
<p>Por que “Inflação”?</p>
<p><br></p>
<p><span class="math display">\[
\text{Var}(\hat{\beta}_j) = \frac{\sigma^2}{(n-1)s_j^2} \cdot VIF_j
\]</span></p>
<p><br></p>
<p>O VIF multiplica a variância do estimador.</p>
<p><br></p>
<pre><code>## 
## ========================================
## O modelo ajustado é: 
## 
## Call:
## lm(formula = y ~ x1 + x2 + x3)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.794 -0.587 -0.104  0.619  2.328 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   2.0315     0.0891   22.79   &lt;2e-16 ***
## x1            2.9695     0.9964    2.98   0.0037 ** 
## x2            0.0883     0.9890    0.09   0.9291    
## x3            3.9684     0.0888   44.68   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.887 on 96 degrees of freedom
## Multiple R-squared:  0.968,  Adjusted R-squared:  0.967 
## F-statistic:  953 on 3 and 96 DF,  p-value: &lt;2e-16
## 
## 
## ========================================
## Os VIF das variáveis são: 
##      x1      x2      x3 
## 135.569 135.292   1.027 
## 
## ========================================
## A correlação linear par a par das variáveis é: 
##         x1      x2      x3
## x1  1.0000  0.9963 -0.1448
## x2  0.9963  1.0000 -0.1377
## x3 -0.1448 -0.1377  1.0000
## 
## ========================================</code></pre>
<p><br></p>
<p>Observações:</p>
<ul>
<li>o <em>VIF</em> só se aplica a modelos de regressão múltipla (com duas ou mais variáveis preditoras). Em regressão linear simples, não há razão para se falar em multicolinearidade.</li>
<li>ele não indica qual variável remover</li>
</ul>
<p><br></p>
<pre><code>## 
## ========================================
## FATOR DE INFLAÇÃO DA VARIÂNCIA (VIF)
## ========================================
## Critério: VIF &gt; 10 indica multicolinearidade severa
##          VIF &gt; 5 sugere atenção
## 
##      x1      x2      x3 
## 135.569 135.292   1.027 
## 
## Variáveis com multicolinearidade severa (VIF &gt; 10):
##  - x1 : 135.57 
##   - x2 : 135.29</code></pre>
<hr />
</div>
</div>
<div id="métricas-de-aferição-da-qualidade-de-ajuste-de-uma-regressão-linear" class="section level2 hasAnchor" number="13.7">
<h2><span class="header-section-number">13.7</span> Métricas de Aferição da Qualidade de Ajuste de uma Regressão Linear<a href="#m%C3%A9tricas-de-aferi%C3%A7%C3%A3o-da-qualidade-de-ajuste-de-uma-regress%C3%A3o-linear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<hr />
<div id="coeficiente-de-determinação-r2-da-regressão-linear" class="section level3 hasAnchor" number="13.7.1">
<h3><span class="header-section-number">13.7.1</span> Coeficiente de determinação <span class="math inline">\(R^{2}\)</span> da Regressão Linear<a href="#coeficiente-de-determina%C3%A7%C3%A3o-r2-da-regress%C3%A3o-linear" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>O coeficiente de determinação (<span class="math inline">\(R^{2}\)</span>) é uma medida estatística que informa o quanto da variação observada na variável <span class="math inline">\(Y\)</span> está sendo explicada no modelo pela relação linear estabelecida com a variável <span class="math inline">\(X\)</span>.</p>
<p><br></p>
<p><span class="math display">\[
R^{2} = \frac{\text{variação explicada}}{\text{variação total}} \\
R^{2}=\frac{b\cdot S_{xy}}{{S}_{yy}}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
R^{2}_{ajustado} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
\]</span>
<br></p>
<p>em que <span class="math inline">\(n\)</span> é o número de observações, <span class="math inline">\(p\)</span> é o número de variáveis independentes.</p>
<p><br></p>
<pre><code>## 
## ========================================
## COEF. DE DETERMINAÇÃO
## ========================================
## R²: 0.9675 
## R² ajustado: 0.9665</code></pre>
<hr />
</div>
<div id="aic-akaike-information-criterion-e-rmse-root-mean-squared-error" class="section level3 hasAnchor" number="13.7.2">
<h3><span class="header-section-number">13.7.2</span> AIC (Akaike <em>Information Criterion</em>) e RMSE (<em>Root Mean Squared Error</em>)<a href="introducao-a-regressao-linear.html#aic-akaike-information-criterion-e-rmse-root-mean-squared-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>O AIC é um critério de informação que equilibra a qualidade do ajuste com a complexidade do modelo, penalizando a inclusão de parâmetros adicionais.</p>
<p><br></p>
<p>O RMSE representa a raiz quadrada da média dos quadrados dos resíduos, fornecendo uma medida da dispersão dos erros na mesma escala da variável resposta, facilitando a interpretação da magnitude dos erros de predição.</p>
<p><br></p>
<p><strong>Menores</strong> valores de AIC e de RMSE indicam melhor modelo, sendo particularmente úteis na comparação.</p>
<p><br></p>
<pre><code>## 
## ========================================</code></pre>
<pre><code>## CRITÉRIOS DE QUALIDADE DO MODELO</code></pre>
<pre><code>## ========================================</code></pre>
<pre><code>## AIC: 265.7</code></pre>
<pre><code>## RMSE: 0.8688</code></pre>
<hr />
</div>
</div>
<div id="testes-de-hipóteses-da-regressão-linear" class="section level2 hasAnchor" number="13.8">
<h2><span class="header-section-number">13.8</span> Testes de Hipóteses da Regressão Linear<a href="#testes-de-hip%C3%B3teses-da-regress%C3%A3o-linear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<hr />
<div id="teste-f-análise-de-variância-da-regressão" class="section level3 hasAnchor" number="13.8.1">
<h3><span class="header-section-number">13.8.1</span> Teste F (Análise de Variância da Regressão)<a href="#teste-f-an%C3%A1lise-de-vari%C3%A2ncia-da-regress%C3%A3o" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>A análise de variância comprova estatisticamente se os dados apresentam a suposta relação linear entre as variáveis preditoras e a variável resposta <span class="math inline">\(Y\)</span>. Um procedimento estatístico para se comprovar se os dados apresentam a suposta relação linear entre as variáveis é testar a hipótese:</p>
<p><br></p>
<p><span class="math display">\[
\begin{cases}
H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0  \hspace{0.5cm}  \text{ie., não há regressão} \\
H_1: \text{pelo menos um } \beta_j \neq 0
\end{cases}
\]</span></p>
<p><br></p>
<p>em que sob <span class="math inline">\(H_0\)</span> a estatística do teste <span class="math inline">\(F_{calc}\)</span> pode ser calculada pela Tabela a seguir:</p>
<p><br></p>
<table style="width:100%;">
<colgroup>
<col width="24%" />
<col width="25%" />
<col width="24%" />
<col width="20%" />
<col width="3%" />
</colgroup>
<thead>
<tr class="header">
<th>Causa de Variação</th>
<th>Graus de Liberdade</th>
<th>Soma de Quadrados</th>
<th>Quadrado Médio</th>
<th>F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regressão</td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\(SQReg\)</span></td>
<td><span class="math inline">\(QMReg = \frac{SQReg}{p}\)</span></td>
<td><span class="math inline">\(\frac{QMReg}{QMRes}\)</span></td>
</tr>
<tr class="even">
<td>Resíduo</td>
<td><span class="math inline">\(n-p-1\)</span></td>
<td><span class="math inline">\(SQRes\)</span></td>
<td><span class="math inline">\(QMRes = \frac{SQRes}{n-p-1}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(n-1\)</span></td>
<td><span class="math inline">\(SQTotal\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><br></p>
<p>em que <span class="math inline">\(F_{calc}\)</span> segue a distribuição F de Fisher-Snedecor com <span class="math inline">\(p\)</span> graus de liberdade no numerador e <span class="math inline">\((n-p-1)\)</span> graus de liberdade no denominador, e <span class="math inline">\(p\)</span> é o número de variáveis independentes no modelo.</p>
<p><br></p>
<blockquote>
<p>Conclusão: se <span class="math inline">\(F_{calc} \geq F_{tab}[p, (n-p-1); \alpha]\)</span>, rejeita-se <span class="math inline">\(H_0\)</span> ao nível de significância adotado, e conclui-se que existe regressão significativa, em que <span class="math inline">\(F_{tab}[p, (n-p-1); \alpha]\)</span> é o quantil <span class="math inline">\((1-\alpha)\)</span> da distribuição F de Fisher-Snedecor com <span class="math inline">\(p\)</span> graus de liberdade no numerador e <span class="math inline">\((n-p-1)\)</span> graus de liberdade no denominador.</p>
</blockquote>
<p><br></p>
<p>Observe que ao realizar a análise de variância, o procedimento é particionar a variação total em duas componentes:</p>
<p><br></p>
<p><span class="math display">\[
\underbrace{\sum_{i=1}^{n}(y_i - \bar{y})^2}_{SQTotal} = \underbrace{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}_{SQRes} + \underbrace{\sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2}_{SQReg}
\]</span></p>
<p><br></p>
<p>ou seja, <span class="math inline">\(SQTotal = SQRes + SQReg\)</span>, onde:</p>
<p><br></p>
<ul>
<li><span class="math inline">\(SQTotal\)</span>: variação total de <span class="math inline">\(Y\)</span> em torno da média</li>
<li><span class="math inline">\(SQRes\)</span>: variação de <span class="math inline">\(Y\)</span> em torno dos valores ajustados (variação não explicada)</li>
<li><span class="math inline">\(SQReg\)</span>: variação dos valores ajustados em torno da média (variação explicada pelo modelo)</li>
</ul>
<p><br></p>
<p>Portanto:</p>
<p><br></p>
<p><span class="math display">\[
SQTotal = \sum_{i=1}^{n}(y_i - \bar{y})^2
\]</span></p>
<p><br></p>
<p><span class="math display">\[
SQReg = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2
\]</span></p>
<p><br></p>
<p><span class="math display">\[
SQRes = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = SQTotal - SQReg
\]</span></p>
<p><br></p>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## x1         1    669     669   851.2 &lt; 2e-16 ***
## x2         1     10      10    12.4 0.00066 ***
## x3         1   1569    1569  1996.1 &lt; 2e-16 ***
## Residuals 96     75       1                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><br></p>
<p>Para regressão linear simples (<span class="math inline">\(p=1\)</span>), os graus de liberdade da regressão são 1 e do resíduo <span class="math inline">\(n-2\)</span>, e o teste F é equivalente ao teste t do coeficiente (<span class="math inline">\(F = t^2\)</span>).</p>
<hr />
</div>
<div id="testes-t-para-os-parâmetros-da-regressão" class="section level3 hasAnchor" number="13.8.2">
<h3><span class="header-section-number">13.8.2</span> Testes ``t’’ para os Parâmetros da Regressão<a href="#testes-t-para-os-par%C3%A2metros-da-regress%C3%A3o" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Para testar hipóteses sobre o intercepto (coeficiente linear) <span class="math inline">\((\alpha)\)</span> e o coeficiente angular <span class="math inline">\((\beta)\)</span> do modelo de regressão, deve-se fazer a suposição de que os <span class="math inline">\((e_i)\)</span> são normalmente distribuídos, ou seja, assume-se que os erros <span class="math inline">\(e_i \sim NID(0, \sigma^2)\)</span>.</p>
<hr />
<div id="coeficiente-linear-alpha" class="section level4 hasAnchor" number="13.8.2.1">
<h4><span class="header-section-number">13.8.2.1</span> Coeficiente Linear (<span class="math inline">\(\alpha\)</span>)<a href="introducao-a-regressao-linear.html#coeficiente-linear-alpha" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>O procedimento para se testar a hipótese sobre o intercepto</p>
<p><br></p>
<p><span class="math display">\[
\begin{cases}
H_0: \alpha = \alpha_0   \\
H_1: \alpha \neq \alpha_0  
\end{cases}
\]</span></p>
<p><br></p>
<p>para o qual se usa a estatística <span class="math inline">\(t_{calc}\)</span>:</p>
<p><br></p>
<p><span class="math display">\[
t_{calc} = \frac{\hat{\alpha} - \alpha_0}{\sqrt{QMRes\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)}}
\]</span></p>
<p><br></p>
<p>que sob <span class="math inline">\(H_0\)</span> segue uma distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade.</p>
<p><br></p>
<blockquote>
<p>Conclusão: rejeita-se a hipótese nula se <span class="math inline">\(|t_{calc}| &gt; t_{\alpha/2;(n-p-1)}\)</span>, em que <span class="math inline">\(t_{\alpha/2;(n-p-1)}\)</span> é o quantil <span class="math inline">\((1-\alpha/2)\)</span> da distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade, para um nível de significância <span class="math inline">\(\alpha\)</span>.
Observação: para regressão linear simples, os graus de liberdade são <span class="math inline">\((n-2)\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="coeficiente-angular-beta_j" class="section level4 hasAnchor" number="13.8.2.2">
<h4><span class="header-section-number">13.8.2.2</span> Coeficiente Angular (<span class="math inline">\(\beta_j\)</span>)<a href="introducao-a-regressao-linear.html#coeficiente-angular-beta_j" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>Um procedimento similar pode ser usado para testar a hipótese sobre os coeficientes angulares. Para testar a hipótese de que o coeficiente angular <span class="math inline">\(\beta_j\)</span> é igual a um valor constante <span class="math inline">\(\beta_{j,0}\)</span>:</p>
<p><br></p>
<p><span class="math display">\[
\begin{cases}
H_0: \beta_j = \beta_{j,0}   \\
H_1: \beta_j \neq \beta_{j,0}  
\end{cases}
\]</span></p>
<p><br></p>
<p>usa-se a estatística teste <span class="math inline">\(t_{calc_j}\)</span>:</p>
<p><br></p>
<p><span class="math display">\[
t_{calc_j} = \frac{\hat{\beta}_j - \beta_{j,0}}{\sqrt{QMRes \cdot C_{jj}}}
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(C_{jj}\)</span> é o <span class="math inline">\(j\)</span>-ésimo elemento da diagonal da matriz <span class="math inline">\((X&#39;X)^{-1}\)</span>, que sob <span class="math inline">\(H_0\)</span> segue uma distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade.</p>
<p><br></p>
<blockquote>
<p>Conclusão: rejeita-se a hipótese nula se <span class="math inline">\(|t_{calc_j}| &gt; t_{\alpha/2;(n-p-1)}\)</span>, em que <span class="math inline">\(t_{\alpha/2;(n-p-1)}\)</span> é o quantil <span class="math inline">\((1-\alpha/2)\)</span> da distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade, para um nível de significância <span class="math inline">\(\alpha\)</span>.</p>
</blockquote>
<p><br></p>
<blockquote>
<p>Observação: um caso especial importante é testar <span class="math inline">\(H_0: \beta_j = 0\)</span> versus <span class="math inline">\(H_1: \beta_j \neq 0\)</span>, cuja hipótese está relacionada com a significância individual do preditor. Se <span class="math inline">\(H_0: \beta_j = 0\)</span> não for rejeitada, isto implica que a variável <span class="math inline">\(x_j\)</span> não contribui significativamente para o modelo na presença das demais variáveis.</p>
</blockquote>
<p><br></p>
<p>Para uma regressão linear simples a hipótese se reduz a</p>
<p><br></p>
<p><span class="math display">\[
\begin{cases}
H_0: \beta = \beta_0   \\
H_1: \beta \neq \beta_0  
\end{cases}
\]</span></p>
<p>e como <span class="math inline">\(C_{11} = 1/S_{xx}\)</span>, os graus de liberdade são <span class="math inline">\((n-2)\)</span>, e a estatística de teste se reduz a:</p>
<p><span class="math display">\[
t_{calc} = \frac{\hat{\beta} - \beta_0}{\sqrt{QMRes/S_{xx}}}
\]</span></p>
<blockquote>
<p>Conclusão: rejeita-se a hipótese nula se <span class="math inline">\(|t_{calc}| &gt; t_{\alpha/2;(n-2)}\)</span>, em que <span class="math inline">\(t_{\alpha/2;(n-2)}\)</span> é o quantil <span class="math inline">\((1-\alpha/2)\)</span> da distribuição t de Student com <span class="math inline">\((n-2)\)</span> graus de liberdade, para um nível de significância <span class="math inline">\(\alpha\)</span>.</p>
</blockquote>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2 + x3)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.794 -0.587 -0.104  0.619  2.328 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   2.0315     0.0891   22.79   &lt;2e-16 ***
## x1            2.9695     0.9964    2.98   0.0037 ** 
## x2            0.0883     0.9890    0.09   0.9291    
## x3            3.9684     0.0888   44.68   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.887 on 96 degrees of freedom
## Multiple R-squared:  0.968,  Adjusted R-squared:  0.967 
## F-statistic:  953 on 3 and 96 DF,  p-value: &lt;2e-16</code></pre>
<hr />
</div>
</div>
</div>
<div id="intervalos-de-confiança-para-os-parâmetros-da-regressão-linear" class="section level2 hasAnchor" number="13.9">
<h2><span class="header-section-number">13.9</span> Intervalos de Confiança para os Parâmetros da Regressão Linear<a href="#intervalos-de-confian%C3%A7a-para-os-par%C3%A2metros-da-regress%C3%A3o-linear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br></p>
<blockquote>
<p><strong>Nota</strong>: apenas nesta seção adotaremos <span class="math inline">\(\gamma\)</span> para o nível de significância do intervalo de confiança de modo a não ser confundido com o coeficiente linear (<span class="math inline">\(\alpha\)</span>) do modelo.</p>
</blockquote>
<hr />
<div id="coeficiente-linear-alpha-1" class="section level3 hasAnchor" number="13.9.1">
<h3><span class="header-section-number">13.9.1</span> Coeficiente Linear (<span class="math inline">\(\alpha\)</span>)<a href="introducao-a-regressao-linear.html#coeficiente-linear-alpha-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>O intervalo de confiança de <span class="math inline">\((1-\gamma)100\%\)</span> para o coeficiente linear é dado por:</p>
<p><br></p>
<p><span class="math display">\[
IC(\alpha; (1-\gamma))  = a \pm t_{\gamma/2;(n-p-1)} \sqrt{QMRes\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)}
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(t_{\gamma/2;(n-p-1)}\)</span> é o quantil <span class="math inline">\((1-\gamma/2)\)</span> da distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade. Para regressão linear simples, os graus de liberdade são <span class="math inline">\((n-2)\)</span>.</p>
<hr />
</div>
<div id="coeficiente-angular-beta_j-1" class="section level3 hasAnchor" number="13.9.2">
<h3><span class="header-section-number">13.9.2</span> Coeficiente Angular (<span class="math inline">\(\beta_j\)</span>)<a href="introducao-a-regressao-linear.html#coeficiente-angular-beta_j-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>O intervalo de confiança de <span class="math inline">\((1-\gamma)100\%\)</span> para o coeficiente angular <span class="math inline">\(\beta_j\)</span> é dado por:</p>
<p><br></p>
<p><span class="math display">\[
IC(\beta_j; 1-\gamma) = \hat{\beta}_j \pm t_{\gamma/2;(n-p-1)} \sqrt{QMRes \cdot C_{jj}}
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(C_{jj}\)</span> é o <span class="math inline">\(j\)</span>-ésimo elemento da diagonal da matriz <span class="math inline">\((X&#39;X)^{-1}\)</span> e <span class="math inline">\(t_{\gamma/2;(n-p-1)}\)</span> é o quantil <span class="math inline">\((1-\gamma/2)\)</span> da distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade. Para regressão linear simples, <span class="math inline">\(C_{11} = 1/S_{xx}\)</span>, os graus de liberdade são <span class="math inline">\((n-2)\)</span>, e o intervalo se reduz a:</p>
<p><br></p>
<p><span class="math display">\[
IC(\beta; 1-\gamma) = b \pm t_{\gamma/2;(n-2)} \sqrt{\frac{QMRes}{S_{xx}}}
\]</span></p>
<pre><code>##               2.5 % 97.5 %
## (Intercept)  1.8546  2.208
## x1           0.9915  4.947
## x2          -1.8749  2.052
## x3           3.7921  4.145</code></pre>
<hr />
</div>
<div id="variância-sigma2" class="section level3 hasAnchor" number="13.9.3">
<h3><span class="header-section-number">13.9.3</span> Variância (<span class="math inline">\(\sigma^2\)</span>)<a href="#vari%C3%A2ncia-sigma2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Se os erros são normalmente e independentemente distribuídos, a distribuição amostral de <span class="math inline">\(\frac{(n-p-1)QMRes}{\sigma^2}\)</span> é qui-quadrado com <span class="math inline">\((n-p-1)\)</span> graus de liberdade, ou seja:</p>
<p><br></p>
<p><span class="math display">\[
P\left\{\chi^2_{1-\gamma/2;(n-p-1)} \leq \frac{(n-p-1)QMRes}{\sigma^2} \leq \chi^2_{\gamma/2;(n-p-1)}\right\} = (1-\gamma)
\]</span></p>
<p><br></p>
<p>e consequentemente, o intervalo de confiança de <span class="math inline">\((1-\gamma)100\%\)</span> para <span class="math inline">\(\sigma^2\)</span> é dado por:</p>
<p><br></p>
<p><span class="math display">\[
IC(\sigma^2; 1-\gamma) = \left[\frac{(n-p-1)QMRes}{\chi^2_{\gamma/2;(n-p-1)}}, \frac{(n-p-1)QMRes}{\chi^2_{1-\gamma/2;(n-p-1)}}\right]
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(\chi^2_{\gamma/2;(n-p-1)}\)</span> e <span class="math inline">\(\chi^2_{1-\gamma/2;(n-p-1)}\)</span> são os quantis <span class="math inline">\((\gamma/2)\)</span> e <span class="math inline">\((1-\gamma/2)\)</span> da distribuição qui-quadrado com <span class="math inline">\((n-p-1)\)</span> graus de liberdade, respectivamente. Para regressão linear simples, os graus de liberdade são <span class="math inline">\((n-2)\)</span>.</p>
<p><br></p>
<pre><code>## 
## ========================================
## INTERVALO DE CONFIANÇA PARA σ²
## ========================================
## Graus de liberdade: 4 
## QMRes: 65.37 
## Nível de confiança: 95%
## IC(σ²; 0.95) = [ 23.47 ; 539.8 ]</code></pre>
<hr />
</div>
<div id="intervalo-de-confiança-para-eymathbfx" class="section level3 hasAnchor" number="13.9.4">
<h3><span class="header-section-number">13.9.4</span> Intervalo de Confiança para <span class="math inline">\(E[Y|\mathbf{X}]\)</span><a href="#intervalo-de-confian%C3%A7a-para-eymathbfx" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>A esperança de <span class="math inline">\(Y\)</span>, dados valores específicos <span class="math inline">\(\mathbf{x}_0\)</span> das variáveis regressoras, é dada por <span class="math inline">\(E[Y|\mathbf{x}_0] = \beta_0 + \beta_1 x_{01} + \beta_2 x_{02} + \ldots + \beta_p x_{0p}\)</span>, sob o modelo de regressão linear múltipla, onde <span class="math inline">\(\beta_0\)</span> é o intercepto e <span class="math inline">\(\beta_j\)</span> são os coeficientes angulares.</p>
<p><br></p>
<p>Na forma matricial, com <span class="math inline">\(\boldsymbol{\theta} = [\beta_0, \beta_1, \beta_2, \ldots, \beta_p]&#39;\)</span> sendo o vetor de parâmetros, pode-se definir um intervalo de confiança para <span class="math inline">\(E[Y|\mathbf{x}_0]\)</span> através da seguinte quantidade pivotal:</p>
<p><br></p>
<p><span class="math display">\[
\frac{\mathbf{x}_0&#39;\hat{\boldsymbol{\theta}} - \mathbf{x}_0&#39;\boldsymbol{\theta}}{\sqrt{QMRes \cdot \mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0}}
\]</span></p>
<p><br></p>
<p>que tem distribuição <span class="math inline">\(t\)</span> de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade, onde <span class="math inline">\(\mathbf{x}_0 = [1, x_{01}, x_{02}, \ldots, x_{0p}]&#39;\)</span> é o vetor de valores específicos das variáveis preditoras (incluindo o 1 para o intercepto).</p>
<p><br></p>
<p>Portanto, o intervalo de confiança de <span class="math inline">\((1-\gamma)100\%\)</span> para <span class="math inline">\(E[Y|\mathbf{x}_0]\)</span> é dado por:</p>
<p><br></p>
<p><span class="math display">\[
IC(E[Y|\mathbf{x}_0]; 1-\gamma) = \mathbf{x}_0&#39;\hat{\boldsymbol{\theta}} \pm t_{\gamma/2;(n-p-1)} \sqrt{QMRes \cdot \mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0}
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(t_{\gamma/2;(n-p-1)}\)</span> é o quantil <span class="math inline">\((1-\gamma/2)\)</span> da distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade.</p>
<p><br></p>
<p>A amplitude deste intervalo é:</p>
<p><br></p>
<p><span class="math display">\[
2 \cdot t_{\gamma/2;(n-p-1)} \sqrt{QMRes \cdot \mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0}
\]</span></p>
<p><br></p>
<p>Para regressão linear simples, <span class="math inline">\(\mathbf{x}_0 = [1, x_0]&#39;\)</span>, <span class="math inline">\(\hat{\boldsymbol{\theta}} = [\hat{\alpha}, \hat{\beta}]&#39;\)</span>, <span class="math inline">\(\mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0 = \frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}}\)</span>, os graus de liberdade são <span class="math inline">\((n-2)\)</span>, e o intervalo se reduz a:</p>
<p><br></p>
<p><span class="math display">\[
IC(E[Y|x_0]; 1-\gamma) = \hat{\alpha} + \hat{\beta} x_0 \pm t_{\gamma/2;(n-2)} \sqrt{QMRes\left[\frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}}\right]}
\]</span></p>
<p><br></p>
<p>O intervalo de confiança fornece informação sobre a precisão das estimativas, no sentido de que quanto menor a amplitude do intervalo maior a precisão.</p>
<p><br></p>
<p>Com os cálculos dos intervalos de confiança para alguns valores de <span class="math inline">\(x\)</span>, pode-se esboçar uma região em torno da reta estimada, indicando os limites superiores e inferiores desses intervalos.</p>
<p><br></p>
<p>Essa região também é chamada de bandas de confiança.</p>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-253-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## 
## Intervalos de Confiança para E[Y|x]:
##      fit    lwr    upr
## 1 140.03 121.10 158.96
## 2  96.06  86.68 105.45
## 3 100.61  90.80 110.43
## 4  82.42  72.92  91.92
## 5  68.78  57.23  80.32
## 6  52.10  36.56  67.64</code></pre>
<hr />
</div>
<div id="intervalo-de-predição" class="section level3 hasAnchor" number="13.9.5">
<h3><span class="header-section-number">13.9.5</span> Intervalo de Predição<a href="#intervalo-de-predi%C3%A7%C3%A3o" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br></p>
<p>Suponha que estamos interessados em fazer uma predição para um valor futuro de <span class="math inline">\(Y_0\)</span> (não observado) correspondente a valores específicos <span class="math inline">\(\mathbf{x}_0\)</span> das variáveis preditoras. Este intervalo frequentemente é mais relevante para o cientista do que o intervalo de confiança para as respostas médias, pois incorpora tanto a incerteza na estimação dos parâmetros quanto a variabilidade aleatória individual.</p>
<p><br></p>
<p>Para o modelo de regressão linear bem ajustado, <span class="math inline">\(\hat{Y}_0 = \mathbf{x}_0&#39;\hat{\boldsymbol{\theta}}\)</span> é a predição de <span class="math inline">\(Y_0\)</span>, onde <span class="math inline">\(\hat{\boldsymbol{\theta}} = [\hat{\beta_0}, \hat{\beta}_1, \ldots, \hat{\beta}_p]&#39;\)</span> é o vetor de parâmetros estimados. Note que a predição de <span class="math inline">\(Y_0\)</span> é igual ao estimador do valor esperado de <span class="math inline">\(Y\)</span> para <span class="math inline">\(\mathbf{x} = \mathbf{x}_0\)</span>.</p>
<p><br></p>
<p>O erro de predição é definido por <span class="math inline">\((Y_0 - \hat{Y}_0)\)</span>, o qual será a base para construirmos o intervalo de predição. Como <span class="math inline">\(Y_0\)</span> não faz parte da amostra utilizada no ajuste do modelo, é independente de <span class="math inline">\(\hat{Y}_0\)</span>. Portanto:</p>
<p><br></p>
<p><span class="math display">\[
E[Y_0 - \hat{Y}_0] = 0
\]</span></p>
<p><br></p>
<p><span class="math display">\[
V[Y_0 - \hat{Y}_0] = \sigma^2\left[1 + \mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0\right]
\]</span></p>
<p><br></p>
<p>em que o termo adicional <span class="math inline">\(1\)</span> representa a variabilidade individual de uma nova observação.</p>
<p><br></p>
<p>Como <span class="math inline">\([Y_0 - \hat{Y}_0]\)</span> segue uma distribuição normal, o intervalo de predição de <span class="math inline">\((1-\gamma)100\%\)</span> para um novo valor <span class="math inline">\(Y_0\)</span> dado <span class="math inline">\(\mathbf{x}_0\)</span> é definido por:</p>
<p><br></p>
<p><span class="math display">\[
IP(Y_0|\mathbf{x}_0; 1-\gamma) = \mathbf{x}_0&#39;\hat{\boldsymbol{\theta}} \pm t_{\gamma/2;(n-p-1)} \sqrt{QMRes\left[1 + \mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0\right]}
\]</span></p>
<p><br></p>
<p>em que <span class="math inline">\(t_{\gamma/2;(n-p-1)}\)</span> é o quantil <span class="math inline">\((1-\gamma/2)\)</span> da distribuição t de Student com <span class="math inline">\((n-p-1)\)</span> graus de liberdade.</p>
<p><br></p>
<p>A amplitude do intervalo de predição é:</p>
<p><br></p>
<p><span class="math display">\[
2 \cdot t_{\gamma/2;(n-p-1)} \sqrt{QMRes\left[1 + \mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0\right]}
\]</span></p>
<p><br></p>
<blockquote>
<p><strong>Diferença fundamental:</strong> Enquanto o intervalo de confiança refere-se ao valor esperado <span class="math inline">\(E[Y|\mathbf{x}_0]\)</span> (uma constante populacional), o intervalo de predição tem probabilidade <span class="math inline">\((1-\gamma)\)</span> de conter um futuro valor individual <span class="math inline">\(Y_0\)</span> da variável aleatória, sendo portanto sempre mais amplo devido ao termo adicional <span class="math inline">\(1\)</span> na variância.</p>
</blockquote>
<p><br></p>
<p>As bandas de confiança e as bandas de predição têm formato hiperbólico, o que enfatiza o risco de fazer extrapolações (predições fora do intervalo observado das variáveis <span class="math inline">\(X\)</span>). Portanto, os modelos devem ser usados com cautela para fazer previsões sobre a variável resposta.</p>
<p><br></p>
<p>Para regressão linear simples, <span class="math inline">\(\mathbf{x}_0 = [1, x_0]&#39;\)</span>, <span class="math inline">\(\mathbf{x}_0&#39;(X&#39;X)^{-1}\mathbf{x}_0 = \frac{1}{n} +
\frac{(x_0-\bar{x})^2}{S_{xx}}\)</span>, os graus de liberdade são <span class="math inline">\((n-2)\)</span>, e o intervalo se reduz a:</p>
<p><br></p>
<p><span class="math display">\[
IP(Y_0|x_0; 1-\gamma) = \hat{\alpha} + \hat{\beta} x_0 \pm t_{\gamma/2;(n-2)} \sqrt{QMRes\left(1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}}\right)}
\]</span></p>
<p><br></p>
<p><img src="apostila_files/figure-html/unnamed-chunk-254-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## 
## Intervalos de Confiança para E[Y|x]:
##      fit    lwr    upr
## 1 140.03 121.10 158.96
## 2  96.06  86.68 105.45
## 3 100.61  90.80 110.43
## 4  82.42  72.92  91.92
## 5  68.78  57.23  80.32
## 6  52.10  36.56  67.64
## 
## Intervalos de Predição para Y₀:
##      fit    lwr    upr
## 1 140.03 110.66 169.39
## 2  96.06  71.73 120.39
## 3 100.61  76.11 125.11
## 4  82.42  58.04 106.80
## 5  68.78  43.53  94.02
## 6  52.10  24.80  79.40</code></pre>
<p><br></p>
<p><strong>Diferença conceitual entre Intervalo de Confiança e Intervalo de Predição:</strong></p>
<p>Para um mesmo valor das variáveis preditoras (por exemplo, 50 anúncios), diferentes vendas podem ocorrer na população devido à variabilidade natural do fenômeno.</p>
<ul>
<li><p><strong>Intervalo de Confiança para <span class="math inline">\(E[Y|x_0]\)</span></strong>: Estima a <strong>média</strong> de todas as vendas possíveis quando fazemos 50 anúncios. É um valor fixo (constante) que queremos estimar. A incerteza vem apenas de não conhecermos perfeitamente os parâmetros do modelo.</p></li>
<li><p><strong>Intervalo de Predição para <span class="math inline">\(Y_0\)</span></strong>: Prevê quantos carros <strong>venderemos especificamente na próxima vez</strong> que fizermos 50 anúncios. Como cada venda individual varia naturalmente (mesmo mantendo x fixo), o intervalo precisa ser mais amplo para capturar essa variabilidade adicional.</p></li>
</ul>
<p><strong>Por que o Intervalo de Predição é mais largo?</strong></p>
<p>Porque incorpora duas fontes de incerteza:</p>
<ul>
<li>Não sabemos exatamente onde está a média (igual ao IC)<br />
</li>
<li>Mesmo sabendo a média exata, os valores individuais variam ao redor dela (variabilidade natural do fenômeno)</li>
</ul>
<p><strong>Analogia:</strong> Se você joga um dado 100 vezes, consegue estimar bem a média (≈3.5). Mas prever o resultado do próximo lançamento específico é muito mais incerto!</p>

<style>
.small-equation80 {
  font-size: 80%; 
}
</style>
<style>
.small-equation70 {
  font-size: 70%; 
}
</style>
<style>
.small-equation60 {
  font-size: 60%; 
}
</style>
<style>
.small-equation40 {
  font-size: 40%; 
}
</style>
<style>
blockquote {
  background-color: #c0c0c0; /* Fundo cinza claro */
  border-left: 4px solid #21130d; /* Barra lateral */
  padding: 10px;
  margin: 10px 0;
  border-radius: 4px; /* Bordas arredondadas */
  /* Remove o itálico padrão */
  color: #21130d; /* Cor do texto */
}
</style>
</div>
</div>
</div>
<script>
$(document).ready(function() {
  var $summary = $('.book-summary');
  var $body = $('.book-body');
  var $header = $('.book-header');
  var isResizing = false;
  var startX, startWidth;

  // Adicionar handle de redimensionamento
  $summary.append('<div class="resize-handle"></div>');
  
  var $handle = $('.resize-handle');

  $handle.on('mousedown', function(e) {
    isResizing = true;
    startX = e.clientX;
    startWidth = $summary.width();
    
    $('body').css('cursor', 'ew-resize');
    e.preventDefault();
  });

  $(document).on('mousemove', function(e) {
    if (!isResizing) return;
    
    var width = startWidth + (e.clientX - startX);
    
    // Limites de largura
    if (width < 200) width = 200;
    if (width > 600) width = 600;
    
    $summary.css('width', width + 'px');
    
    // Atualizar left do body e header se o TOC estiver visível
    if ($('.book').hasClass('with-summary')) {
      $body.css('left', width + 'px');
      $header.css('left', width + 'px');
    }
  });

  $(document).on('mouseup', function() {
    if (isResizing) {
      isResizing = false;
      $('body').css('cursor', 'default');
    }
  });
  
  // Observar mudanças na classe 'with-summary'
  var observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      if (mutation.attributeName === 'class') {
        var hasSummary = $('.book').hasClass('with-summary');
        if (!hasSummary) {
          // TOC colapsado
          $body.css('left', '0');
          $header.css('left', '0');
        } else {
          // TOC expandido
          var width = $summary.width();
          $body.css('left', width + 'px');
          $header.css('left', width + 'px');
        }
      }
    });
  });
  
  observer.observe($('.book')[0], {
    attributes: true
  });
});
</script>
            </section>

          </div>
        </div>
      </div>
<a href="introducao-a-correlacao-linear-de-pearson.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introducao-a-modelagem-dados.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/fjrcosta/apostila/edit/main/13-regressao_linear.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "none",
    "depth": 8
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
