[["index.html", "", " UNIVERSIDADE ESTADUAL DE LONDRINA CCE - Centro de Ciências Exatas DSTA - Departamento de Estatística (sala 11) Prof. M.e Eng.\\(^{o}\\) Felinto Junior Da Costa fjcosta@uel.br Londrina, 01 de September de 2025. "],["introdução-histórica-daquilo-que-veio-a-se-chamar-estatística.html", "Capítulo 1 Introdução histórica daquilo que veio a se chamar estatística", " Capítulo 1 Introdução histórica daquilo que veio a se chamar estatística do latim: statisticum collegium (colegiado dos assuntos do Estado); do alemão: statistik (Gottfried Achenwall, 1719-1772); no inglês: statistics (Enciclopédia Britânica, 1797). De acordo com a revista do Instituto Internacional de Estatística cinco homens são chamados de fundadores dos primórdios da estatística: Hermann Conring; Gottfried Achenwall; Johan Peter Süßmilch; John Graunt, e William Petty. "],["filosofia-da-ciência-teoria-do-conhecimento-epistemologia.html", "1.1 Filosofia da ciência (teoria do conhecimento, epistemologia)", " 1.1 Filosofia da ciência (teoria do conhecimento, epistemologia) Estritamente falando, todo o conhecimento fora da matemática, da lógica demonstrativa (um ramo da mesma) e da taxonomia, encontra-se fundamentado em hipóteses (naturalmente há inúmeros tipos de hipóteses, mas as que estamos a nos referir são altamente confiáveis, como as expressas em certas leis gerais da física e da química como, por exemplo, a Lei de Hooke as Leis de Kepler dentre tantas outras). O raciocínio lógico demonstrativo permeia as ciências até onde a matemática lhe suporta; todavia, em si (assim como também a matemática), é incapaz de gerar novos conhecimentos sobre o mundo que nos rodeia. O método lógico demonstrativo é próprio para objetos que existem apenas idealmente, que são construídos inteiramente pelo nosso pensamento. O método hipotético experimental é próprio das ciências naturais (física, química, biologia, etc.), que observam seus objetos e realizam experimentos. Figure 1.1: Método demonstrativo e Método experimental hipotético (George Polya, 1954) Hipotético porque os cientistas partem de hipóteses sobre os objetos que guiam os experimentos e a avaliação dos resultados e experimental porque se baseia em observações e em experimentos, tanto para formular quanto para verificar as teorias. O método hipotético experimental pode ser indutivo (fatos \\(\\to\\) lei geral) ou dedutivo (lei geral \\(\\to\\) fatos). Isso é observado em qualquer que seja a área do conhecimento: ciências biológicas; ciências exatas; ciências agrárias; ciências humanas; ciência sociais e outras. Figure 1.2: Representação esquemática do fluxo de infomações da amostra à produção de conhecimento Assim, na investigação científica é imposto ao pesquisador formular perguntas que deverão ser apropriadamente respondidas. comparar esses resultados a outros valores; ou, comparar resultados obtidos pela aplicação de diferentes métodos/ou produtos (valores centrais, variabilidade, proporções) observados em diferentes amostras. Figure 1.3: Método experimental hipotético Uma hipótese é uma conjectura racional feita após um grande número de observações e experimentos; é uma tese que precisa ser confirmada ou verificada por meio de novas observações e experimentos. Uma hipótese estatística é uma suposição feita sobre uma determinada característica de interesse de uma população sob estudo (um parâmetro) que subsiste (perdura, sobrevive, permanece incontestável) até que alguma informação sobre essa população seja estatisticamente significativa para contradizê-la. ``A ciência não consegue provar coisa alguma. Ela pode apenas refutar as coisas’’ (Karl Popper) Uma teoria científica é, portanto, transitória. Uma conjectura temporariamente sustentada que um dia poderá ser refutada e substituída por outra. Conclusões baseadas em raciocínios plausíveis são provisórias, ao contrário daquelas produzidas por raciocínios lógico demonstrativos. "],["diferentes-usos-relacionados-ao-termo-primeiros-levantamentos-estudos-e-publicações-o-passado-distante.html", "1.2 Diferentes usos relacionados ao termo, primeiros levantamentos, estudos e publicações (o passado distante)", " 1.2 Diferentes usos relacionados ao termo, primeiros levantamentos, estudos e publicações (o passado distante) O Domesday Book (link) foi encomendado em dezembro de 1085 por Guilherme, o Conquistador (King William I), que invadiu a Inglaterra em 1066. O primeiro esboço foi concluído em agosto de 1086 e continha registros de 13.418 assentamentos nos condados ingleses ao sul dos rios Ribble e Tees (a fronteira com a Escócia) com informações sobre terras, proprietários, uso da terra, empregados e animais cujo propósito básico era fundamentar a taxação (Figura 1.4). Figure 1.4: Domesday Book O dramaturgo inglês William Shakespeare usou a palavra statists (estadistas e, portanto, num sentido não relacionado com números ou matemática) no diálogo da Cena II de Hamlet (link). “Hamlet: Cercado assim por tantas vilanias, mesmo antes de eu poder dizer o prólogo, representava o cérebro. Sentei-me e escrevi com capricho nova carta. Já pensei, como os nossos estadistas, que é feio escrever bem, tendo insistido, até, em desaprendê-lo; mas, nessa hora muito bom me foi isso. Quererias saber qual o conteúdo da mensagem?[…]” Um ponto de partida para a compreensão a ligação entre estado e estatística é Hermann Conring (1606-1681), professor de filosofia, medicina e política da Universidade de Helmstadt (atual Alemanha), criou um curso de Ciência política em 1660 para funcionários do estado, que descrevia e examinava as questões fundamentais do Estado. Nele a palavra estatística (parece ter) passado a ser considerada como uma disciplina autônoma que tinha por objetivo a descrição das coisas do Estado (Figura 1.5). Figure 1.5: Hermann Conring (1606-1681) Microscopium Statisticum: quo status imperii Romano-Germanici cum primis extraordinarius, ad vivum repraentatur (Statistical Microscope: An Analysis of the State, in which the State of the Germanic Roman Empire is vividly represented, above all extraordinary) é um livro cujo título normalmente chama mais atenção do que seu conteúdo: o uso de statisticum – que significa do estado – é reconhecido como um dos primeiros passos em direção ao uso da palavra estatística como hoje empregamos (Figura 1.6). Foi publicado sob o pseudônimo de Helenus Politanus em 1672. Figure 1.6: Microscopium Statisticum (ed. de 1672) Johan Peter Süßmilch é mais conhecido por seu notável trabalho de 1741 sobre população, conectando o direito natural e a aritmética política. O trabalho de Süssmilch foi amplamente referenciado por Robert Malthus (1798). Ele reconheceu não apenas a “proporção geométrica” da fertilidade humana, mas também o efeito de padrões de vida mais elevados sobre o problema populacional (Figura 1.7). Figure 1.7: The Divine order in the circumstances of the human sex (1741) Com um sentido não relacionado com números ou matemática, a palavra estatística parece ter sido também proposta no século XVII, pelo historiador e professor alemão (à época Transilvânia) Martin Schmeitzel (1679-1747) da Universidade de Jena e, posteriormente adotada por seu aluno, (igualmente) historiador e jurista Gottfried Achenwall (1719-1772) em 1749, em Abriß der neuen Staatswissenschaft der vornehmen Europäischen Reiche und Republiken (Esboço da nova ciência política dos nobres impérios europeus e repúblicas, Figura 1.8). Figure 1.8: Abriß der neuen Staatswissenschaft der vornehmen Europäischen Reiche und Republiken (1749) Muitos anos depois, William Hooper usou a palavra estatística em sua tradução de The Elements of Universal Erudition(Elementos da Erudição Universal) escrita por Jacob Friedrich Freiherr von Bielfeld (1717-1770). Nesse livro, a estatística foi definida como a ciência que nos ensina o arranjo político de todos os estados modernos do mundo conhecido (novamente num sentido não associado a números ou matemática, Figura 1.9). Figure 1.9: The Elements of Universal Erudition (1771) Na Inglaterra a palavra estatística estava mais associada ao estudo de dados numéricos como modo de se obter insights sobre questões sociais e demográficas no país. Dois pioneiros nessa linha foram William Petty e John Graunt. Em 1687 o economista e filósofo inglês William Petty (1623-1687) publicou Several Essays on Political Arithmetic (Vários ensaios sobre aritmética política), sugerindo ao governo inglês a criação de um departamento para registro de estatísticas vitais (Figura 1.10). Figure 1.10: Several Essays in Political Arithmetick (ed. de 1699) O negociante inglês John Graunt (1620-1674) substituiu a crença pela evidência em Natural and Political Observations Mentioned in a Following Index and Made upon the Bills of Mortality (Observações naturais e políticas feitas sobre as notas de mortalidade). Nesse trabalho, realizado com dados coletados das paróquias de Londres entre 1604 e 1660, Graunt tirou as seguintes conclusões: que havia maior nascimento de crianças do sexo masculino, mas havia distribuição aproximadamente igual de ambos os sexos na população geral; alta mortalidade nos primeiros anos de vida; maior mortalidade nas zonas urbanas em relação às zonas rurais (Figura 1.11). Figure 1.11: Natural and Political Observations Mentioned in a Following Index and Made upon the Bills of Mortality (ed. de 1662) O matemático e astrônomo inglês Edmond Halley (1656-1742) construiu em 1693, baseado em dados coletados na cidade (à época) alemã de Bresláu, uma Life Table (Tábua de sobrevivência), um estudo que analisa as probabilidades de sobrevivência e morte em relação à idade (Figura 1.12). Figure 1.12: Halley’s life table (1693) O jurista e político escocês John Sinclair propôs que se realizasse uma detalhada pesquisa em 938 paróquias para elucidar a história natural e política de seu país ( Statistics Accounts ). Essa pesquisa fazia parte de um projeto muito mais ousado: The Pyramid of Statistical Enquiry (A Pirâmide da Pesquisa Estatística, Figura 1.13). Figure 1.13: The Pyramid of Statistical Enquiry (1814) Outro trabalho com vertente demográfica foi o de Sébastien Le Prestre, Marquês de Vauban (1633– 1707), intitulado Méthode générale et facile pour faire le dénombrement des peuples, detakhado num livro de 1686 (Figura 1.14). Figure 1.14: Méthode générale et facile pour faire le dénombrement des peuples (1686) O médico inglês (considerado por alguns como o “pai” da epidemiologia moderna) John Snow (1813-1858) estudou a dispersão espacial dos casos de cólera em Londres e concluiu que sua causa residia na contaminação da água consumida (poço localizado na Broad Street, no distrito do Soho): Report to the Cholera Outbreak in the Parish of St. James, Westminster during the Autumn of 1854 (Relatório sobre o surto de cólera na paróquia de St. James, Westminster durante o outono de 1854, Figura 1.15). Figure 1.15: Mapa dos casos de cólera (1854) "],["visualização-de-dados-estudos-e-primeiras-publicações.html", "1.3 Visualização de dados &amp; Estudos e primeiras publicações", " 1.3 Visualização de dados &amp; Estudos e primeiras publicações O teólogo e filósofo inglês Joseph Priestley (1733-1804) introduziu como inovação os primeiros gráficos com linha temporal, em que barras individuais eram usadas para visualizar o tempo de vida de uma pessoa e o todo pode ser usado para comparar a expectativa de vida de várias pessoas (Figura 1.16). Figure 1.16: Expectativa de vida de diversas pessoas (1765) O engenheiro e economista escocês William Playfair (1759-1823) é considerado comumente como fundador dos métodos gráficos para apresentação de estatísticas. Playfair concebeu vários tipos de diagramas para visualização de dados: em 1786, o gráfico de barras (Figura 1.17); e, em 1801, o gráfico de setores (Figura 1.18). Figure 1.17: Commercial and Political Atlas (Atlas Comercial e Político de 1786): cada barra representa as exportações e importações da Escócia para 17 países em 1781 Figure 1.18: Statistical Breviary (Breviário Estatístico de 1801): proporção da extensão do Império Turco em diferentes regiões do mundo: Asia, Europa e África, antes de 1789 A enfermeira inglesa Florence Nightingale (1820-1910) conduziu um trabalho pioneiro ao chegar no hospital militar britânico na Turquia em 1856, estabelecendo uma ordem e um método muito necessários aos registros médicos estatísticos e que indicaram serem as precárias práticas sanitárias o culpado da alta mortalidade (link) , Figuras 1.19 e 1.20. Figure 1.19: Esse diagrama (coxcomb) feito durante a Guerra da Crimeia foi dividido igualmente em 12 setores, representando os meses do ano, com a área sombreada do setor de cada mês proporcional à taxa de mortalidade naquele mês. Seu sombreamento com código de cores indicava a causa da morte em cada área do diagrama Figure 1.20: Gráfico de barras de Florence Nightingale mostrando as diferenças de mortalidade entre soldados britânicos e a população masculina inglesa geral (civis) "],["pesquisadores-cuja-contribuição-foi-fundamental-na-área.html", "1.4 Pesquisadores cuja contribuição foi fundamental na área", " 1.4 Pesquisadores cuja contribuição foi fundamental na área Uma breve biografia de cada um dos pesquisadores a seguir relacionados pode ser obtida em: (link). Niccolò Fontana Tartaglia (Veneza à época, hoje Itália: 1499-1557) Girolamo Cardano (Pávia à época, hoje Itália: 1501-1576) Galileu Galilei (Florença à época, hoje Itália: 1564-1642) Pierre de Fermat (França: 1607-1665) Blaise Pascal (França: 1623-1662) Jakob Bernoulli (Suíça: 1655-1705) Abrahan de Moivre (França: 1667-1754) Thomas Bayes (Inglaterra: 1702-1761) Pierre-Simon Laplace (França: 1749-1827) Johann Carl Friedrich Gauss (Alemanha: 1777-1856) Lambert Adolphe Jacques Quételet (França à época, hoje Bélgica: 1796-1874) Pafnuti Lvovitch Chebyshev (Rússia: 1821-1894) Francis Galton (Inglaterra: 1822-1911) Wilhelm Lexis (Alemanha: 1837-1914) Thorvald Nicolai Thiele (Dinamarca: 1838-1910) Friedrich Robert Helmert (Saxônia: 1843-1917) Francis Ysidro Edgeworth (Inglaterra: 1845-1926) James Douglas Hamilton Dickson (Escócia: 1849-1931) Andrei Andreyevich Markov (Rússia: 1856-1922) Aleksandr Mikhailovich Lyapunov (Rússia: 1857-1918) Walter Frank Raphael Weldon (Inglaterra: 1860-1906) Karl Pearson (Inglaterra: 1857-1936) William Seally Gosset (Inglaterra: 1876-1937) Ronald Aylmer Fisher (Inglaterra: 1890-1962) Andrei Nikolaevich Kolmogorov (Rússia: 1903-1987) "],["revista-biometrika.html", "1.5 Revista Biometrika", " 1.5 Revista Biometrika Karl Pearson (1857-1936) é amplamente considerado o fundador da disciplina moderna de estatística, e também é famoso como um filósofo da ciência, como escritor sobre o darwinismo social e como um dos principais impulsionadores para instalar a eugenia como a ciência social chave. “Pretende-se que a Biometrika sirva como um meio não apenas de coletar ou publicar, sob um título, dados biológicos de um tipo não coletados sistematicamente ou publicados em outro lugar em qualquer outro periódico, mas também de disseminar um conhecimento de tal teoria estatística para o seu tratamento científico[…]” Em outubro de 1901 foi fundada a Biometrika, the Journal for the Statistical Study of Biological Problems (Biometrika, o Jornal para o Estudo Estatístico de Problemas Biológicos) com o propósito de promover a análise estatística de fenômenos biológicos, isto é, a matematização da biologia. Os fundadores da Biometrika foram Sir Francis Galton (primo de Charles Darwin), Walter Frank Raphael Weldon e Karl Pearson. A maior parte do trabalho foi feita por Pearson e Weldon, este último focando na edição do conteúdo (ou seja, o aspecto biológico) e o primeiro nos detalhes, incluindo correções de prova. Galton e o eugenista americano Charles Davenport atuaram, respectivamente, como consultor e editor. Alguns dos tópicos abordados na revista incluem criminologia, botânica, zoologia, epidemiologia e outros aspectos da saúde humana. Na década de 1930, o caráter da Biometrika mudou, e ``representou a vanguarda internacional da pesquisa em métodos estatísticos e sua aplicação na ciência e tecnologia’’, ao invés de focar a hereditariedade. Sir Francis Galton, que serviu como editor da primeira edição (1901), escreveu a Introdução, que incluiu uma declaração de propósito para a revista (link). "],["eugenia.html", "1.6 Eugenia", " 1.6 Eugenia Em 16 de maio de 1883 Sir Francis Galton cunhou o termo eugenia, posteriormente descrevendo-o como “o estudo das agências sob controle social que podem melhorar ou reparar as qualidades raciais das gerações futuras, seja fisicamente ou mentalmente”. Galton detalha o conceito em seu livro Inquiries into Human Faculty and its Development, e recomenda que indivíduos de famílias altamente classificadas em seu sistema de mérito sejam encorajados a se casar cedo e receber incentivos para ter filhos. Ele também condenou os casamentos tardios dentro desse mesmo grupo como “disgênicos” ou desvantajosos para a espécie humana. A palavra eugenia foi extraída da palavra grega eu, que significa bem, e genos, que significa prole. Juntos, significaria bem-nascido. Este livro caiu em domínio público e pode ser lido na íntegra online. A caracterização original de eugenia de Galton pode ser encontrada na página 17 desta edição de domínio público (Parte 1 do pdf): “uma breve palavra para expressar a ciência de melhorar o rebanho, que não está de modo algum confinado a questões de acasalamento criterioso, mas que, especialmente no caso do homem, toma conhecimento de todas as influências que tendem, mesmo que em grau remoto, a dar ao raças ou linhagens de sangue mais adequadas uma melhor chance de prevalecer rapidamente sobre os menos adequados do que teriam de outra forma […]”(Galton, 1883, p.17) Mais recentemente, alguns grupos sociais viram no trabalho e opiniões de Fisher endossos ao colonialismo, à supremacia branca e à eugenia (como hoje interpretada). Outros grupos, todavia, afirmam que Fisher não era racista nem eugenista, embora ele achasse que havia diferenças comportamentais e de inteligência entre os grupos humanos. Figure 1.21: Gráfico de linhagens para alergias Figure 1.22: Gráfico de linhagens para aptidão musical &lt;img src=“images1/chart_Kallikak_pedigree2.jpg” alt=“Linhas”normais” e “degeneradas” da família Kallikak (New Jersey)” width=“75%” /&gt; Figure 1.23: Linhas “normais” e “degeneradas” da família Kallikak (New Jersey) Figure 1.24: Lei da Inegridade Racia (Virginia, EUA, 1924) Figure 1.25: Licença para casamento "],["estatística-e-machine-learning-uma-livre-tradução-deste-link.html", "1.7 Estatística e machine learning : uma livre tradução deste link", " 1.7 Estatística e machine learning : uma livre tradução deste link Figure 1.26: Autor: estatístico anônimo Para se raciocinar rigorosamente sob incerteza, precisamos invocar a linguagem da probabilidade (Zhang et al. 2020). Qualquer modelo que não forneça a quantificação da incerteza associada ao seu resultado provavelmente produzirá uma imagem incompleta e potencialmente enganosa. Embora este seja um consenso irrevogável na estatística, um equívoco comum, embora muito persistente, é que os algoritmos de machine learning geralmente carecem de formas adequadas de quantificar a incerteza. Apesar do fato dos dois termos existirem em paralelo e serem indistintamente utilizados, a percepção de que algoritmos de machine learning e a estatística implicam um conjunto de técnicas não sobrepostas permanece viva, tanto entre profissionais como acadêmicos. Isso é vividamente retratado pela declaração provocativa (e potencialmente irônica) de Brian D. Ripley de que “o aprendizado de máquina é estatística menos qualquer verificação de modelos e suposições” que ele fez durante a “useR! 2004”, conferência de Viena que serviu para ilustrar a diferença entre aprendizado de máquina e estatística. Na verdade, a relação entre estatística e algoritmos de machine learning é artificialmente complicada por tais afirmações e, na melhor das hipóteses, isto é lamentável, pois implica numa distinção profunda e qualitativa entre as duas disciplinas (Januschowski et al. 2020). O artigo de Leo Breiman (2001) é uma exceção notável, pois propõe diferenciar os dois com base na cultura científica, e não apenas nos métodos. Embora as abordagens discutidas em Breiman (2001) constituam uma divisão admissível do espaço de análise e modelação de dados, os avanços mais recentes tornaram gradualmente esta distinção menos clara. Na verdade, a tendência atual de investigação tanto em estatística com algoritmos de machine learning gravita no sentido de aproximar ambas as disciplinas. Numa era de necessidade crescente de que os resultados dos modelos de previsão sejam transformados em conhecimentos explicáveis e confiáveis, este é um desenvolvimento extremamente promissor e encorajador, uma vez que ambas as disciplinas têm muito a aprender uma com a outra. Junto com Januschowski et al. (2020) , argumentamos que é mais construtivo procurar um terreno comum do que introduzir fronteiras artificiais. "],["introdução-conceitual-essencial.html", "Capítulo 2 Introdução conceitual essencial", " Capítulo 2 Introdução conceitual essencial “Estatística é a ciência de coletar, organizar, apresentar, analisar e interpretar dados[…]” (Ronald A. Fisher) De modo geral, a estatística pode ser dividida em três grandes áreas: descritiva; probabilidade; e, inferencial. "],["estatística-descritiva.html", "2.1 Estatística descritiva", " 2.1 Estatística descritiva Nos primeiros trabalhos estatísticos, os dados coletados eram inicialmente apresentados na forma de tabelas e gráficos. A estatística descritiva se ocupa de tudo o que seja relacionado a dados: coleta, processamento, descrição (seja na forma tabular ou gráfica) e sínteses numéricas (de locação, de dispersão, de repartição) sem inferir coisa alguma além da informação trazida pelos dados. Vem experimentando crescente uso em todas as áreas científicas e desenvolvimento: crescente uso de uma abordagem quantitativa em todas as ciências; disponibilidade de recursos computacionais; quantidade de dados coletados. A palavra estatística pode assumir diferentes significados: no singular: estatística refere-se à ciência que compreende métodos que são usados na coleta, análise, interpretação e apresentação de dados quantitativos ou qualitativos (numéricos ou não); e, denota uma medida ou fórmula específica (tais como uma média, um intervalo de valores, uma taxa de crescimento, um índice). no plural: estatísticas refere-se a dados coletados de maneira sistemática com um propósito específico definido em qualquer campo de estudo (nesse sentido, as estatísticas também podem ser consideradas como agregados de fatos expressos em forma numérica). "],["estatística-inferencial.html", "2.2 Estatística inferencial", " 2.2 Estatística inferencial A estatística inferencial tem o objetivo de estabelecer níveis de confiança da tomada de decisão de associar uma estimativa amostral a um parâmetro populacional. Divide-se em estimação e testes de significância. “Dedução e indução são procedimentos racionais que nos levam do já conhecido ao ainda não conhecido; isto é, permitem que adquiramos conhecimentos novos graças a conhecimentos já adquiridos.[…]” Dedução. Na dedução parte-se de uma verdade já conhecida para demonstrar que ela se aplica a todos os casos particulares iguais. Vai do geral ao particular. Indução. Na indução parte-se de alguns casos particulares iguais ou semelhantes para se estipular uma lei geral. Vai do particular ao geral. Na dedução, dado X, infiro (concluo) a, b, c, d. Na indução, dados a, b, c, d, infiro (concluo) X. Exemplo: testes de aceleração (0-60 mph) feitos com 6 carros importados em 1999 resultaram nas seguintes medidas: 12,9 s; 16,50 s; 11,30 s; 15,20 s; 18,20 s e 17,70 s. Um estudo descritivo poderia afirmar que: metade dos dados coletados acelera de 0-60 mph em menos de 16,00 s; e a aceleração média de 0-60 mph é de 15,30 s. Mas, a partir dessa amostra concluir que a aceleração média de todos os carros importados em 1999 seja de 15,30 s; ou, que metade dos carros importados em 1999 acelerem de 0-60 mph em menos de 16,00 s são afirmações que pertencem à inferência estatística. "],["produção-de-conhecimento.html", "2.3 Produção de conhecimento", " 2.3 Produção de conhecimento “A ciência não consegue provar coisa alguma. Ela pode apenas refutar as coisas […]” (Karl Popper) Figure 2.1: Método demonstrativo e Método experimental hipotético (George Polya, 1954) Na expansão de qualquer área do conhecimento propomos hipóteses que serão avaliadas mediante a coleta de dados que, depois de analisados, revelarão informações que, eventualmente, nos conduzirão ao afastamento da hipótese original e à proposição de outras, num processo contínuo. Figure 2.2: Método experimental hipotético Uma investigação científica deve envolver, em linhas gerais: observação dos fatos; descrição das características essenciais, segundo o que se obteve através da observação; explicação dessas características descritivas; previsão; e, decisão pertinente à investigação. O planejamento de uma pesquisa deve envolver, em linhas gerais: definição do universo: é necessário delimitar claramente, no tempo e espaço, o âmbito do inquérito, definindo, em termos precisos, o universo a ser trabalhado; exame das informações disponíveis: deve-se reunir todo o material existente: mapas, artigos, livros, relatórios relativos a levantamentos semelhantes; tipos de levantamentos: completo ou amostral; prazo; custo; precisão. "],["população-universo-amostra.html", "2.4 População (universo) &amp; amostra", " 2.4 População (universo) &amp; amostra Figure 2.3: Universo e amostra Quase que, invariavelmente, em todo ramo de conhecimento, o pesquisador esbarra em uma série de limitações das mais variadas ordens (econômica, técnica, ética, geográfica, temporal,…) que impossibilitam o estudo dos dados e informações associados a todos os casos existentes (população ou universo). Por essa razão, através de um procedimento estatístico denominado de amostragem, estuda-se uma população (universo) a partir de uma amostra. Amostra é, portanto, um subconjunto finito e representativo da população (universo), extraído de modo sistemático (planejado). "],["parâmetros-e-estatísticas.html", "2.5 Parâmetros e estatísticas", " 2.5 Parâmetros e estatísticas É comum a adoção de letras gregas para as características descritivas que se referirem à poúlação (universo) e letras do alfabeto latino para aquelas relativas à amostra extraída: Característica estudada Notação populacional Notação amostral Número de elementos N n Média \\(\\mu\\) (“mi”) \\(\\stackrel{-}{x}\\) Variância \\(\\sigma^{2}\\) (“sigma”) \\({s}^{2}\\) Desvio padrão \\(\\sigma\\) (“sigma”) s Proporção \\(\\Pi\\) (“pi”) p ou \\(\\hat{p}\\) Figure 2.4: Parâmetros e estatísticas Figure 2.5: Alfabeto grego "],["tipos-de-variáveis.html", "2.6 Tipos de variáveis", " 2.6 Tipos de variáveis Variáveis quantitativas contínuas: são os dados com maior potencial de produzir informação significativa dentre todos: comprimentos, áreas, pesos, densidades; e, discretas: são dados com um pouco menos de informação que os de natureza contínua mas possuem mais informação que dados qualitativos: número de andares de um prédio, de degraus de uma escada, número de filhos de um casal. Variáveis qualitativas ordinais: apresentam um pouco mais de informação que os dados qualitativos puramente nominais na medida que suas classes podem ser interpretadas como possuindo um ordenamento inerente: padrão construtivo (baixo, médio, alto), classe econômica de rendimento (baixa, média, alta), nível de escolaridade (fundamental, médio e superior); e, nominais: são dados a menor quantidade de informação: sexo, cor, códigos postais de cidades; Codificação de variáveis qualitativas binárias: pela associação de valores numéricos: 0 ou 1 a uma variável qualitativa nominal que se apresente com apenas dois aspectos: sim ou não, ausência ou presença. Pela composição de mais variáveis binárias pode-se codificar variáveis que possuam um número maior de classes; e, proxy: pela associação de valores numéricos contínuos que guardam ``correlação’’ com as classes da variável qualitativa nominal. Figure 2.6: Tipos e codificações de variáveis "],["indexação-de-dados-i.html", "2.7 Indexação de dados (\\(i\\))", " 2.7 Indexação de dados (\\(i\\)) Muitas operações matemáticas são representadas trazendo os valores dos dados indicados de modo genérico por letras (gregas ou romanas) e índices como, por exemplo, \\(x_{i}\\). Tal notação está a indicar que, se dispuséssemos os dados em uma linha virtual (às vezes necessitando que estejam ordenados, como para a determinação de uma separatiz), cada um de seus valores estaria a ocupar uma posição indicada pelo índice i: Figure 2.7: Entendendo a indexação de dados "],["noções-básicas-sobre-somatórios-sigma.html", "2.8 Noções básicas sobre somatórios (\\(\\Sigma\\))", " 2.8 Noções básicas sobre somatórios (\\(\\Sigma\\)) Somatório é um operador matemático utilizado para simplificar expressões que envolvam soma de mais de um elemento. Digamos, por exemplo, que estamos interessados saber o total de comissões a pagar em um determinado setor de uma empresa. Admita que esse setor tenha 6 funcionários: Pedro, Guilherme, Lucas, Maria, Fernanda e Roberto e que suas comissões sejam R$ 3000; R$ 3300; R$ 3900; R$ 2950; R$ 3150 e R$ 3450. A representação da soma das comissões pode ser expressa de vários modos como, por exemplo, nesse extensa frase: O total de comissões a pagar em um determinado setor de uma empresa é a Renda do Pedro mais a Renda do Guilherme mais a Renda do Lucas mais a Renda da Maria mais a Renda da Fernanda mais Renda do Roberto. Atribuindo os valores para cada uma das rendas: O total de comissões a pagar em um determinado setor de uma empresa é: : R$ 3000 + R$ 3300 + R$ 3900 + R$ 2950 + R$ 3150 + R$ 3450. Chamando-se “O total de comissões a pagar em um determinado setor de uma empresa é” de \\(X\\), teremos: \\(X\\) = R$ 3000 + R$ 3300 + R$ 3900 + R$ 2950 + R$ 3150 + R$ 3450. Para simplificar a representação dessa operação, vamos enumerar os funcionários: Pedro (1), Guilherme (2), Lucas (3), Maria (4), Fernanda (5) e Roberto (6). Além disso, vamos chamar a comissão a ser paga pela letra X. Para diferenciar a fração da comissão \\(X\\) a ser paga a cada um dos funcionários podemos por um índice na letra \\(X\\) para indicar a quem estamos nos referindo. Assim \\(X_{1}\\) seria a comissão do Pedro, \\(X_{2}\\) a do Guilherme, \\(X_{3}\\) a do Lucas, \\(X_{4}\\) a da Maria, \\(X_{5}\\) a da Fernanda e \\(X_{3}\\) a do Roberto. Com essa notação podemos representar matematicamente o total das comissões a pagar em um determinado setor de uma empresa por: \\(X=X_{1}+X_{2}+X_{3}+X_{4}+X_{5}+X_{6}\\) Cada um desses fatores pode ser generalizado como um \\(X_{i}\\), a comissão de um i-ésimo funcionário qualquer. Sabendo que o setor tem apenas 6 funcionários (Pedro, Guilherme, Lucas, Maria, Fernanda e Roberto) então esse i irá variar de 1 a 6 (Pedro:1, Guilherme: 2, Lucas: 3, Maria: 4, Fernanda: 5 e Roberto: 6). Com todas essas considerações podemos representar a soma das comissões utilizando a notação matemática do somatório. A letra grega maiúscula \\(\\Sigma\\) (“sigma”) é habitualmente adotada na matemática para representar o somatório de uma quantidade de fatores. Assim, nosso exemplo da soma de 6 fatores (comissões) pode ser representada matematicamente por: \\[ \\sum_{i=1}^{6}{X_{i}} = X_{1}+X_{2}+X_{3}+X_{4}+X_{5}+X_{6} \\] Observe que abaixo da letra \\(\\Sigma\\) (“sigma”) vemos \\(i=1\\) indicando que o índice dos fatores (X) a serem somados (a i-ésima comissão) irá se iniciar pela comissão do primeiro funcionário, quando então i = 1. Acima da letra \\(\\Sigma\\) (“sigma”) vemos o número \\(6\\) indicando que o índice dos fatores (X) a serem somados irá se dar até o valor da comissão do sexto funcionário, quando então i=6. Generalizando-se para uma soma de \\(n\\) fatores \\(X\\): \\[ \\sum_{i=1}^n{X_{i}}. \\] A representação matemática do somatório pode ser inserida junto a qualquer outra operação como, por exemplo, podemos, depois de realizar a soma, dividi-la por um valor \\(n\\) qualquer \\[ \\frac{\\sum_{i=1}^n{X_{i}}}{n} \\\\ \\] ou elevá-la ao quadrado: \\[ \\left(\\sum_{i=1}^n{X_{i}}\\right)^{2} \\] Atenção para a diferença entre essas duas operações: \\[ \\left(\\sum_{i=1}^n{X_{i}}\\right)^{2} \\] e \\[ \\sum_{i=1}^n{X_{i}^{2}} \\] A primeira indica que devemos realizar a soma dos fatores e só então elevar esse resultado ao quadrado. A segunda indica que devemos realizar a soma dos quadrados de cada um dos fatores. library(formattable) comissoes=c(3000, 3300, 3900, 2950, 3150, 3450) #Somatório das comissões currency(sum(comissoes), symbol = &quot;R$&quot;, digits = 2L, format = &quot;f&quot;, big.mark= &quot;.&quot;, decimal.mark= &quot;,&quot;, sep= &quot; &quot;) ## [1] R$ 19.750,00 #Somatório das comissões dividido pelo número de comissões currency(sum(comissoes)/length(comissoes), symbol = &quot;R$&quot;, digits = 2L, format = &quot;f&quot;, big.mark= &quot;.&quot;, decimal.mark= &quot;,&quot;, sep= &quot; &quot;) ## [1] R$ 3.291,67 #Quadrado do somatório das comissões currency(sum(comissoes)^2, symbol = &quot;R$&quot;, digits = 2L, format = &quot;f&quot;, big.mark= &quot;.&quot;, decimal.mark= &quot;,&quot;, sep= &quot; &quot;) ## [1] R$ 390.062.500,00 #Somatório dos quadrados das comissões currency(sum(comissoes^2), symbol = &quot;R$&quot;, digits = 2L, format = &quot;f&quot;, big.mark= &quot;.&quot;, decimal.mark= &quot;,&quot;, sep= &quot; &quot;) ## [1] R$ 65.627.500,00 "],["análise-combinatória-métodos-de-enumeração.html", "2.9 Análise combinatória (métodos de enumeração)", " 2.9 Análise combinatória (métodos de enumeração) A análise combinatória (ou métodos de enumeração) é um conjunto de técnicas para agrupamento de objetos conforme regras definidas e obtenção, através de cálculos, do número de agrupamentos possíveis. 2.9.1 Princípio básico da contagem (regra da multiplicação) Suponha a realização de dois experimentos. Se o experimento \\(E\\) pode gerar qualquer um de \\(n\\) resultados possíveis (\\(E_{1},E_{2},E_{3}, \\dots, E_{n}\\)) e se, para cada um dos resultados do experimento, houver \\(m\\) resultados possíveis para o experimento \\(F\\) (\\(F_{1},F_{2},F_{3}, \\dots, F_{m}\\)), então os dois experimentos possuem conjuntamente \\(n \\cdot m\\) diferentes resultados possíveis. Figure 2.8: Regra da multiplicação Esse princípio recebe o nome de Princípio multiplicativo, e é aplicado nos casos em que os eventos são interligados pelo conectivo e, característico de decisões sucessivas: ocorrem os dois. Se um homem tem 2 camisas e 4 gravatas, então ele tem \\(2 \\times 4 = 8\\) formas de combinar uma camisa com uma gravata. Um diagrama como ilustrado na Figura 2.9 (denominado diagrama de árvore em virtude de sua aparência) geralmente é usado para explicar o princípio acima Figure 2.9: Diagrama de árvore Ao lançarmos uma moeda três vezes (assumindo-se que K: cara e C: coroa) haverá \\(2 \\times 2 \\times 2 = 8\\) possibilidades distintas. O diagrama de árvore associado será (cf. Figura 2.10: Figure 2.10: Diagrama de árvore 2.9.2 Regra da adição Suponha agora os mesmos expermentos \\(E\\) e \\(F\\) que geram \\(n\\) e \\(m\\) resultados possíveis (\\(E_{1},E_{2},E_{3}, \\dots, E_{n}\\) e \\(F_{1},F_{2},F_{3}, \\dots, F_{m}\\)), mas que eeses experimentos não estejam mais alinhados sequencialmente: ocorre o evento \\(E\\) ou o evento \\(F\\). Então o número de maneiras pelas quais o evento \\(E\\) ou o evento \\(F\\) poderão se manifestar será de \\(n + m\\) maneiras diferentes: Figure 2.11: Regra da adição Esse princípio recebe o nome de Princípio aditivo, e é aplicado nos casos em que os eventos são interligados pelo conectivo ou, característico de eventos mutuamente exclusivos: ocorre um ou outro. Uma cantina de um colégio possui três tipos de sucos e dois tipos de refrigerantes. Um aluno pode adquirir apenas 1 suco ou 1 refrigerante. Quantas possibilidades de escolha ele tem? Seja \\(E_{1}\\) definido como escolher um tipo de suco (\\(n_{1}=3\\)) e \\(E_{2}\\) definido como escolher 1 tipo de refrigerante (\\(n_{2}=2\\)). Então o número total de possíveis escolhas será dado aplicando-se o princípio aditivo: \\[ n_{1} + n_{2}=5 \\] 2.9.3 Permutações (ordenação de elementos) Suponha \\(n\\) objetos diferentes. Permutar os \\(n\\) objetos equivaleria a colocá-los dentro de uma caixa com \\(n\\) compartimentos em alguma ordenação. O primeiro compartimento pode ser ocupado por qualquer um dos \\(n\\) objetos, o segundo por \\(n-1\\) e o último por apenas 1 objeto. Assim, pelo princípio básico vemos que essa caixa poderá ser carregada de: \\[ n.(n-1).(n-2).1= n! \\text{ maneiras diferentes.} \\]. Exemplo: quantos diferentes arranjos ordenados das letras a, b e c são possíveis? Pela enumeração direta vemos que são 6, ou seja, ‘abc’, ‘acb’, ‘bac’, ‘bca’, ‘cab’ e ‘cba’, resultado de \\(3!=6\\). Exemplo: quantas diferentes ordens de rebatedores são possíveis em um time de beisebol formado por 9 jogadores? \\(9! = 362.880\\) ordenamentos possíveis para os rebatedores. Exemplo: uma turma de teoria da probabilidade é formada por 6 estudantes do sexo masculino e 4 do sexo feminino. Aplica-se uma prova e os estudantes são classificados de acordo com o seu desempenho. Suponha que nenhum dos estudantes tenha tirado a mesma nota. (a) Quantas diferentes classificações são possíveis? (b) Se os estudantes do sexo masculino forem classificados apenas entre si e também os do sexo feminino apenas entre si, quantas diferentes classificações são possíveis? Como cada classificação corresponde a um arranjo particular das 10 pessoas, a resposta é \\(10! = 3.628.800\\). Como há \\(6!\\) possíveis classificações dos homens entre si e \\(4!\\) classificações possíveis das mulheres entre si, segue do princípio básico que há \\((6!)(4!)=(720)(24) = 17.280\\) classificações possíveis neste caso. 2.9.4 Arranjos sem repetição Considere um conjunto de \\(n\\) objetos diferentes. Suponha que desejamos selecionar uma quantidade \\(p\\) desses objetos, onde \\(p &lt; n\\), e dispor os \\(p\\) objetos escolhidos em uma ordem específica. Quando selecionamos e ordenamos um subconjunto de objetos de um conjunto maior, estamos formando arranjos ordenados. Nesse caso, o número total de arranjos possíveis de \\(p\\) elementos retirados de um conjunto de \\(n\\) objetos distintos (sem repetir nenhum elemento) é dado por: \\[ P_{(n,p)} = \\frac{n!}{(n-p)!} \\] A fórmula para \\(P_{(n,p)}\\) considera tanto a seleção de \\(p\\) elementos quanto a ordenação deles, de modo que qualquer alteração na ordem dos elementos resulta em um novo arranjo. Dessa forma, arranjos com os mesmos objetos em ordens distintas são considerados distintos. Exemplo: quantos agrupamentos distintos, formados por 3 letras cada, podem ser formados com as 7 letras: A, B, C, D, E, F, G, considerando que não é permitido repetir nenhuma letra e que a ordem dos elementos importa? \\[\\begin{align*} n &amp; = 7 \\\\ p &amp; = 3 \\\\ P_{(n,p)} &amp; = \\frac{7!}{ (7-3)!} \\\\ &amp; = \\frac{7!}{4!} = \\\\ &amp; = \\frac{ 7 \\times 6 \\times 5 \\times 4! }{4!} \\\\ &amp; = 7 \\times 6 \\times 5 = 210 \\end{align*}\\] \\(ABC, ACB, BAC, BCA, CAB, CBA, \\dots\\) 2.9.5 Arranjos com repetição Considere um conjunto de \\(n\\) objetos diferentes. Suponha que desejamos selecionar uma quantidade \\(p\\) desses objetos, onde \\(p &lt; n\\), e dispor os \\(p\\) objetos escolhidos em uma ordem específica. Ao permitirmos a repetição dos objetos no agrupamento, cada posição pode ser ocupada por qualquer um dos \\(n\\) objetos, independentemente das escolhas anteriores. Dessa forma, o número de arranjos com repetição de \\(p\\) elementos selecionados de um conjunto de nn objetos distintos é dado por: \\[ P_{(n,p)} = n ^{p} \\] Essa fórmula ocorre porque, ao preencher cada uma das \\(p\\) posições com qualquer um dos \\(n\\) objetos, multiplicamos \\(n\\) possibilidades para cada posição, resultando em: \\[ n \\times n \\times \\ldots \\times n = n^p \\] Exemplo: Quantos agrupamentos diferentes (onde a ordem dos elementos é razão para distinção: permutações) formados por 3 letras cada podem ser formados com as 7 letras: A, B, C, D, E, F, G com repetição? \\[\\begin{align*} n &amp; = 7 \\\\ p &amp; = 3 \\\\ P_{(n,p)} &amp; = n^{p} \\\\ &amp; = 7 ^{3} = 343 \\end{align*}\\] Primeira posição: temos 7 opções (uma das 7 letras). Segunda posição: também temos 7 opções, pois a repetição é permitida. Terceira posição: novamente, temos 7 opções. \\(AAA, AAB, AAC, \\dots\\) 2.9.6 Combinações sem repetição Em uma permutação, a ordem dos objetos em cada agrupamento é essencial: qualquer alteração na ordem cria um agrupamento distinto. Por exemplo, o agrupamento abc é diferente de bca numa permutação, pois a ordem importa. Em muitos problemas, entretanto, estamos interessados somente na seleção dos objetos, sem considerar a ordem em que eles aparecem. Nesse caso, os agrupamentos onde os mesmos elementos aparecem em ordens diferentes são considerados equivalentes. Tais seleções são chamadas de combinações. Por exemplo, abc e bca representam uma mesma combinação, pois contêm os mesmos elementos independentemente da ordem. O conceito de uma combinação refere-se a um conjunto de \\(n\\) objetos distintos, dos quais escolhemos \\(p\\) objetos \\((p &lt; n)\\) sem repetição. Assim: a ordem não importa: então \\(abc\\) é igual a \\(bca\\) (os agrupamentos que contêm os mesmos objetos em qualquer ordem são considerados iguais); a repetição não permitida: cada objeto só aparece uma vez em cada agrupamento. O número total de combinações possíveis de \\(p\\) objetos selecionados de um conjunto de \\(n\\) objetos distintos é dado por: \\[ C_{(n,p)} = \\frac{n!}{p! \\times (n-p)!} \\] em que o numerador (\\(n!\\)) representa o número total de maneiras de permutar todos os \\(n\\) objetos, o denominador (\\(p! \\times (n - p)!\\)) remove as redundâncias causadas pela permutação dos \\(p\\) objetos escolhidos (\\(p!\\)) e das maneiras de ordenar os \\((n - p)\\) objetos restantes (\\((n - p)!\\)). A fórmula resulta no número de agrupamentos únicos onde apenas a composição do conjunto importa, e a ordem dos elementos dentro de cada conjunto não afeta a contagem. Exemplo: Qual é número de formas nas quais \\(3\\) cartas podem ser escolhidas ou selecionadas de um total de \\(8\\) cartas diferentes? \\[\\begin{align*} n &amp; = 8 \\\\ p &amp; = 3 \\\\ C_{(n,p)} &amp; = \\frac{n!}{p! \\times (n - p)!}\\\\ C_{(8,3)} &amp; = \\frac{8!}{ 3! (8-3)!} \\\\ &amp; = \\frac{8!}{3! \\times 5!} \\\\ &amp; = \\frac{ 8 \\times 7 \\times 6 \\times 5! }{ 3! \\times 5! } \\\\ &amp; = \\frac{ 8 \\times 7 \\times 6 }{3!} = 56 \\end{align*}\\] O número total de combinações com repetição, de \\(p\\) objetos selecionados de \\(n\\) (também chamado de combinações de \\(n\\) elementos tomados \\(p\\) a cada vez com repetição) é representado por: \\[ C_{(n+p-1,p)} = \\frac{ (n+p-1)! }{ p! \\times ( n-1)!} \\] Exemplo: um comitê de três pessoas deve ser formado a partir de um grupo de 20 pessoas. Quantos comitês diferentes são possíveis? \\[\\begin{align*} n &amp; = 20 \\\\ p &amp; = 3 \\\\ C_{(n,p)} &amp; = \\frac{n!}{p! \\times (n - p)!}\\\\ C_{(20,3)} &amp; = \\frac{20!}{3! \\times (20 - 3)!}\\\\ &amp; = \\frac{20 \\times 19 \\times 18 \\times 17!}{3! \\times 17!}\\\\ &amp; = \\frac{20 \\times 19 \\times 18}{3 \\times 2 \\times 1}\\\\ &amp; = \\frac{6840}{6} = 1140 \\end{align*}\\] Exemplo: de um grupo de cinco mulheres e sete homens, quantos comitês diferentes formados por duas mulheres e três homens podem ser formados? E se dois dos homens estiverem brigados e se recusarem a trabalhar juntos? Para resolver esse problema, vamos dividi-lo em duas partes: calcular o número de comitês possíveis formados por duas mulheres e três homens (combinações \\(C_{(5,2})\\) e \\(C_{(7,3})\\)) Calcular o número de comitês possíveis se dois dos homens estiverem brigados e se recusarem a trabalhar juntos. Parte 1: Comitês sem restrição Escolha das Mulheres: Temos 5 mulheres e precisamos escolher 2. O número de maneiras de fazer isso é uma combinação, dada por: \\[ C_{(5, 2)} = \\frac{5!}{2! \\times (5 - 2)!} = \\frac{5 \\times 4}{2 \\times 1} = 10 \\] Escolha dos Homens: Temos 7 homens e precisamos escolher 3. O número de maneiras de fazer isso também é uma combinação, dada por: \\[ C_{(7, 3)} = \\frac{7!}{3! \\times (7 - 3)!} = \\frac{7 \\times 6 \\times 5}{3 \\times 2 \\times 1} = 35 \\] O número total de comitês é o produto das duas combinações (princípio básico): \\[ \\text{Total} = C_{(5, 2)} \\times C_{(7, 3)} = 10 \\times 35 = 350 \\] Portanto, sem restrições, há 350 comitês diferentes que podem ser formados. Parte 2: Comitês com restrição (Dois Homens Brigados) Para calcular o número de comitês possíveis com essa restrição, usamos o princípio da exclusão: (total de comitês sem restrição - total de comitês com restrição). Começamos contando quantos comitês incluem os dois homens brigados: \\(A\\) e \\(B\\). Se ambos estão no comitê então precisamos escolher apenas mais 1 homem entre os 5 homens restantes. O número de maneiras de fazer isso é: \\[ C_{(5, 1)} = 5 \\] Então, do total de 35 possíveis grupos de 3 homens formados sem restrição apenas 5 são grupos onde \\(A\\) e \\(B\\) estão presentes, resultando então em \\(35-5=30\\) grupos onde \\(A\\) e \\(B\\) estão ausentes. Compondo com o número de possíveis grupos de mulheres (princípio básico) teremos: \\[ \\text{Total} = C_{(5, 2)} \\times30 = 10 \\times 30 = 300 \\] Portanto, com essa restriçãose, há 300 comitês diferentes que podem ser formados. 2.9.7 Combinações com Repetição Em muitos problemas de contagem, estamos interessados em selecionar um subconjunto de objetos de um conjunto maior, sem nos preocupar com a ordem dos objetos. Isso é chamado de combinação. No entanto, ao contrário das combinações convencionais, em alguns casos é permitido que os mesmos objetos sejam escolhidos mais de uma vez. Essas são conhecidas como combinações com repetição. Nas combinações com repetição, selecionamos \\(p\\) objetos a partir de um conjunto de \\(n\\) objetos distintos, permitindo que cada objeto seja escolhido mais de uma vez. Nesse contexto: a ordem dos objetos não importa, ou seja, uma seleção de A,B,A é considerada igual a A,A,B e a repetição é permitida, então podemos escolher o mesmo objeto mais de uma vez. Combinações com repetição são úteis em contextos onde é necessário selecionar subconjuntos com elementos repetidos, como: - escolha de itens com reposição (como selecionar moedas em valores específicos). - problemas de contagem em álgebra combinatória. - distribuição de recursos em cenários onde itens podem ser alocados mais de uma vez. O número de combinações com repetição para selecionar \\(p\\) objetos de um conjunto de \\(n\\) objetos distintos é dado por: \\[ C_{(n + p - 1, p)} = \\frac{(n + p - 1)!}{p! \\times (n - 1)!} \\] Essa fórmula reflete o fato de que, ao permitir repetições, cada seleção possível pode ser visualizada como uma combinação com um conjunto “expandido” de opções, onde as escolhas de um mesmo objeto múltiplas vezes são válidas e contadas. Exemplo: quantas maneiras existem de selecionar 3 frutas de um conjunto com 5 tipos diferentes (maçã, banana, laranja, uva e pera), onde repetições são permitidas? \\[\\begin{align*} n &amp; = 5 \\\\ p &amp; = 3 \\\\ C_{(n + p - 1, p)} &amp; = \\frac{(n + p - 1)!}{p! \\times (n - 1)!}\\\\ C_{(5 + 3 - 1, 3)} &amp; = \\frac{(5 + 3 - 1)!}{3! \\times (5 - 1)!} = \\frac{7!}{3! \\times 4!}\\\\ &amp; = \\frac{7 \\times 6 \\times 5 \\times 4!}{3! \\times 4!} = \\frac{7 \\times 6 \\times 5}{3 \\times 2 \\times 1} = \\frac{210}{6} = 35 \\end{align*}\\] Exemplo: supondo que você queira comprar um sorvete com 4 bolas em uma sorveteria que possui 3 sabores disponíveis: chocolate, baunilha e morango. De quantos modos diferentes você pode fazer esta compra? (Note que nesta combinação é possível repetir a ordem de dois ou mais sabores, assim tratando de uma combinação com repetição). \\[\\begin{align*} n &amp; = 3 \\\\ p &amp; = 4 \\\\ C_{(n + p - 1, p)} &amp; = \\frac{(n + p - 1)!}{p! \\times (n - 1)!}\\\\ C_{(n+p-1,p)} &amp; = \\frac{(3+4-1)!}{ 4! (3-1)!} = 15 \\end{align*}\\] "],["fatoriais.html", "2.10 Fatoriais", " 2.10 Fatoriais \\(n!=n.(n-1).(n-2)\\dots(n-(n-1)).(n-n)\\) \\(1!=1\\) \\(0!=1\\) \\[\\begin{align*} P_{(n,n)} &amp; = \\frac{n!}{(n-n)!} = \\frac{n!}{0!} = n! \\\\ C_{(n,0)} &amp; = \\frac{n!}{ 0! \\times (n-0)! } = \\frac{n!}{ 1 \\times (n)!} = 1 \\\\ C_{(n,1)} &amp; = \\frac{n! }{ 1! (n-1)!} \\\\ &amp; = \\frac{ n! }{(n-1)! } \\\\ &amp; = \\frac{ n \\times (n-1)! }{ (n-1)!} = n \\end{align*}\\] "],["conectivos-lógicos.html", "2.11 Conectivos lógicos", " 2.11 Conectivos lógicos Muitos dos problemas ligados à probabilidade de ocorrência de eventos são propostos com o auxílio de conectivos lógicos: Proposição: a afirmação de que algo é verdadeiro. Após analisarmos qualquer proposição, podemos defini-la como verdadeira ou falsa como, por exemplo: “o céu é azul”; Negação: negação do valor lógico de uma proposição. A negação de uma proposição verdadeira é falsa. A negação de uma proposição falsa é verdadeira. Os símbolos da negação são o til \\(^{-}\\) ou \\(^{c}\\); Conjunção: proposição composta com a utilização do conectivo “e” como, por exemplo: “o céu é azul e as nuvens são brancas”. Os símbolos usuais para uma conjunção são: \\(\\cap\\) ou a letra “V” invertida; e, Disjunção: proposição composta com a utilização do conectivo “ou” como, por exemplo, “o céu é azul ou os pássaros são pretos”. Os símbolos usuais para uma disjunção são: \\(\\cup\\) ou a letra \\(V\\). "],["leis-de-de-morgan.html", "2.12 Leis de De Morgan", " 2.12 Leis de De Morgan Augustus de Morgan foi um matemático e lógico indiano. Figure 2.12: Augustus De Morgan (1806 - 1871) Primeira Lei de De Morgan: Negar duas proposições ligadas com “e” (\\(\\cap\\)); ou seja, uma conjunção, é o mesmo que negar duas proposições e ligá-las com “ou”’ (ou seja, transformá-las em uma disjunção). Considerando as proposições “p” e “q” teremos: \\(\\sim (p \\cap q) = (~p) \\cup (~q)\\); ou, \\((p \\cap q)^{c} = (p^{c}) \\cup (q^{c})\\). Segunda Lei de De Morgan: Negar duas proposições ligadas por “ou”’ (\\(\\cup\\)); ou seja, uma disjunção, é o mesmo que negar as duas proposições e ligá-las com “e” (ou seja, transformá-las em uma conjunção). Considerando as proposições “p” e “q” teremos: \\(\\sim (p \\cup q) = (~p) \\cap (~q)\\); ou, \\((p \\cup q)^{c}= (p^{c}) \\cap (q^{c})\\). "],["noções-básicas-para-o-uso-de-calculadora-cassio-fx-82ms.html", "2.13 Noções básicas para o uso de calculadora (Cassio fx-82MS)", " 2.13 Noções básicas para o uso de calculadora (Cassio fx-82MS) Em estatística trabalha-se muito com a análise de um ou mais conjuntos de dados, sendo comum a realização de diversas operações matemáticas com esses dados. Muitas dessas operações envolvem somatórios, por exemplo, e para simplificar essas operações o uso da calculadora se torna essencial. Neste curso recomenda-se o uso de uma calculadora científica. Existem diversas calculadoras que cumprem as funções necessárias nesse curso. Para padronizar as aulas, alguns professores sugerem a calculadora científica de código: FX82MS, que é a calculadora que cujo funcionamento será exibido a seguir, passo a passo. A seguir serão descritas algumas das funções básicas mais importantes no uso desta calculadora. Primeiro vamos deixar a calculadora no modo de regressão linear. Esse modo permite que a calculadora funcione normalmente para as operações comuns (soma, subtração, multiplicação e divisão), e ainda libera todas as funções importantes nesse curso. Sempre que o aluno for utilizar a calculadora, ele deve se certificar que ela esteja no modo de regressão linear, da seguinte forma: PASSO 1: ON MODE Aperte 3 para escolher REG Aperte 1 para escolher LIN Repare que no topo do visor da calculadora apareceu o símbolo REG, que indica que a calculadora está em modo de regressão. Desde que esteja no modo de regressão, podemos passar para o passo seguinte. O nosso objetivo aqui é inserir o conjunto de dados na calculadora para então realizarmos as operações necessárias. Mas antes de inserir os dados, temos que garantir que a calculadora esteja vazia para o novo conjunto de dados. Ou seja, devemos limpar a calculadora: PASSO 2: SHIFT MODE Aperte 1 para escolher Scl ( Stat Clear) Aperte = para limpar a calculadora Entrada de dados. Agora que a calculadora está em modo de regressão e está limpa, podemos inserir o conjunto de dados. Para ilustrar esta função, vamos inserir o seguinte conjunto de dados: \\(X= {5,3,6,2}\\). Para inserir cada um desses elementos você deve digitar o número e em seguida o botão M+. A sequência fica assim: 5 M+ 3 M+ 6 M+ 2 M+. A cada vez que você insere uma observação, a calculadora atualiza o número de observações inseridas. No final, nesse caso, aparece n=4 porque inserimos 4 observações. Funções envolvendo somatórios. Observe na calculadora os botões shift e alpha. Geralmente estes botões aparecem nas cores amarela e vermelha, respectivamente. Observe ainda que alguns botões da calculadora possuem termos nessas cores. Para selecionar as funções em amarelo, antes devemos ligar o modo shift. Enquanto que para selecionar as funções em vermelho deve-se ligar o modo alpha. Por exemplo, para abrir a função S-SUM que está em amarelo no botão 1, faz-se: SHIFT 1. A função S-SUM é a que contém todos os somatórios importantes. Ao abrir esta função aparecem três opções da seguinte forma: \\[ \\Sigma(x) \\\\ \\Sigma(x^{2})\\\\ n \\] Aperta-se 1 = para ter o somatório de \\(x\\); 2 = para ter o somatório de \\(x^{2}\\) ou 3 = para saber o número \\(n\\) de obervações inseridas. Funções para obter a média e o desvio padrão. A função S-VAR fornece a média e o desvio padrão dos dados. Essas são medidas importantes, que serão utilizadas durante todo o curso. Para abrir esta função faz-se: SHIFT 2. \\[ \\stackrel{-}{x} \\sigma_{x} S_{x} \\] A opção 1 retorna a média dos dados, a opção 2 retorna o desvio padrão populacional e a opção 3 o desvio padrão amostral. Como inserir dois conjuntos de dados. Quando se deseja estudar dois conjuntos de dados, de mesmo tamanho, pode-se inseri-los de forma simultânea na calculadora. Para ilustrar vamos inserir os seguintes conjuntos de dados: \\(X={2,7,4,3,2}\\) e \\(Y={1,2,3,6,5}\\). Antes de inserir os dados, lembre-se de limpar a calculadora. Em seguida vamos inserir os dados de 2 em 2: o primeiro de X com o primeiro de Y e assim por diante. Repare que ao lado do botão M+ tem um botão com uma vírgula. Esta vírgula é utilizada para separar as observações de X das de Y . A sequência fica assim: 2,1 M+ 7,2 M+ 4,3 M+ 3,6 M+ 2,5 M+ Se você usar a função S-SUM, na tela vai aparecer os somatórios apenas de X, que foi pela ordem, o primeiro a ser inserido. Na calculadora tem um botão grande e style=“color:gray;”&gt;S-SUM, com 4 setas. Depois de selecionar a função amarelo aperte a seta para frente que aparecerão os somatórios para Y . O mesmo acontece para a função S-VAR. Figure 2.13: Calculadora Cassio "],["instalação-do-software-r-em-conjunto-com-a-interface-gráfica-rstudio.html", "2.14 Instalação do software R em conjunto com a interface gráfica RStudio", " 2.14 Instalação do software R em conjunto com a interface gráfica RStudio “A pergunta não é se o R faz; mas sim, como ele faz […] (anônimo)” R é uma linguagem e ambiente para computação estatística e gráficos. É um projeto GNU que é semelhante à linguagem e ambiente S que foi desenvolvido nos Laboratórios Bell (anteriormente AT&amp;T, agora Lucent Technologies) por John Chambers e colegas. R pode ser considerado como uma implementação diferente de S. Existem algumas diferenças importantes, mas muito código escrito para S roda inalterado sob R. R fornece uma ampla variedade de técnicas estatísticas (modelagem linear e não linear, testes estatísticos clássicos, análise de séries temporais, classificação, clustering, …) e gráficas, e é altamente extensível. A linguagem S costuma ser o veículo escolhido para pesquisa em metodologia estatística, e R fornece uma rota de código aberto para participação nessa atividade. Um dos pontos fortes do R é a facilidade com que gráficos de qualidade de publicação bem projetados podem ser produzidos, incluindo símbolos matemáticos e fórmulas quando necessário. Grande cuidado foi tomado sobre os padrões para as escolhas de design menores em gráficos, mas o usuário mantém o controle total. R está disponível como Software Livre sob os termos da Licença Pública Geral GNU da Free Software Foundation em forma de código-fonte. Ele compila e roda em uma ampla variedade de plataformas UNIX e sistemas similares (incluindo FreeBSD e Linux), Windows e MacOS. R é um conjunto integrado de recursos de software para manipulação de dados, cálculo e exibição gráfica. Inclui: uma instalação eficaz de manipulação e armazenamento de dados, um conjunto de operadores para cálculos em arrays, em particular matrizes, uma coleção grande, coerente e integrada de ferramentas intermediárias para análise de dados, facilidades gráficas para análise de dados e exibição na tela ou em cópia impressa, e uma linguagem de programação bem desenvolvida, simples e eficaz que inclui condicionais, loops, funções recursivas definidas pelo usuário e recursos de entrada e saída. O termo “ambiente” destina-se a caracterizá-lo como um sistema totalmente planejado e coerente, em vez de um acréscimo incremental de ferramentas muito específicas e inflexíveis, como é frequentemente o caso de outros softwares de análise de dados. R, como S, é projetado em torno de uma verdadeira linguagem de computador e permite aos usuários adicionar funcionalidades adicionais definindo novas funções. Grande parte do sistema é escrito no dialeto R de S, o que torna mais fácil para os usuários seguirem as escolhas algorítmicas feitas. Para tarefas de computação intensiva, os códigos C, C++ e Fortran podem ser vinculados e chamados em tempo de execução. Usuários avançados podem escrever código C para manipular objetos R diretamente. Muitos usuários pensam no R como um sistema estatístico. Preferimos pensar nisso como um ambiente no qual as técnicas estatísticas são implementadas. R pode ser estendido (facilmente) via packages . Existem cerca de oito pacotes fornecidos com a distribuição R e muitos mais estão disponíveis através da família CRAN de sites da Internet, cobrindo uma ampla gama de estatísticas modernas. R tem seu próprio formato de documentação semelhante ao LaTeX, que é usado para fornecer documentação abrangente, tanto on-line em vários formatos quanto em cópia impressa. A página principal pode ser acessa em: The R Project for Statistical Computing e as informações acima foram traduzidas de Fonte das informações. 2.14.1 RStudio RStudio é um ambiente de desenvolvimento integrado (IDE) para R e Python. Ele inclui um console, editor de realce de sintaxe que oferece suporte à execução direta de código e ferramentas para plotagem, histórico, depuração e gerenciamento de espaço de trabalho. O RStudio está disponível em código aberto e edições comerciais e é executado na área de trabalho (Windows, Mac e Linux). A página principal pode ser acessada em: RStudio. Há inúmeros tutoiais para a instalação do \\(R\\) e o \\(RStudio\\) (uma IDE: Integrated development environment para poder utilizar o software de um mod mais amigável), dentre os quais: Tutorial de instalação (UFPr). 2.14.2 Pacotes Os pacotes na linguagem de programação R são um conjunto de funções R , código compilado e dados de amostra. Estes são armazenados em um diretório chamado “biblioteca” dentro do ambiente R. Por padrão, o R instala um grupo de pacotes durante a instalação. Assim que iniciarmos o console R, apenas os pacotes padrão estarão disponíveis por padrão. Outros pacotes que já estão instalados precisam ser carregados explicitamente para serem utilizados pelo programa R que os usará. Uma lista de todos os pacotes disponibilizados para os mais variados problemas de anáise estatística pode ser vista em Lista de pacotes. "],["introdução-à-estatística-descritiva.html", "Capítulo 3 Introdução à estatística descritiva", " Capítulo 3 Introdução à estatística descritiva Sobre o estudo da estatística por áreas nas quais, aparentemente, não se vislumbra sua utilidade trazemos o prefácio da tradução do livro de Jack Levin (Estatística aplicada às ciências humanas) por Sérgio Francisco Costa, ao dizer que o livro: “[…] destina-se a um público muito específico: estudantes de Ciências Humanas, refúgio errôneo dos que fogem das equações e dos cálculos, pois que, embora humanas - e talvez por isso mesmo - não podemos prescindir das tão odiadas quantificações […]” "],["análise-exploratória.html", "3.1 Análise exploratória", " 3.1 Análise exploratória A análise exploratória de dados ( EDA: Exploratory Data Analisys , originalmente desenvolvida pelo matemático e estatístico norte-americano John Tukey na década de 1970) é usada para se investigar conjuntos de dados e resumir suas principais características, muitas vezes usando métodos de visualização de dados por gráficos e apresentação de tabelas. Figure 3.1: John Tukey (1915-2000) Habitualmente uma EDA envolve: verificar quais são os tipos de variáveis presentes nos dados; verificar os padrões de cada variável e eventuais associações entre duas ou mais delas; e, apresentar os valores assumidos por cada uma das variáveis em formas resumidas como: resumos (sínteses) numéricas tabulares e gráficos. "],["dados-brutos-em-rol-diagrama-de-ramos-folhas-e-de-dispersão-unidimensional.html", "3.2 Dados brutos, em rol, diagrama de ramos &amp; folhas e de dispersão unidimensional", " 3.2 Dados brutos, em rol, diagrama de ramos &amp; folhas e de dispersão unidimensional Consideremos os dados obtidos da medição das alturas em metros de 60 estudantes de uma determinada classe de um certo curso aqui na UEL: alturas=c(1.63,1.67,1.47,1.64,1.66,1.73,2.00,1.62,1.65,1.56,1.65,1.85,1.73, 1.78,1.82,1.68,1.67,1.83,1.72,1.71,1.73,1.67,1.66,1.95,1.76,1.73, 1.77,1.68,1.65,1.64,1.66,1.68,1.61,1.73,1.72,1.83,1.69,1.84,1.66, 1.78,1.54,1.74,1.56,1.66,1.56,1.62,1.55,1.86,1.44,1.67,1.76,1.79, 1.75,1.41,1.65,1.58,1.93,1.57,1.71,1.58,0.1,3.68,0,NA) alturas ## [1] 1.63 1.67 1.47 1.64 1.66 1.73 2.00 1.62 1.65 1.56 1.65 1.85 1.73 1.78 1.82 ## [16] 1.68 1.67 1.83 1.72 1.71 1.73 1.67 1.66 1.95 1.76 1.73 1.77 1.68 1.65 1.64 ## [31] 1.66 1.68 1.61 1.73 1.72 1.83 1.69 1.84 1.66 1.78 1.54 1.74 1.56 1.66 1.56 ## [46] 1.62 1.55 1.86 1.44 1.67 1.76 1.79 1.75 1.41 1.65 1.58 1.93 1.57 1.71 1.58 ## [61] 0.10 3.68 0.00 NA Garbage in, garbage out. Não são raras as vezes nas quais o relatório com os dados coletados em uma pesquisa apresentam uma série de erros. Não estamos a nos refeir aqui aos erros amostrais mas sim aos erros experimentais (não amostrais), aqueles decorrentes de dados coletados incorretamente, tais como aqueles resultantes de omissões na transcrição das informações, da leitura de instrumentos descalibrados ou de informações simplesmente não coletadas. Denomina-se pré-processamento essa etapa de limpeza do conjunto de dados na qual busca-se corrigir de mdo extremamente criterioso esses problemas e, para tanto, um profundo conhecimento do objeto que está sendo pesquisado é necessário de modo a não serem liminarmente eliminados dados simplesmente por destoarem da alguma tendência (para essas tituações há ferramentas estatísticas apropriadas). O conjunto original de dados ( dataset) refere-se a alturas de pessoas (estudantes ) e assim, tata-se de uma variável quantitativa e contínua e como tal será analisada. As omissões de informação “NA” ( not available) e as medidas transcritas com erros grosseiros (0 m; 0,10 m; 3,68 m) serão removidas. Assim, o dataset será composto pelos dados abaixo: alturas=c(1.63,1.67,1.47,1.64,1.66,1.73,2.00,1.62,1.65,1.56,1.65,1.85,1.73, 1.78,1.82,1.68,1.67,1.83,1.72,1.71,1.73,1.67,1.66,1.95,1.76,1.73, 1.77,1.68,1.65,1.64,1.66,1.68,1.61,1.73,1.72,1.83,1.69,1.84,1.66, 1.78,1.54,1.74,1.56,1.66,1.56,1.62,1.55,1.86,1.44,1.67,1.76,1.79, 1.75,1.41,1.65,1.58,1.93,1.57,1.71,1.58) alturas ## [1] 1.63 1.67 1.47 1.64 1.66 1.73 2.00 1.62 1.65 1.56 1.65 1.85 1.73 1.78 1.82 ## [16] 1.68 1.67 1.83 1.72 1.71 1.73 1.67 1.66 1.95 1.76 1.73 1.77 1.68 1.65 1.64 ## [31] 1.66 1.68 1.61 1.73 1.72 1.83 1.69 1.84 1.66 1.78 1.54 1.74 1.56 1.66 1.56 ## [46] 1.62 1.55 1.86 1.44 1.67 1.76 1.79 1.75 1.41 1.65 1.58 1.93 1.57 1.71 1.58 Esse conjunto de dados certamente contém diversas informações acerca da altura dessas pessoas; todavia, da maneira como está apresentado a compreensão dessas informações fica bastante comprometida. "],["resumos-descritivos.html", "3.3 Resumos descritivos", " 3.3 Resumos descritivos Esse modo de apresentação é chamado de dados brutos. Com um pequeno refinamento, como pela simples ordenação desses dados (são medidas numéricas contínuas), algumas informações começam a se destacar: sort(alturas) ## [1] 1.41 1.44 1.47 1.54 1.55 1.56 1.56 1.56 1.57 1.58 1.58 1.61 1.62 1.62 1.63 ## [16] 1.64 1.64 1.65 1.65 1.65 1.65 1.66 1.66 1.66 1.66 1.66 1.67 1.67 1.67 1.67 ## [31] 1.68 1.68 1.68 1.69 1.71 1.71 1.72 1.72 1.73 1.73 1.73 1.73 1.73 1.74 1.75 ## [46] 1.76 1.76 1.77 1.78 1.78 1.79 1.82 1.83 1.83 1.84 1.85 1.86 1.93 1.95 2.00 A interpretabilidade das informações trazidas por esses dados começa a ficar mais fácil como, por exemplo, as alturas: mínima; e, máxima dos estudantes. A uma listagem de valores ordenada (de modo crescente ou decrescente) dá-se o nome de rol. Além da apresentação elementar de algumas informações relacionadas aos dados brutos da amostra, tais como os valores mínimo e máximo observados, a estatística descritiva possui muitas outras ferramentas para condensar e expor a informação trazida pelos dados por meio resumos (sínteses) descritivos: numéricos tabulares gráficos. Resumos (sínteses) numéricos descritivos são quantidades que condensam variados aspectos relacionados aos valores dos dados. As mais conhecidas sínteses numéricas podem ser agrupadas conforme o aspecto que expõem dos dados: tendência central (posição): média (simples ou aritmética, geométrica, harmônica, anarmônica, quadrática, biquadrática), moda e mediana; dispersão (variabilidade): absolutas (amplitude total, variância e desvio padrão) ou relativas (coeficiente de variação, unidades padronizadas); e, subdivisão (separatrizes, quantis): mediana (50%), quartis (25%, 50%, 75%), decis (10%, ….90%) e percentis (1%….99%). Uma medida de posição ou dispersão é dita resistente quando forem pouco afetadas pela alteração de uma pequena porção dos dados. A mediana é uma medida resistente, já a média e a variância não são. 3.3.1 Estimadores Figure 3.2: Viés e variância 3.3.2 Medidas de tendência central (posição) 3.3.2.1 Média Sejam \\(x_{1}, x_{2}, ..., x_{n}\\) os \\(n\\) valores assumidos pela variável \\(X\\) (dados brutos). A média aritmética simples será dada por: \\[ \\stackrel{-}{x}=\\frac{\\sum _{i=1}^{n}{x}_{i}}{n} \\] Algumas propriedades da média aritmética: somando-se (ou subtraindo-se) cada um dos elementos do conjunto de dados por uma constante arbitrária qualquer \\(k\\), a média aritmética ficará adicionada (ou subtraída) dessa essa constante \\(k\\) alturas_ad=alturas+0.05 par(mfrow=c(1,2)) stripchart(alturas,method = &quot;stack&quot;, at=0.5, main=&quot;&quot;,pch = 20, col=&quot;blue&quot;, cex=1, xlab=&quot;Alturas originais dos estudantes (m)&quot;, ylab=&quot;Quantidades observadas (un)&quot;) abline(v=mean(alturas), col=&quot;red&quot;) text(mean(alturas)-0.2, 1, &quot;Média=1,69 m&quot;, col = &quot;red&quot;, srt=90) stripchart(alturas_ad,method = &quot;stack&quot;, at=0.5, main=&quot;&quot;,pch = 20, col=&quot;blue&quot;, cex=1, xlab=&quot;Alt. dos estudantes (m) adic. de 5cm&quot;, ylab=&quot;Quantidades observadas (un)&quot;) abline(v=mean(alturas_ad), col=&quot;red&quot;) text(mean(alturas_ad)-0.2, 1, &quot;Média=1,74 m&quot;, col = &quot;red&quot;, srt=90) Figure 3.3: Mudanças na média pela adição (subtração) de uma constante \\(k=0.05\\) multiplicando-se (ou dividindo-se) cada um dos elementos do conjunto de dados por uma constante arbitrária \\(k\\), a média aritmética ficará multiplicada (ou dividida) por essa constante \\(k\\) alturas_mult=alturas*1.2 par(mfrow=c(1,2)) stripchart(alturas,method = &quot;stack&quot;, at=0.5, main=&quot;&quot;,pch = 20, col=&quot;blue&quot;, xlab=&quot;Alturas originais dos estudantes (m)&quot;, ylab=&quot;Quantidades observadas (un)&quot;) abline(v=mean(alturas), col=&quot;red&quot;) text(mean(alturas)-0.1, 1, &quot;Média=1,69 m&quot;, col = &quot;red&quot;, srt=90) stripchart(alturas_mult,method = &quot;stack&quot;, at=0.5, main=&quot;&quot;,pch = 20, col=&quot;blue&quot;, xlab=&quot;Alt. dos estudantes (m) mult. por 1,2&quot;, ylab=&quot;Quantidades observadas (un)&quot;) abline(v=mean(alturas_mult), col=&quot;red&quot;) text(mean(alturas_mult)-0.1, 1, &quot;Média= 2,02 m&quot;, col = &quot;red&quot;, srt=90) Figure 3.4: Mudanças na média pela multiplicação (divisão) de uma constante \\(k=1.2\\) a soma dos desvios observados entre cada um dos valores assumidos pela variável \\(X\\) e sua média \\(\\stackrel{-}{x}\\) é nula; a soma dos quadrados dos desvios é mínima; em uma distribuição de frequências, a soma dos produtos dos desvios entre a média o valor médio de cada uma das classes, pelas respectivas frequências é nula; e, multiplicando-se (ou dividindo-se) todas as frequências de uma distribuição por uma constante arbitrária, a média aritmética não se altera. Usando os dados das medidas das alturas dos 60 estudantes teremos o seguinte valor para a média: round(mean(alturas),2) ## [1] 1.69 3.3.2.2 Moda Moda é o valor que ocorre com maior frequência na amostra. Uma amostra pode se apresentar como: unimodal; bimodal; plurimodal; ou, amodal. tab_alturas=table(alturas) tab_alturas ## alturas ## 1.41 1.44 1.47 1.54 1.55 1.56 1.57 1.58 1.61 1.62 1.63 1.64 1.65 1.66 1.67 1.68 ## 1 1 1 1 1 3 1 2 1 2 1 2 4 5 4 3 ## 1.69 1.71 1.72 1.73 1.74 1.75 1.76 1.77 1.78 1.79 1.82 1.83 1.84 1.85 1.86 1.93 ## 1 2 2 5 1 1 2 1 2 1 1 2 1 1 1 1 ## 1.95 2 ## 1 1 barplot(tab_alturas, main=&quot;Valores observados da alturas dos estudantes&quot;, xlab=&quot;Altura (cm)&quot;, ylab=&quot;Quantidade observada (un)&quot;, ylim=c(0,6), col=&quot;blue&quot;, las=0, hor=&quot;FALSE&quot;) Figure 3.5: Bimodal: 1,66 m e 1,73 m Usando os dados das medidas das alturas dos 60 estudantes teremos os seguintes valores para a moda: # função em R para extrair a moda: Modes &lt;- function(x) { ux &lt;- unique(x) tab &lt;- tabulate(match(x, ux)) ux[tab == max(tab)] } Modes(alturas) ## [1] 1.66 1.73 3.3.3 Medidas de dispersão (variabilidade) O conhecimento de uma medida de tendência central nos provê uma informação útil mas incompleta. As medidas de dispersão nos ajudam a ter uma perspectiva melhor dos dados. amplitude total dos dados; desvio padrão (variância): é considerada a mais útil das medidas de dispersão; coeficiente de variação; e, unidades padronizadas. Diferentes tipos quanto à dimensão (unidade): medidas absolutas são aquelas expressas na mesma unidade de medida da variável do fenômeno estudado (\\(m;kg;\\frac{R\\$}{mês};\\dots\\)); medidas relativas são adimiensionais e assim podem ser usadas para se comparar a variabilidade de dois ou mais conjuntos de dados, mesmo quando as variáveis se refiram a diferentes fenômenos ou que sejam expressas, originalmente, em diferentes unidades. 3.3.3.1 Amplitude total dos dados A amplitude total dos dados é a simples diferença entre o maior e o menor dos valores observados: \\[ A=x_{max} - x_{min} \\] 3.3.3.2 Variância (e desvio padrão) Sejam \\(x_{1}, x_{2}, ..., x_{n}\\) os \\(n\\) valores assumidos pela variável \\(X\\). Dá-se o nome de desvios a contar da média as diferenças entre cada uma das observações e a média: \\(x_{i} - \\stackrel{-}{x}\\) com \\(i=1,2,...,n\\). Não é possível considerar a possibilidade de se adotar o valor médio desses desvios pois uma das propriedades da média é que a soma dos desvios em torno de si é nula. \\[ \\stackrel{-}{d} = \\frac{\\sum _{i=1}^{n}\\left(x_{i}-\\stackrel{-}{x}\\right)}{n} \\] \\[ \\sum _{i=1}^{n}\\left(x_{i}-\\stackrel{-}{x}\\right)=0 \\] constitui-se numa restrição linear dos desvios porque qualquer \\(n-1\\) deles completamente determina o outro. Tampouco se considera a possibilidade de se adotar o valor médio desses desvios em módulo, pelas dificuldades teóricas em problemas de inferência. \\[ \\stackrel{-}{d} = \\frac{\\sum _{i=1}^{n}\\left|x_{i}-\\stackrel{-}{x}\\right|}{n} \\] Uma alternativa é adotar o valor médio do quadrado desses desvios. \\[ S^{2}=\\frac{\\sum _{i=1}^{n}\\left(x_{i}-\\stackrel{-}{x}\\right)^{2}}{n-1} \\] ou, \\[ S^{2}=\\frac{1}{(n-1)} \\times \\left[ \\sum _{i=1}^{n} (x_{i}^{2}) - \\frac{({\\sum _{i=1}^{n}x_{i})}^{2} }{n}\\right] \\] Diz-se que a variância amostral (variância ajustada) possui \\((n-1)\\) graus de liberdade, denotado pela letra grega \\(\\nu\\). A perda de um grau de liberdade deve-se à necessidade de se substituir a média populacional desconhecida (\\(\\mu\\)) por sua estimativa amostral (\\(\\stackrel{-}{x}\\)), deduzida a partir dos dados coletados. Pode-se demonstrar que em razão dessa restrição a melhor estimativa para a variância populacional é obtida dividindo-se a soma dos quadrados dos desvios por \\((n-1)\\). Assim \\(S^{2}\\) será um estimador não tendencioso para a variância amostral ao ser dividido por \\((n-1)\\). IC.Na = function (N, n, mu, sigma) { dados=data.frame() plot(0, 0, type=&quot;n&quot;, xlim=c(sigma-0.1*sigma,sigma+0.1*sigma), ylim=c(0,N), bty=&quot;l&quot;, xlab=&quot;Desvio padrão&quot;, ylab=&quot;Amostras extraídas&quot;, main=paste0(&quot;Flutuação dos valores dos desvios padrão \\nobtidos em &quot;, N,&quot; amostras de tamanho &quot;,n), sub=paste0(&quot;A população de origem tem uma distribuição ~ N (\\u03bc:&quot;,mu,&quot; ; \\u03c3:&quot;, sigma,&quot;)&quot;)) abline(v=sigma, col=&#39;darkgreen&#39;, lwd=2, lty=2) for (i in 1:N) { x = rnorm(n, mu, sigma) media = mean(x) sd = sqrt(sum((x-mean(x))^2)/(n-1)) sd_vies = sqrt(sum((x-mean(x))^2)/(n)) temp=cbind(mu, media, sd, sd_vies) dados=rbind(dados, temp) plotx = c(sd) ploty = c(i,i) if ( sd &lt; sigma) points(sd, i, col=&quot;blue&quot;,cex=1)+text(y=i+3,x=sd, labels=round(sd,3), cex=1, col=&#39;blue&#39;) else points(sd, i, col=&quot;blue&quot;, cex=1)+text(y=i+3,x=sd, labels=round(sd,3), cex=1, col=&#39;blue&#39;) plotx = c(sd_vies) ploty = c(i,i) if ( sd_vies &lt; sigma) points(sd_vies, i, col=&quot;red&quot;,cex=1)+text(y=i+3,x=sd_vies, labels=round(sd_vies,3), cex=1, col=&#39;red&#39;) else points(sd_vies, i, col=&quot;red&quot;, cex=1)+text(y=i+3,x=sd_vies, labels=round(sd_vies,3), cex=1, col=&#39;red&#39;) } abline(v=mean(dados$sd), col=&#39;blue&#39;, lwd=2, lty=2) abline(v=mean(dados$sd_vies), col=&#39;red&#39;, lwd=2, lty=2) } IC.Na(N=100, n=15, mu=170, sigma=7) Figure 3.6: Flutuação dos valores do desvio padrão obtidos pelo estimador não viesado (em azul) e pelo estimador viesado (em vermelho) para diversas amostras extraídas de uma mesma população distribuição \\(\\sim N (\\mu; \\sigma)\\) (em verde o desvio padrão populacional, em azul a média dos desvios padrão amostrais correta e em vermelho a estimada de modo viesado) Figure 3.7: A distribuição das variâncias amostrais segue uma curva aproximada pela distribuição Qui-quadrado com (n-1) graus de liberdade Uma medida de dispersão que apresenta a mesma unidade que a das observações originais é o desvio-padrão, definido como a raiz quadrada positiva da variância. \\[ S= \\sqrt{\\frac{\\sum _{i=1}^{n}\\left(x_{i}-\\stackrel{-}{x}\\right)^{2}}{n-1}} \\] Tanto a variância quanto o desvio padrão indicam, em média, qual será o erro (desvio) cometido ao tentar substituir cada observação pela medida resumo do conjunto de dados (média). Usando os dados das medidas das alturas dos 60 estudantes teremos o seguinte valor para a variância (com unidade igual a \\(m^{2}\\)) e o desvio padrão (com unidade igual a \\(m\\)): # Variãncia var(alturas) ## [1] 0.0130809 # Desvio padrão sd(alturas) ## [1] 0.1143718 Propriedades da variância: somando-se (ou subtraindo-se) cada um dos elementos do conjunto de dados por uma constante arbitrária, a variância (e o desvio padrão) não se altera; e, multiplicando-se (ou dividindo-se) cada um dos elementos do conjunto de dados por uma constante arbitrária, a variância ficará multiplicada (ou dividida) pelo quadrado dessa constante. O desvio padrão fica multiplicado (ou dividido) por essa constante # Adicionando-se uma constante k=0.05 alturas_ad=alturas+0.05 # Variância não se altera var_ad= var(alturas_ad) var_ad ## [1] 0.0130809 # Multiplicando-se uma constante k=1.2 alturas_mult=alturas*1.2 # Variância fica multiplicada (dividida) pelo quadrado dessa constante) var(alturas_mult) ## [1] 0.0188365 all.equal(var(alturas_mult), var(alturas)*(1.2^2)) ## [1] TRUE 3.3.3.3 Coeficiente de variação. O coeficiente de variação (uma medida adimensional) é dado pela razão do desvio padrão pela média: \\[ CV(\\%)= 100\\cdot(\\frac{s}{\\stackrel{-}{x}}) \\] Classificação da variabilidade a partir da medida do Coeficiente de variação Classificação Medida do Coeficiente de variação (CV %) Baixo CV ≤ 10% Médio 10% ≤ CV ≤ 20% Alto 20% ≤ CV ≤ 30% Muito alto CV ≥ 30% 3.3.4 Medidas de subdivisão (separatrizes) Separatrizes (quantis) são medidas quantitativas que delimitam uma proporção de observações existentes em um conjunto de dados previamente ordenados com valores menores que ela. Assim, se tomamrmos como exemplo a mediana, ela é dita uma separatriz (ou quantil) de 50% pois aproximadamente 50% dos dados de um conjunto possuem valores menores que ela. Para se determinar qualquer separatriz necessitamos saber antes qual a posição que ela ocupa nos dados ordenados crescentemente: \\(x_{1}&lt;x_{2}&lt; \\dots&lt; x_{n}\\): Figure 3.8: Entendendo a indexação de dados De modo geral, um quantil de ordem \\(p\\) (ou também \\(p-quantil\\), indicado por \\(q_{p}\\)) é uma medida separatriz onde \\(p\\) é uma proporção qualquer (limitada no intervalo 0 &lt; p &lt; 1), tal que 100\\(p\\)% das observações sejam menores que seu valor \\(q_{p}\\). Desse modo, o valor \\(q_{p}\\) de uma variável aleatória \\(X\\) remete à medida da probabilidade: \\[ P(X=x | x\\leq q_{p})=p \\] Para calcular o p-ésimo quantil de uma amostra de tamanho \\(n\\), Hyndman e Fan (1996) estabeleceram: \\[j = (floor) [Np + m \\rfloor]\\], em que: \\(j\\) = índice calculado \\(N\\) = tamanho da amostra \\(p\\) = percentil desejado (0 ≤ p ≤ 1) \\(m\\) = parâmetro específico do método Tipo Parâmetro m 1 0 2 0 3 -0.5 4 0 5 0.5 6 p 7 1 - p 8 (p+1)/3 9 p/4 + 3/8 Os quantis recebem diferentes nomes em função do modo como subdividem o conjunto de dados: percentil (subdivisão em 100 partes): \\(p_{1}, \\dots, p_{99}\\) decis (subdivisão em 10 partes): \\(d_{10}, \\dots, d_{90}\\) quartis (subdivisão em 4 partes): \\(Q_{1}, \\dots, Q_{3}\\) Naturalmente que os valores de \\(p_{50}\\), \\(d_{5}\\) e \\(Q_{2}\\) são os mesmos posto tratarem-se de separatrizes que subdividem os dados na mesma propoção (50%). Os quantis mais informativos (e que por essa razão são usados para um importante gráfico que mais adiante será exposto em detalhes - Boxplot) são: \\ 1\\(^{o}\\) Quartil (\\(q_{0,25}\\)): 25% dos dados possuem valores abaixo desse valor e 75% estão acima; 2\\(^{o}\\) Quartil ou mediana (\\(q_{0,50}\\)): 50% dos dados possuem valores abaixo desse valor e 50% estão acima; e, 3\\(^{o}\\) Quartil (\\(q_{0,75}\\)): 75% dos dados possuem valores abaixo desse valor e 25% estão acima. Todavia a medicina utiliza para muitos propósitos os percentis como, por exemplo, as curvas de crescimento idade \\(versus\\) altura. De modo geral, para se calcular a posição L que um quantil qualquer de ordem p assume em um rol de dados há algumas regras empíricas: \\[ L_{p}=\\frac{p}{100} \\times (n)\\\\ L_{p}=\\frac{p}{100} \\times (n+1)\\\\ L_{p}=[\\frac{p}{100} \\times (n-1)]+1\\\\ \\] Onde: p é a ordem do quantil em % (50% no caso mediana, por exemplo); n é o número de dados do rol; e, L é a posição do valor referente ao quantil desejado. Os quartis calculados a partir das posições determinadas por essas regras aproximadamente subdividem o conjunto de dados em 25%, 50% e 75%. Assim, para a determinação dos quartis pela primeira regra o valor de p seria: para o primeiro quartil (\\(Q_{1}\\)): \\(L_{q_{0,25}}=\\frac{25}{100} \\times (n)\\); para o segundo quartil (a mediana ou \\(Q_{2}\\)): \\(L_{q_{0,50}}=\\frac{50}{100} \\times (n)\\); ou, para o terceiro quartil (\\(Q_{3}\\)): \\(L_{q_{0,75}}=\\frac{75}{100} \\times (n)\\). Novamente podemos nos deparar com duas situações possíveis para o valor calculado para a posição L qualquer que seja a regra: se valor calculado da posição L for um inteiro, nessa posição encontraremos o valor referente ao quantil desejado; se o valor calculado da posição L for fracionário, o valor desse quantil será determinado pela média entre os dois valores dos dados que estão nas posições imediatamente anterior e imediatamente posterior à posição L calculada. Juntamente com as observações mínima (\\(x_{i}\\)) e máxima (\\(x_{n}\\)), o 1\\(^{o}\\), 2\\(^{o}\\) e 3\\(^{o}\\) Quartis são importantes para se ter uma boa idéia da assimetria da distribuição dos dados. Para uma distribuição simétrica (ou aproximadamente simétrica) deveremos observar (Distribuição Gaussiana): a dispersão inferior: \\(q_{2} - x_{1} \\approx x_{n} - q_{2}\\) à dispersão superior ; \\(q_{2} - q_{1} \\approx q_{3} - q_{2}\\); e, \\(q_{1} - x_{1} \\approx x_{n} - q_{3}\\). Para nosso conjunto de dados, segundo a regra empírica apresentada teremos as seguintes posições para determinação dos valores dos quartis: para o primeiro quartil: \\[\\begin{align*} L_{Q_{1}} &amp; =\\frac{p}{100} \\times (n) \\\\ &amp; =\\frac{25}{100} \\times (60) \\\\ &amp; = 0,25*60 \\\\ &amp; = 15 \\end{align*}\\] para o segundo quartil: \\[\\begin{align*} L_{Q_{2}} &amp; =\\frac{p}{100} \\times (n) \\\\ &amp; =\\frac{50}{100} \\times (60) \\\\ &amp; = 0,5*60 \\\\ &amp; = 30 \\end{align*}\\] - para o terceiro quartil: \\[\\begin{align*} L_{Q_{3}} &amp; =\\frac{p}{100} \\times (n) \\\\ &amp; =\\frac{75}{100} \\times (60) \\\\ &amp; = 0,75*60 \\\\ &amp; = 45 \\end{align*}\\] E os quartis serão: -\\(Q_{1}\\)=1,63 -\\(Q_{2}\\)=1,67 -\\(Q_{3}\\)=1,75 Há muitos modos de se estabelecer os quantis descritos na literatura. O próprio R apresenta 9 modos diferentes: quantile(alturas, type=1) ## 0% 25% 50% 75% 100% ## 1.41 1.63 1.67 1.75 2.00 quantile(alturas, type=2) ## 0% 25% 50% 75% 100% ## 1.410 1.635 1.675 1.755 2.000 quantile(alturas, type=3) ## 0% 25% 50% 75% 100% ## 1.41 1.63 1.67 1.75 2.00 quantile(alturas, type=4) ## 0% 25% 50% 75% 100% ## 1.41 1.63 1.67 1.75 2.00 quantile(alturas, type=5) ## 0% 25% 50% 75% 100% ## 1.410 1.635 1.675 1.755 2.000 quantile(alturas, type=6) ## 0% 25% 50% 75% 100% ## 1.4100 1.6325 1.6750 1.7575 2.0000 quantile(alturas, type=7) ## 0% 25% 50% 75% 100% ## 1.4100 1.6375 1.6750 1.7525 2.0000 quantile(alturas, type=8) ## 0% 25% 50% 75% 100% ## 1.410000 1.634167 1.675000 1.755833 2.000000 quantile(alturas, type=9) ## 0% 25% 50% 75% 100% ## 1.410000 1.634375 1.675000 1.755625 2.000000 Para grandes conjuntos de dados a diferença entre os quantis determinados sob esses diferentes modos será desprezível. ## Padronização (z-scores) À conversão do valor assumido por uma variável em unidades de desvio padrão acima (ou abaixo) do valor médio de sua distribuição é dado o nome de padronização. Essa métrica permite comparações com outras, procedentes de outros fenômenos. Para padronizar (achar o seu z-score Z) o valor de uma variável procede-se segundo a fórmula: \\[ Z=\\frac{x_{i} - \\stackrel{-}{x}}{s} \\] O valor \\(Z\\) expressa quantos desvios esse dado está acima (ou abaixo) da média da distribuição. Pelo Teorema de Tchebichev pode-se estimar a probabilidade mínima dos dados situados a certa distância de \\(k\\) desvios da média dessa distribuição: \\[ P(|X-\\mu|\\ge k\\sigma) \\leq 1 - \\frac{1}{k^{2}} \\] Assim, se \\(k=2\\) ao menos 75% das observações devem estar entre a média e dois desvios padrões acima ou abaixo da média. med=round(mean(alturas),2) desv= round(sd(alturas),2) No exemplo das alturas dos estudantes temos a média de 1.69 m e um desvio padrão de 0.11 m. Assim, ao menos 75% das alturas deverão estar entre 1.47 m e 1.91 m. sort(alturas) ## [1] 1.41 1.44 1.47 1.54 1.55 1.56 1.56 1.56 1.57 1.58 1.58 1.61 1.62 1.62 1.63 ## [16] 1.64 1.64 1.65 1.65 1.65 1.65 1.66 1.66 1.66 1.66 1.66 1.67 1.67 1.67 1.67 ## [31] 1.68 1.68 1.68 1.69 1.71 1.71 1.72 1.72 1.73 1.73 1.73 1.73 1.73 1.74 1.75 ## [46] 1.76 1.76 1.77 1.78 1.78 1.79 1.82 1.83 1.83 1.84 1.85 1.86 1.93 1.95 2.00 # Duas observações menores que 1,47m e trẽs maiores que 1,91m. # Assim, 54 observações dentro do intervalo, equivalendo a 91,66% do total. "],["medidas-de-forma-assimetria-curtose.html", "3.4 Medidas de forma (assimetria &amp; curtose)", " 3.4 Medidas de forma (assimetria &amp; curtose) Quando analisamos o histograma (a representação gráfica da distribuição das frequências dos valores agrupados em classes) de uma determinada variável, não é muito comum que ele se mostre simétrico tal como seria se os dados fossem distribuídos de modo exatamente Normal. Ao observarmos que a cauda se mostra mais alongada para a direita (indicativo da existência de uma quantidade maior de dados com grandes valores, arrastando a média para a direita: moda \\(&lt;\\) mediana \\(&lt;\\) média) diz-se que a distribuição é assimétrica à direita. Na situação oposta (moda \\(&gt;\\) mediana \\(&gt;\\) média) diz-se que ela é assimétrica à esquerda. a=rbeta(10000,5,2) c=rbeta(10000,5,5) b=rbeta(10000,2,5) par(mfrow=c(1,3)) hist(a, xlab=&quot;Valores&quot;,col = &#39;lightblue&#39;, ylab=&quot;Frequência&quot;, main=&quot;Assimetria à esq.&quot;) hist(c, xlab=&quot;Valores&quot;,col = &#39;lightblue&#39;, ylab=&quot;Frequência&quot;, main=&quot;Relativa simetria&quot;) hist(b, xlab=&quot;Valores&quot;,col = &#39;lightblue&#39;, ylab=&quot;Frequência&quot;, main=&quot;Assimetria à dir.&quot;) Figure 3.9: Diferentes formas na distribuição dos dados De modo assemelhado, o histograma pode denotar uma forma mais plana ou menos aguda, onde um cume mostra-se mais destacado. Nesse aspecto da forma, uma variável com distribuição Gaussiana apresentaria uma curva a que denominamos mesocúrtica. Distribuições com um aspecto mais plano são denominadas de platicúrticas e as com um cume agudo são denominadas leptocúrticas. A curtose é uma medida da agudeza da distribuição dos dados em relação à distribuição Gaussiana. Figure 3.10: Diferentes aspectos de uma distribuição quanto à sua inclinação Essas possíveis variações na forma de uma distribuição podem ser numericamente quantificadas através dos coeficientes de assimetria e curtose. Uma das medidas do coeficiente de assimetria é através do primeiro ou segundo coeficientes de Pearson, dados pelas seguintes relações: Primeiro coeficiente de assimetria de Pearson: \\(AS= \\frac{ \\stackrel{-}{x} - M_{o} }{ s }\\) Segundo coeficiente de assimetria de Pearson: \\(AS = \\frac{ 3 ( \\stackrel{-}{x} - Q_{2}) } { s }\\) Onde: \\(\\stackrel{-}{x}\\) é a média; \\(M_{o}\\) é a moda; \\(S\\) é o desvio padrão; e, \\(Q_{2}\\) é o segundo quartil (mediana). A assimetria é classificada do modo seguinte: \\(-1 \\leq AS \\leq 1%=\\) : distribuição simétrica; \\(AS&lt;-1\\): distribuição com assimetria negativa; e, \\(AS&gt;1\\): distribuição com assimetria positiva. Uma das medidas do coeficiente de curtose é através da seguinte relação entre quartis e percentis: \\[ K = \\frac{Q_{3} - Q_{1}} {2 \\times(P_{90} - P_{10})} \\] Onde: \\(Q_{3}\\) = \\(3^{o}\\) quartil; \\(Q_{1}\\) = \\(1^{o}\\) quartil; \\(P_{90}\\) = \\(90^{o}\\) percentil; e, \\(P_{10}\\) = \\(10^{o}\\) percentil. O coeficiente de curtose é classificado do modo seguinte: k = 0,263: distribuição mesocúrtica; k &lt; 0,263: distribuição leptocúrtica; e, k &gt; 0,263: distribuição platicúrtica. "],["diferentes-posições-da-média-moda-e-mediana-2o-quartil.html", "3.5 Diferentes posições da média, moda e mediana (2\\(^{o}\\) quartil)", " 3.5 Diferentes posições da média, moda e mediana (2\\(^{o}\\) quartil) Essas três medidas podem se apresentar com valores em posições alternadas quando as comparamos: quando a moda=mediana=média temos uma distribuição de frequências razoavelmente simétrica; quando a moda \\(\\leq\\) mediana \\(\\leq\\) média (há uma quantidade maior de dados com grandes valores, arrastando a média para a direita, para cima) temos uma distribuição de frequências positivamente assimétrica, ; e, quando a moda \\(\\geq\\) mediana \\(\\geq\\) média (há uma quantidade maior de dados com pequenos valores, arrastando a média para a esquerda, para baixo) temos uma distribuição de frequências negativamente assimétrica. barplot(tab_alturas, main=&quot;Valores observados da alturas dos estudantes&quot;, xlab=&quot;Altura (cm)&quot;, ylab=&quot;Quantidade observada (un)&quot;, ylim=c(0,6), col=&quot;blue&quot;, las=0, hor=&quot;FALSE&quot;) abline(v=mean(19.9, 21.1), col=&quot;red&quot;) text( mean(19.9, 21.1)-0.5, 5, &quot;Média=1,69 m&quot;, col = &quot;red&quot;, srt=90) abline(v=median(18.7 , 19.9), col=&quot;darkgreen&quot;) text(median(18.7 , 19.9)-0.5, 5, &quot;Mediana=1,675 m&quot;, col = &quot;darkgreen&quot;, srt=90) abline(v=c(16.3, 23.5), col=&quot;darkgrey&quot;) text(c(16.3-0.5, 23.5-0.5), 5, c(&quot;Moda=1,66&quot;,&quot;Moda=1,73&quot;), col = &quot;darkgray&quot;, srt=90) Figure 3.11: Valores observados das alturas dos estudantes e as posições da média, moda e mediana Figure 3.12: Quadro comparativo entre as medidas de tendência central (posição) "],["apresentação-tabular-de-dados.html", "3.6 Apresentação tabular de dados", " 3.6 Apresentação tabular de dados As sínteses numéricas expostas condensam ao máximo a informação trazida pelos dados na forma de estatísticas associadas à: posição: média, moda, mediana; dispersão: amplitude total dos dados, variância (esvio padrão), coeficiente de variação; separatrizes (repartição): como por exemplo os quartis (\\(Q_{1}\\); \\(Q_{2}\\)/mediana e \\(Q_{3}\\)). A correta exposição dos dados na forma de tabelas e gráficos auxilia o entendimento de muitas outras características relacionadas aos dados trabalhados por parte do leitor com grande riqueza visual. Ao se lidar com grandes conjuntos de dados a visualização da informação contida nos dados fica comprometida se eles forem simplesmente apresentados como uma listagem, mesmo que depurados de eventuais inconsistências e ordenados como a lista abaixo: sort(alturas) ## [1] 1.41 1.44 1.47 1.54 1.55 1.56 1.56 1.56 1.57 1.58 1.58 1.61 1.62 1.62 1.63 ## [16] 1.64 1.64 1.65 1.65 1.65 1.65 1.66 1.66 1.66 1.66 1.66 1.67 1.67 1.67 1.67 ## [31] 1.68 1.68 1.68 1.69 1.71 1.71 1.72 1.72 1.73 1.73 1.73 1.73 1.73 1.74 1.75 ## [46] 1.76 1.76 1.77 1.78 1.78 1.79 1.82 1.83 1.83 1.84 1.85 1.86 1.93 1.95 2.00 Um dos modos de se lidar com isso é condensando a informação dos dados brutos em tabelas. Uma tabela é uma forma não discursiva de apresentar informações nas quais o dado numérico se destaca como informação central. Uma tabela se diferencia de um quadro por este ter todos os seus campos delimitados por linhas e conter apenas informações de natureza qualitativa. Uma tabela deve conter algumas informações essenciais, fora daquela estritamente relacionada aos dados, para que a compreensão do leitor acerca dos dados expostos seja a mais imediata possível: título que explique o que a tabela contém, local, data; cabeçalho nas colunas e linhas com a explicação, ainda que resumis, a que se referem as quantidades expostas no corpo; corpo formado pelos dados referentes às variáveis; fonte dos dados; uniformidade no número de casas decimais apresentadas no corpo; todas as casas devem apresentar valores ou símbolos que expliquem a ausência da informação (NI, NE, ou 0-zero). Trabalhos de natureza acadêmica ou científica deveriam obrigatoriamente seguir, quando publicados no Brasil, a norma vigente publicada pela ABNT: Associação Brasileira de Normas Técnicas e algumas punlicações do IBGE: Instituto Brasileiro de Geografia e Estatística (como em link). Observa-se frequentemente, todavia, que as publicações seguem normas particulares das instituições de ensino (para trabalhos de conclusão de curso, monografias, dissertações e teses) ou das editoras (artigos), muitas vezes mescladas com recomendações da ABNT. Na Universidade Estadual de Londrina o portal da biblioteca possui uma ligação para a seção “Normas para trabalhos” (link). 3.6.1 Apresentação tabular de dados qualitativos 3.6.1.1 Dados qualitativos em entrada única Para alguns tipos de dados, a apresentação tabular é bastante imediata. Admita que tenha sido realizada uma pesquisa junto a um terminal de desembarque internacional em algum aeroporto sobre o continente de procedência do passageiro, num determinado período de um certo dia, tendo sido anotados os seguintes valores: AM, AM, A, A, A, AM, EU, EU, EU, EU, AM, AS, AS, AS, OC, AS, EU, AM, onde os continente anotados são assim identificados: americano (AM); africano (A), europeu (EU); asiático (AS) e da oceania (OC). Uma tabela para a apresentação dos resultados poderia ser: Desembarques no terminal internacional A em Cumbica (SP, Brasil) (10/10/2021: 8 h 00min às 12 h 00 min) Continente de procedência Desembarques América 5 África 3 Europa 5 Ásia 4 Oceania 1 Total 18 Fonte: Próprio autor Outro exemplo de apresentação tabular onde são apresentadas as proporções relativas observadas de cada nível da variável estudada (“tipo de família”, com quatro níveis diferentes), de um levantamento amostral feito pela Agência do Censo dos Estados Unidos em 2005. Estrutura domiciliar dos Estados Unidos Estrutura domiciliar Número (milhões) Freq. rel. Freq. rel. (%) Casal com filhos 24,1 0,22 22 Casal sem filhos 31,1 0,28 28 Solteiro, sem parceiro 19,1 0,17 17 Morando sozinho 30,1 0,27 27 Outros domicílios 6,7 0,06 6 Total 111.1 1,00 100% Fonte: Censo dos EUA (2005) 3.6.1.2 Dados qualitativos em entrada dupla Outros tipos de dados são provenientes de pesquisas que têm por base respostas de natureza binária como, por exemplo: sim ou não; gosto ou não gosto; voto em “A” ou voto em “B”; ou, concordo ou não concordo. Como resultado final, são obtidas contagens que expressam as frequências absolutas observadas para cada uma das variáveis (ou seus níveis) como na apresentação tabular de dados qualitativos por Tabelas de Contingência. As tabelas de contingência são usadas para associar duas ou mais variáveis qualitativas (ou seus níveis) às contagens das respostas obtidas, na forma das frequências absoluta e relativa observadas em cada uma dessas variáveis (ou seus níveis). O uso desse tipo de tabela é comum quando se pretende investigar se as variáveis estudadas têm alguma associação por meio de testes não paramétricos. Esse tipo de apresentação facilita a extração de informações relacionadas às probabilidades marginais ou condicionadas de cada uma variáveis ou seus níveis. Admita agora que a pesquisa anterior junto ao terminal de desembarque internacional tenha também apontado o sexo do passageiro em seu desembarque. Uma tabela de dupla entrada com aqueles dados assumiria a forma: Desembarques no terminal internacional A em Cumbica (SP, Brasil) (10/10/2021: 8 h 00min às 12 h 00 min) Desembarques no terminal internacional A em Cumbica (SP, Brasil) Sexo do passageiro Total M F América 3 2 5 África 3 0 3 Europa 1 4 5 Ásia 2 2 4 Oceania 0 1 1 Total 9 9 18 Fonte: Próprio autor Um outro exemplo, usando dados da incidência de baixo peso ao nascer em recém-nascidos de Pelotas (RS) segundo o hábito tabágico da mãe durante a gravidez (1982): Incidência de baixo peso ao nascer em recém-nascidos de Pelotas, RS, segundo o hábito tabágico da mãe durante a gravidez (1982) Classificação da mãe Baixo peso ao nascer Total Sim Não Fumante 275 2.144 2.419 Não fumante 311 4.496 4.807 Total 586 6.640 7.226 Fonte: Próprio autor Ou ainda neste outro estudo que analisa a inclinação partidária de dois tipos de núcleos familiares em relação à presença de filhos: Inclinação partidária (frequências absolutas) Estrutura domiciliar Democrata Republicano Totais Casal com filho(s) 762 468 1230 Casal sem filhos 484 477 961 Totais 1246 945 2191 Fonte: Próprio autor A partir das contagens obtidas na pesquisa (as frequências absolutas), uma tabela com as frequências relativas pode ser construída, passando a apresentar as proporções relativas de cada categoria em relação aos níveis pesquisados: Inclinação partidária (frequências relativas) Estrutura domiciliar Democrata (%) Republicano (%) Totais (%) Casal com filho(s) 34,78 21,36 56,14 Casal sem filhos 22,09 21,77 43,86 Totais (%) 56,87 43,13 100 Fonte: Próprio autor 3.6.2 Apresentação tabular de dados quantitativos Todavia, para grandes quatidades de observações de dados quantitativos, a apresentação na forma de tabelas deve ser precedida do agrupamento dos valores observados em classes. O procedimento estatístico de agrupar os dados em classes ou categorias envolve construir uma tabela de distribuição de frequências. Uma tabela de distribuição de frequências associa cada classe (intervalo) de valores da variável estudada ao número de ocorrências observadas. Como regra prática, a repartição dos dados brutos em classes deve sempre observar para que não haja um número excessivo de classes (diminuição da finalidade de resumir os dados, criação de classes sem nenhuma observação) nem tampouco poucas (que não possibilitem a visualização da distribuição e promovam perda da informação original). A construção de uma distribuição de frequências consiste essencialmente em: escolher as classes ou intervalos (dados quantitativos) ou categorias (dados qualitativos); separar ou enquadrar os dados nessas classes ou categorias; e, contar o número de dados de cada classe ou categoria. A literatura propõe vários modos para se determinar o número k de classes: Crítério Tamanho da amostra (n) Fórmula n \\(\\leq\\) 25 k=5 Raiz quadrada 25 \\(\\leq\\) n \\(\\leq\\) 220 k=\\(\\sqrt{n}\\) Raiz quadrada 25 \\(\\leq\\) n \\(\\leq\\) 220 \\(2^{k} &gt; n\\) Herbert Sturges 135 \\(\\leq\\) 572237 k=1+3,22.log(n)\\(^{(1)}\\) Giuseppe Milone 20 \\(\\leq 36315\\) k=-1+2.ln(n) \\(^{(2)}\\) \\(^{(1)}\\): logarítmo na base 10; e \\(^{(2)}\\): logarítmo na base e. Ao se escolher um número (\\(k\\)) de classes deve-se ponderar para que: os intervalos das classes tenham, geralmente, a mesma amplitude (raramente se necessita dispor de classes com amplitudes diferentes); os intervalos, a faixa de variação que vai do limite inferior da primeira classe ao limite superior da **última classe*, devem conter todos os valores possíveis da variável; cada valor observado deve pertencer apenas a uma classe; nenhuma classe deverá estar vazia (sem observação alguma); não adotar um número muito elevado de classes (de modo que cada classe possua poucas observações) e nem muito reduzido (de modo a esconder a variabilidade dos dados ao se reunir todas as observações em poucas faixas de valores); alguns autores recomendam um número mínimo de 5 classes e um máximo de 15; podemos considerar a amplitude de cada classe com uma casa decimal a mais que os dados de modo a facilitar a incorporação do último valor (mais elevado) na última classe. Em nosso exemplo das alturas dos estudantes, a determinação do número de classes pelo critério da raiz quadrada ( n=60) sugere 8 classes (outros critérios: pelo menor inteiro tq. \\(2^{k}&gt;n; k=6\\), pelo critério de Sturges \\(k=6,86 \\sim 7\\), de Giuseppe Milone \\(k=8,18 \\sim 9)\\)). \\[\\begin{align*} k &amp; =\\sqrt{n} \\\\ &amp; = 7,74 \\\\ \\end{align*}\\] Arredondar para mais: \\(k=8\\). A amplitude total (A) dos valores observados é a simples diferença entre o valor máximo (2,00 m) e o valor mínimo (1,41 m): \\[\\begin{align*} A &amp; =2,00-1,41 \\\\ &amp; =0,59 m \\end{align*}\\] A amplitude de cada uma das classes ( \\(\\Delta\\)) será dada pelo quociente da amplitude total ( A) pelo número de classes ( k). \\[\\begin{align*} \\Delta &amp; = \\frac{A}{k} \\\\ &amp; = \\frac{0,59}{8}\\\\ &amp; = 0,07375 m \\end{align*}\\] Arredondar para mais: \\(\\Delta=0,075 m\\). As classes são então assim construídas: Limite inferior da \\(1^{a}\\) classe (\\(L_{inf_1}\\)): valor mínimo observado; e, Limite superior da \\(1^{a}\\) classe (\\(L_{sup_1}\\)): \\(L_{inf_1}\\) + \\(\\Delta\\). e assim sucessivamente atá a última classe. Símbolos gráficos para intervalos: Os símbolos abaixo indicam que o valor situado à sua esquerda está incluído no intervalo e o da direita não está:   \\[ \\vdash \\\\ {\\bullet}-{\\circ} \\] Os símbolos abaixo indicam que o valor situado à sua esquerda não está incluído no intervalo e o da direita **está incluído*: \\[ \\dashv \\\\ {\\circ}-{\\bullet} \\] As tabelas que serão apresentadas a seguir estão sem os requisitos essenciais expostos anteriormente uma vez que o propósito é explicar a construção e cálculo dos valores de suas células. Com \\(\\Delta=0,075m\\) as 8 classes ficam assim estabelecidas, tendo-se como ponto de partida o valor mínimo observado: 1,41 m - 1,485 m; 1,485 m - 1,56 m; 1,56 m - 1,635 m; 1,635 m - 1,71 m; 1,71 m - 1,785 m; 1,785 m - 1,86 m; 1,86 m - 1,935m; 1,935 - 2,01 m. Ordenando-se os dados (para facilitar a identificação das observações) e atribuíndo cada elemento a uma única classe: { 1,41 ; 1,44 ; 1,47 ; 1,54 ; 1,55 ; 1,56 ; 1,56 ; 1,56; 1,57 ; 1,58 ; 1,58 ; 1,61 ; 1,62 ; 1,62 ; 1,63; 1,64 ;1,64 ; 1,65 ; 1,65 ; 1,65 ; 1,65 ; 1,66 ; 1,66 ; 1,66 ; 1,66 ; 1,66 ; 1,67 ; 1,67 ; 1,67 ; 1,67 ; 1,68 ; 1,68 ;1,68 ; 1,69 ; 1,71 ; 1,71 ; 1,72 ; 1,72 ; 1,73 ; 1,73 ; 1,73 ; 1,73 ; 1,73 ; 1,74 ; 1,75 ; 1,76 ; 1,76 ; 1,77 ; 1,78 ; 1,78; 1,79 1,82 ; 1,83 ; 1,83 ; 1,84 ; 1,85 ; 1,86 ; 1,93; 1,95 ; 2,00} A tabela de distribuição de frequências com 8 classes, cada uma com amplitude 0,075 m, assume a forma: Classe Frequência absoluta (\\(f_{i}\\)) 1,41 m \\(\\vdash\\) 1,485 m 3 1,485 m \\(\\vdash\\) 1,56 m 2 1,56 m \\(\\vdash\\) 1,635 m 10 1,635 m \\(\\vdash\\) 1,71 m 19 1,71 m \\(\\vdash\\) 1,785 m 16 1,785 m \\(\\vdash\\) 1,86 m 6 1,86 m \\(\\vdash\\) 1,935 m 2 1,935m \\(\\vdash\\) 2,01 m 2 Total 60 Alternativamente, caso adotássemos como ponto de partida (um pouco abaixo do valor mínimo observado) o valor de 1,40 m e com amplitude de classe 0,08 m, uma tabela alternativa de distribuição de frequẽncias teria como classes : 1,40 m - 1,48 m; 1,48 m - 1,56 m; 1,56 m - 1,64 m; 1,64 m - 1,72 m; 1,72 m - 1,80 m; 1,80 m - 1,88 m; 1,88 m - 2,06 m. Ordenando-se os dados (para facilitar a identificação das observações) e atribuíndo cada elemento a uma única classe: { 1,41 ; 1,44 ; 1,47 ; 1,54 ; 1,55 ; 1,56 ; 1,56 ; 1,56 ; 1,57 ; 1,58 ; 1,58 ; 1,61 ; 1,62 ; 1,62 ; 1,63 ; 1,64 ;1,64 ; 1,65 ; 1,65 ; 1,65 ; 1,65 ; 1,66 ; 1,66 ; 1,66 ; 1,66 ; 1,66 ; 1,67 ; 1,67 ; 1,67 ; 1,67 ; 1,68 ; 1,68 ;1,68 ; 1,69 ; 1,71 ; 1,71 ; 1,72 ; 1,72 ; 1,73 ; 1,73 ; 1,73 ; 1,73 ; 1,73 ; 1,74 ; 1,75 ; 1,76 ; 1,76 ; 1,77 ; 1,78 ; 1,78 ;1,79; 1,82 ; 1,83 ; 1,83 ; 1,84 ; 1,85 ; 1,86 ; 1,93 ; 1,95 ; 2,00; } A tabela de distribuição de frequências com 7 classes, cada uma com amplitude 0,08 m, assume a forma: Classe Frequência absoluta (\\(f_{i}\\)) 1,40 m \\(\\vdash\\) 1,48 m 3 1,48 m \\(\\vdash\\) 1,56 m 2 1,56 m \\(\\vdash\\) 1,64 m 10 1,64 m \\(\\vdash\\) 1,72 m 21 1,72 m \\(\\vdash\\) 1,80 m 15 1,80 m \\(\\vdash\\) 1,88 m 6 1,88 m \\(\\vdash\\) 2,06 m 3 Total 60 Também podemos cogitar adotar alternativamente um intervalo de classe \\(\\Delta=0,10\\) m, com a primeira classe começando (um pouco abaixo do valor mínimo observado) na altura de 1,40 m; todavia, a última classe não iria contemplar o valor máximo observado (2,00 m) e necessitaíamos abrir mais uma classe apenas para incluí-lo. Mas começando-se no valor mínimo obseravado (1,41 m) estaríamos assegurando que o limite superior da última classe incluiria o valor máximo observado (2,00 m). Assim, essas seriam as classes sob uma amplitude de 0,10 m: 1,41 m - 1,51 m; 1,51 m - 1,61 m; 1,61 m - 1,71 m; 1,71 m - 1,81 m; 1,81 m - 1,91 m; 1,91 m - 2,01 m. O total de 6 classes (1,41 m a 2,01 m) cobre toda faixa de variação dos valores dos dados (de 1,41 m a 2,00 m ) e é de rápida assimilação pelo leitor. Ordenando-se os dados (para facilitar a identificação das observações) e atribuíndo cada elemento a uma única classe: {1,41 ; 1,44 ; 1,47 ; 1,54 ; 1,55 ; 1,56 ; 1,56 ; 1,56 ; 1,57 ; 1,58 ; 1,58 ; 1,61 ; 1,62 ; 1,62 ; 1,63 ; 1,64 ;1,64 ; 1,65 ; 1,65 ; 1,65 ; 1,65 ; 1,66 ; 1,66 ; 1,66 ; 1,66 ; 1,66 ; 1,67 ; 1,67 ; 1,67 ; 1,67 ; 1,68 ; 1,68 ;1,68 ; 1,69 ; 1,71 ; 1,71 ; 1,72 ; 1,72 ; 1,73 ; 1,73 ; 1,73 ; 1,73 ; 1,73 ; 1,74 ; 1,75 ; 1,76 ; 1,76 ; 1,77 ; 1,78 ; 1,78 ; 1,79 ; 1,82 ; 1,83 ; 1,83 ; 1,84 ; 1,85 ; 1,86 ; 1,93 ; 1,95 ; 2,00} A tabela de distribuição de frequências com 6 classes, cada uma com amplitude 0,10 m, assume a forma: Classe Frequência absoluta (\\(f_{i}\\)) 1,41 m \\(\\vdash\\) 1,51 m 3 1,51 m \\(\\vdash\\) 1,61 m 8 1,61 m \\(\\vdash\\) 1,71 m 23 1,71 m \\(\\vdash\\) 1,81 m 17 1,81 m \\(\\vdash\\) 1,91 m 6 1,91 m \\(\\vdash\\) 2,01 m 3 Total 60 Tabelas de distribuição de frequências mais completas podem montadas agregando muitas informações adicionais em novas colunas, mediante simples operações aritméticas. Essas informações servem para tornar a visualização mais imediata e muitas delas são obtidas com operações matemáticas elementares: Classe i: é a simples identificação de cada classe; Amplitude (\\(\\Delta_{i}\\)) da classe \\(i\\): a diferença entre o valor do limite superior e o do inferior de cada classe; Intervalo de valores da classe \\(i\\) (onde seu limite inferior está contido e o limite superior não está contido); Valor médio (\\(\\stackrel{-}{x}_{i}\\)) de cada classe \\(i\\): o valor de seu limite inferior mais a metade da amplitude da classe; Frequência absoluta (\\(f_{i}\\)) da classe \\(i\\): o número de observações contidas no intervalo da classe considerada; Frequência relativa (\\(fr_{i}= \\frac{f_{i}}{n}\\)) da classe \\(i\\) (ou frequência relativa percentual, se assim apresentada): o quociente do número de observações \\(n_{i}\\) contidas no intervalo da classe \\(f_{i}\\), pelo número total de observações (\\(n\\)); Frequência acumulada (\\(F_{i}\\)) da classe \\(i\\) (ou frequência acumulada percentual, se assim apresentada): o número de observações com medidas contidas na classe \\(i\\) e nas anteriores a ela; Densidade absoluta (\\(\\delta_{i}=\\frac{f_{i}}{\\Delta_{i}}\\)): o quociente do número de observações da classe (\\(f_{i}\\)) pela sua amplitude (\\(\\Delta_{i}\\)); Densidade relativa \\(\\delta_{fr_{i}}=\\frac{fr_{i}}{\\Delta_{i}}\\): o quociente da frequência relativa (\\(fr_{i}\\)) pela amplitude (\\(\\Delta_{i}\\)) da classe. Vejo como exemplo as tabelas abaixo: Classe Int. de valores Alt. média Freq. abs. Freq. rel. Freq. rel. (%) Freq. acumulada Freq. acum. (%) (\\(\\stackrel{-}{x}_{i}\\)) (\\(f_{i}\\)) (\\(fr_{i}\\)) (\\(fr_{i\\%}\\)) (\\(F_{i}\\)) (\\(F_{i\\%}\\)) 1 1,41 \\(\\vdash\\) 1,51 1,46 3 0,05 5 3 5,00 2 1,51 \\(\\vdash\\) 1,61 1,56 8 0,13 13,33 11 18,33 3 1,61 \\(\\vdash\\) 1,71 1,66 23 0,38 38,33 34 56,66 4 1,71 \\(\\vdash\\) 1,81 1,76 17 0,28 28,34 51 85,00 5 1,81 \\(\\vdash\\) 1,91 1,86 6 0,10 10 57 95,00 6 1,91 \\(\\vdash\\) 2,01 1,96 3 0,05 5 60 100,00 Totais - 60 1,00 100,00 - - Classe Int. de valores Freq. abs. Amplitude Dens. abs Freq. rel. Dens. rel. (\\(f_{i}\\)) (\\(\\Delta_{i}\\)) (\\(\\delta_{i}\\)) (\\(fr_{i}\\)) (\\(\\delta_{fr_{i}}\\)) 1 1,41 \\(\\vdash\\) 1,51 3 0,10 30 0,05 0,5 2 1,51 \\(\\vdash\\) 1,61 8 0,10 80 0,13 1,33 3 1,61 \\(\\vdash\\) 1,71 23 0,10 230 0,39 3,83 4 1,71 \\(\\vdash\\) 1,81 17 0,10 170 0,28 2,83 5 1,81 \\(\\vdash\\) 1,91 6 0,10 60 0,10 1 6 1,91 \\(\\vdash\\) 2,01 3 0,10 30 0,05 0,5 Totais - 60 - - 1,00 - Determinadas as densidades relativas da cada classe, uma poligonal pode ser traçada e as áreas formadas por ela e a base desse gráfico somam aproximadamente 1. Figure 3.13: Polígono de densidades relativas como uma densidade empírica de probabilidade 3.6.3 Média Nas tabelas de distribuições de frequências os resultados estão agrupados em intervalos de classes (\\(i\\)). Por essa razão, os dados perdem sua identidade individual e passam a se representados pelo valor médio de cada intervalo (\\(\\stackrel{-}{x}_{i}\\)). A média será então dada pelo produto deste valor médio de cada intervalo (\\(\\stackrel{-}{x}_{i}\\)) pela frequência absoluta que ele apresentou (\\({n}_{i}\\)), dividido pela quantidade de dados (\\(n\\)). Sejam \\(n_{1}, n_{2}, ..., n_{n}\\) as frequências apresentadas para cada intervalo \\(i\\) dos valores assumidos pela variável \\(X\\) para o total \\(n\\) de observações. Assim a média aritmética simples para dados agrupados será dada por: \\[ \\stackrel{-}{x}=\\frac{\\sum _{i=1}^{k}{f}_{i}\\cdot{\\stackrel{-}{x}}_{i}}{n} \\] onde: \\(\\stackrel{-}{x}_{i}\\): o valor médio do intervalo da classe \\(i\\); \\(f_{i}\\): a frequência absoluta da classe \\(i\\); \\(k\\) é o número de classes da tabela de distribuição de frequências; \\(n\\) é o número de dados da tabela (eventualmente, os dados podem se referir a toda a população sob estudo) . 3.6.4 Moda Moda para dados apresentados na forma de uma distribuição de frequências: \\[ Mo = l_{inf} + (\\frac{\\Delta_{1}}{\\Delta_{1} + \\Delta_{2}}) \\times \\Delta_{i} \\] Primeiramente identificamos a(s) classe(s) modal(is): a(s) classe(s) com maior(es) frequência(s) absoluta \\(f_{i}\\). Os demais elementos da expressão são: \\(l_{inf}\\): limite inferior da classe modal; \\(\\Delta_{1}\\) frequência absoluta da classe modal menos a frequência absoluta da classe anterior à classe modal; \\(\\Delta_{2}\\) frequência absoluta da classe modal menos a frequência absoluta da classe posterior à classe modal; e, \\(\\Delta_{i}\\) é o intervalo da classe modal. 3.6.5 Mediana (\\(=Q_{2}=D_{5}=P_{50}\\)) Mediana para dados apresentados na forma de uma distribuição de frequências: \\[ Md = l_{inf} + \\left[ \\frac{\\frac{\\sum_{i}^{k} f_{i}}{2} - F_{(md-1)}}{f_{md}} \\right] \\times \\Delta_{i} \\] Primeiramente identificamos a classe mediana: a classe que contem o elemento de posição \\(\\frac{n}{2}\\) (basta observar a coluna da frequência absoluta acumulada: \\(F_{i}\\), percorrendo-a até a classe \\(i\\) que tenha valor \\(&gt;\\frac{n}{2}\\). Os demais elementos da expressão da mediana são: \\(l_{inf}\\): limite inferior da classe mediana; \\(F_{{(md-1)}}\\): é a frequência absoluta acumulada até a classe anterior à classe mediana; \\(f_{md}\\): é a frequência absoluta da classe mediana; e, \\(\\Delta_{i}\\): é o intervalo da classe mediana. 3.6.6 Variância Variância para dados agrupados: \\[ S^{2}= \\frac{1}{n-1} \\times \\left[ \\sum _{i=1}^{k}{(\\stackrel{-}{x}}_{i})^{2} \\cdot {f}_{i} - \\frac{{\\left(\\sum _{i=1}^{k}{\\stackrel{-}{x}}_{i} \\cdot {f}_{i}\\right)}^{2} }{n}\\right] \\] em que: \\(\\stackrel{-}{x}_{i}\\): o valor médio do intervalo de cada classe \\(i\\) (\\(lim_{sup_i}-lim_{inf_i}\\)); \\(f_{i}\\): a frequência absoluta de cada classe \\(i\\); \\(k\\) é o número de classes da tabela de distribuição de frequências; \\(n\\) é o número de dados da tabela (eventualmente, os dados podem se referir a toda a população sob estudo) . 3.6.7 Quartis Quartis para dados agrupados: \\[ Q_{i}= l_{inf_{Q_{i}}} + \\Delta_{i} \\frac{L_{Q_{i}} - f_{ac_{Q_{i-1}}}}{f_{Q_{i}}} \\] em que: \\(n\\) é o número de dados; \\(Q_{i}\\) é o quartil desejado: \\(i=1, 2, 3\\); \\(L_{Q_{i}}\\) é posição do quartil desejado tal que: \\(L_{Q_{1}}=0.25n\\) \\(L_{Q_{2}}=0.5n\\) \\(L_{Q_{3}}=0.75n\\) classe quartílica é a classe onde a posição do quartil desejado ( \\(L_{Q_{i}}\\)) se localiza; \\(l_{inf_{Q_{i}}}\\) é o limite inferior da classe quartílica; \\(f_{{Q_{i-1}}}\\) é a frequência acumulada da classe imediatamente anterior à classe quartílica; \\(f_{Q_{i}}\\) é a frequência absoluta de classe quartílica; \\(\\Delta_{i}\\) é a amplitude da classe quartílica (frequentemente igual para todas). "],["apresentação-gráfica-de-dados.html", "3.7 Apresentação gráfica de dados", " 3.7 Apresentação gráfica de dados Uma apresentação na forma gráfica torna ainda mais fácil a visualização das informações contidas nos dados. Há uma gama enorme de gráficos para a representação de dados a depender de sua natureza (qualitativa ou quantitativa). 3.7.1 Gráficos para uma variável qualitativa ranking: barras; parte em relação ao todo: setores; 3.7.1.1 Colunas A partir das tabelas mostradas na seção 3.5.1.1 Dados qualitativos em entrada única, podemos elaborar a apresentação gráfica na forma de Gráficos de colunas: desembarque=c(&#39;AM&#39;,&#39;AM&#39;,&#39;A&#39;,&#39;A&#39;,&#39;A&#39;,&#39;AM&#39;,&#39;EU&#39;,&#39;EU&#39;,&#39;EU&#39;,&#39;EU&#39;,&#39;AM&#39;,&#39;AS&#39;,&#39;AS&#39;,&#39;AS&#39;,&#39;OC&#39;,&#39;AS&#39;,&#39;EU&#39;,&#39;AM&#39;) tab_desembarque=table(desembarque) barplot(tab_desembarque, main=&quot;Desembarques no terminal internacional A em Cumbica \\n(10/10/2021: 8 h 00min às 12 h 00 min)&quot;, sub= &quot;Continente de procedência: América: AM; África: A; Europa: EU; Ásia: AS; Oceania: OC \\nfonte: próprio autor&quot;, xlab=&quot;&quot;, ylab=&quot;Quantidade observada (un)&quot;, ylim=c(0,6), col=&quot;blue&quot;, las=0, hor=&quot;FALSE&quot;) Figure 3.14: Gráfico de barras dos dados observados no terminal de desembarque internacional do aeroporto library(ggplot2) dados=data.frame(tipo=c(&quot;Casal com filhos&quot;, &quot;Casal sem filhos&quot;, &quot;Solteiro, s/parceiro&quot;, &quot;Morando sozinho&quot;, &quot;Outros domicíclios&quot;), quant=c(24.1, 31.1, 19.1, 30.1, 6.7)) ggplot(dados, aes(x=tipo, y=quant, color=tipo)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge())+ ggtitle(&quot;Estrutura domiciliar dos Estados Unidos, 2005&quot;) + theme(legend.position=&quot;bottom&quot;)+ geom_text(aes(label=quant), vjust=1.6, color=&quot;white&quot;, position = position_dodge(0.9), size=3.5)+ scale_fill_brewer(palette=&quot;Paired&quot;)+ theme_minimal()+ xlab(&quot;&quot;) + ylab(&quot;Frequência absoluta observada (milhões)&quot;)+ labs(colour = &quot;Tipos de domicílios&quot;) Figure 3.15: Gráfico de barras da estrutura domiciliar dos Estados Unidos 3.7.1.2 Setores Em um Gráfico de setores a representação das quantidades está associada a uma fração do comprimento de um círculo. Para sua confecção considera-se a proporção da quantidade observada específica da quantidade total de dados, expressa na forma de fração do ângulo de um setor circular em relação ao ângulo interno total de um círculo (360o). library(scales) ## ## Attaching package: &#39;scales&#39; ## The following objects are masked from &#39;package:formattable&#39;: ## ## comma, percent, scientific library(ggplot2) desembarques_classes=data.frame( group = c(&quot;América&quot;,&quot;África&quot;,&quot;Europa&quot;,&quot;Ásia&quot;,&quot;Oceania&quot;), value = c(5,3,5,4,1)) blank_theme=theme_minimal()+ theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=14, face=&quot;bold&quot;) ) ggplot(desembarques_classes, aes(x=&quot;&quot;, y=value, fill=group)) + blank_theme + scale_fill_brewer(&quot;Blues&quot;)+ labs(title=&quot;Desembarques no terminal internacional A em Cumbica&quot;, subtitle=&quot;(10/10/2021: 8 h 00min às 12 h 00 min)&quot;, caption = &quot;Fonte: próprio autor&quot;) + theme(axis.text.x=element_blank()) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=0) + geom_text(aes(y = value/2 + c(0, cumsum(value)[-length(value)]), label = percent(value/18 )), size=5)+ guides(fill = guide_legend(title = &quot;Legenda&quot;, label.position = &quot;right&quot;, title.position = &quot;top&quot;, title.vjust = 1)) Figure 3.16: Gráfico de setores dos desembarques observados no terminal de desembarque internacional do aeroporto library(ggplot2) library(scales) blank_theme=theme_minimal()+ theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=14, face=&quot;bold&quot;) ) bp=ggplot(dados, aes(x=&quot;&quot;, y=quant, fill=tipo))+ geom_bar(width = 1, stat = &quot;identity&quot;) pie=bp + coord_polar(&quot;y&quot;, start=0) pie + scale_fill_brewer(&quot;Blues&quot;)+ blank_theme + theme(axis.text.x=element_blank()) + geom_text(aes(x = 1.2,label = quant), position = position_stack(vjust = 0.5)) + ggtitle(&quot;Estrutura domiciliar dos Estados Unidos, 2005&quot;) + theme(legend.position = &quot;right&quot;, legend.justification = &quot;center&quot;, legend.direction = &quot;vertical&quot;, legend.spacing.x = unit(0.5, &#39;cm&#39;),legend.spacing.y = unit(0.5, &#39;cm&#39;))+ guides(fill = guide_legend(title = &quot;Tipos de domicílios&quot;, label.position = &quot;right&quot;, title.position = &quot;top&quot;, title.vjust = 1)) Figure 3.17: Gráfico de setores da estrutura domiciliar dos Estados Unidos 3.7.1.3 Colunas para dados em uma tabela de dupla entrada library(ggplot2) # Carrega a biblioteca ggplot2 # Dados fornecidos casal_com_filho_democratas &lt;- 3478 casal_com_filho_republicano &lt;- 2136 casal_sem_filho_democratas &lt;- 2209 casal_sem_filho_republicano &lt;- 2177 # Criar um dataframe com os dados dados &lt;- data.frame( Categoria = c(&quot;Com Filhos&quot;, &quot;Com Filhos&quot;, &quot;Sem Filhos&quot;, &quot;Sem Filhos&quot;), Partido = c(&quot;Democratas&quot;, &quot;Republicanos&quot;, &quot;Democratas&quot;, &quot;Republicanos&quot;), Contagem = c(casal_com_filho_democratas, casal_com_filho_republicano, casal_sem_filho_democratas, casal_sem_filho_republicano) ) # Criar o gráfico de barras empilhadas ggplot(dados, aes(x = Categoria, y = Contagem, fill = Partido)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Contagem de Votos por Categoria e Partido (Censo dos EUA,2005)&quot;, x = &quot;Categoria&quot;, y = &quot;Contagem&quot;) + scale_fill_manual(values = c(&quot;Democratas&quot; = &quot;lightgreen&quot;, &quot;Republicanos&quot; = &quot;lightblue&quot;)) + theme_minimal() Figure 3.18: Gráfico de barras da estrutura familiar em relação à inclinação partidária nos Estados Unidos library(ggplot2) # Carrega a biblioteca ggplot2 # Dados fornecidos fumantes_filho_bp = 275 fumantes_filho_pn = 2144 n_fumantes_filho_bp = 311 n_fumantes_filho_pn = 6640 # Criar um dataframe com os dados dados &lt;- data.frame( Risco = c(&quot;Fumante&quot;, &quot;Fumante&quot;, &quot;Não fumante&quot;, &quot;Não fumante&quot;), Peso = c(&quot;Baixo peso&quot;, &quot;Peso normal&quot;, &quot;Baixo peso&quot;, &quot;Peso normal&quot;), Contagem = c(fumantes_filho_bp, fumantes_filho_pn, n_fumantes_filho_bp, n_fumantes_filho_pn) ) # Criar o gráfico de barras empilhadas ggplot(dados, aes(x = Risco, y = Contagem, fill = Peso)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Peso de recém nascidos em Pelotas (RS, 1982)&quot;, x = &quot;Exposição ao risco&quot;, y = &quot;Contagem&quot;) + scale_fill_manual(values = c(&quot;Baixo peso&quot; = &quot;gray&quot;, &quot;Peso normal&quot; = &quot;lightgreen&quot;)) + theme_minimal() Figure 3.19: Gráfico de barras da exposição ao fator de risco e o efeito 3.7.2 Gráficos para uma variável quantitativa diagrama de ramos e folhas; dispersão unidimensional; e barras; histograma; e, setores; box plot. 3.7.2.1 Ramos e Folhas Diagrama de Ramos e Folhas é uma apresentação híbrida pois ao mesmo tempo que espelha a quantidade de medidas observadas para cada altura, mantém as informações da listagem. stem(alturas) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 14 | 147 ## 15 | 45666788 ## 16 | 12234455556666677778889 ## 17 | 11223333345667889 ## 18 | 233456 ## 19 | 35 ## 20 | 0 À esquerda do traço vertical (os ramos) são apresentadas frações das medidas das alturas (no caso, decímetros) e à direita (as folhas) são apresentadas os complementos dessas medidas (os centímetros) de tal modo que cada um dos dados da amostral original possa ter sua medida resgatada fazendo-se a leitura dos valores à esquerda com cada um deles à direita. Essa apresentação também oferece uma apreciação visual a respeito de como os valores se distribuem. 3.7.2.2 Gráficos de dispersão unidimensional Um Gráfico de dispersão unidimensional (stripchart) expressa visualmente duas informações: a localização de cada uma das medidas e a dispersão dos dados. stripchart(alturas, method = &quot;stack&quot;, offset=1, pch=20, at=0.5, main=&quot;Gráfico de dispersão unidimensional&quot;, col=&quot;blue&quot;,cex=1, xlab=&quot;Alturas dos estudantes (m)&quot;, ylab=&quot;Quantidades observadas (un)&quot;) Figure 3.20: Gráfico de dispersão unidimensional (stripchart) 3.7.2.3 Barras Se modificarmos o diagrama de ramos e folhas dos comprimentos e quantidades observadas, representando cada uma das alturas medidas por um retângulo cujas alturas sejam proporcionais à quantidade contada de cada uma dessas alturas teremos um Gráfico de barras. tab_alturas=table(alturas) barplot(tab_alturas, main=&quot;Valores observados da alturas dos estudantes&quot;, xlab=&quot;Altura (cm)&quot;, ylab=&quot;Quantidade observada (un)&quot;, ylim=c(0,6), col=&quot;blue&quot;, las=0, hor=&quot;FALSE&quot;) Figure 3.21: Gráfico de barras dos dados brutos: uma barra para cada observação e sua altura expressando o número de observações com esse valor 3.7.2.4 Histograma Para dados quantitativos, o agrupamento dos valores brutos observados em classes (cada uma com um valor mínimo e máximo fixado) permite a geração de um Histograma, um tipo diferente de Gráfico de barras onde cada coluna está unida às colunas imediatamente adjacentes (indicando a continuidade de valores das medidas) e sua altura expressa a quantidade de observações contidas nessa classe. Para as classes estabelecias na seção anterior o histograma das alturas dos estudantes terá esse aspecto: h1=hist(alturas, breaks=seq(1.41 , 2.01 , 0.1), include.lowest = TRUE, right = FALSE, main= &quot;Histograma das alturas dos estudantes&quot;, col=&quot;blue&quot;, xlab=&quot;Classes de alturas (m)&quot;, ylab=&quot;Frequência absoluta observada (un)&quot; , cex=0.7, ylim=c(0,30)) text(h1$mids,h1$counts,labels=h1$counts, adj=c(0.5, -0.5)) abline(v=mean(alturas), col=&quot;red&quot;) text(mean(alturas)-0.01, 28, &quot;Média=1,69 m&quot;, col = &quot;red&quot;, srt=90) abline(v=median(alturas), col=&quot;darkgreen&quot;) text(median(alturas)-0.01, 27.2, &quot;Mediana=1,675 m&quot;, col = &quot;darkgreen&quot;, srt=90) abline(v=Modes(alturas), col=&quot;darkgrey&quot;) text(Modes(alturas)+c(-0.01, -0.01), 27, c(&quot;Moda=1,66&quot;,&quot;Moda=1,73&quot;), col = &quot;darkgray&quot;, srt=90) Figure 3.22: Histograma das alturas dos estudantes com as posições da média, moda e mediana Um histograma é a representação gráfica de uma tabela de distribuição de frequências em colunas (retângulos). A base de cada retângulo representa o intervalo de cada classe e a altura, a quantidade ou a frequência absoluta com que aquele valor da classe ocorre no conjunto de dados. O termo histograma foi cunhado por Karl Pearson (c. 1891) e vem da composição em grego de istos (mastro) com gramma (escrita), convertida em inglês para historical diagram: histogram. Como elemento gráfico, seu uso é anterior à sua denominação (maiores detalhes em: (link) ). Num histograma de densidade, a altura de cada retângulo representa uma densidade relacionada à frequência relativa no intervalo de cada classe. h2=hist(alturas,breaks=seq(1.41 , 2.01 , 0.10), include.lowest = TRUE, right = FALSE, main= &quot;Histograma das densidades das alturas dos estudantes&quot;, col=&quot;blue&quot;, xlab=&quot;Classes de alturas (m)&quot;, ylab=&quot;Densidade da freq. relativa&quot;, prob=&quot;TRUE&quot;, ylim=c(0,5)) text(h2$mids,h2$density,labels=round(h2$density, 5), adj=c(0.5, -0.5), cex=0.7) lines(density(alturas), col=&quot;red&quot;) lines(density(alturas, adjust=2), col=&quot;orange&quot;) Figure 3.23: A linha vermelha é uma aproximação da Função de Densidade da frequência relativa de observação (a linha preta é a curva da função densidade de uma distribuição Normal com média e variâncias dadas pelos dados Como a área de cada retângulo é igual à proporção (\\(fr_{i}\\)) da classe (\\(i\\)) a soma de todas essas áreas será igual a 1: (0.10*0.5)+(0.10*1.333)+(0.10*3.833)+(0.10*2.833)+(0.10*1)+(0.10*0.50) ## [1] 0.9999 Uma aproximação para a área sob a curva da Função de Densidade pode ser soma das áreas de um dos retângulo com: Base = \\(\\Delta_{i}\\); e,\\ Altura =\\(\\frac{fr_{i}}{\\Delta_{i}}\\). A área da curva da Função de Densidade delimitada por dois valores quaisquer é uma analogia para a probabilidade de que um determinado valor de altura de um estudante (amostrado aleatoriamente dentre todos os 60 estudantes) esteja contida nesse intervalo. Equivale dizer que, amostrando-se aleatoriamente um estudante dentre todos os 60 alunos, a probabilidade de que a altura desse estudante estaje contida entre os valores mínimo e máximo da amostra é, naturalmente, igual a 1 (100%) 3.7.2.5 Setores Em um Gráfico de setores a representação das quantidades está associada a uma fração do comprimento de um círculo. Para sua confecção considera-se a proporção da quantidade observada específica da quantidade total de dados, expressa na forma de fração do ângulo de um setor circular em relação ao ângulo interno total de um círculo (360o). library(scales) library(ggplot2) alturas_classes=data.frame( group = c(&quot;1,41-1,51&quot;, &quot;1,51-1,61&quot;, &quot;1,61-1,71&quot;, &quot;1,71-1,81&quot;, &quot;1,81-1,91&quot;, &quot;1,91-2,01&quot;), value = c(3,8,23,17,6,3)) blank_theme=theme_minimal()+ theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=14, face=&quot;bold&quot;) ) ggplot(alturas_classes, aes(x=&quot;&quot;, y=value, fill=group)) + blank_theme + scale_fill_brewer(&quot;Blues&quot;)+ ggtitle(&quot;Alturas dos estudantes&quot;) + theme(axis.text.x=element_blank()) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=0) + geom_text(aes(y = value/2 + c(0, cumsum(value)[-length(value)]), label = percent(value/60 )), size=5)+ guides(fill = guide_legend(&quot;Classes de valores (m)&quot;, label.position = &quot;right&quot;, title.position = &quot;top&quot;, title.vjust = 1)) Figure 3.24: Gráfico de setores das alturas dos estudantes 3.7.2.6 Box-plot (gráfico de caixas) O gráfico Box-plot ( box and whisker plot ): esse gráfico apresenta de modo conjunto, informações sobre a posição, dispersão, assimetria e dados discrepantes do conjunto analisado: o segundo quartil (mediana): \\(Q_{2}\\)); os valores mínimo: \\(x_{1}\\) e máximo: \\(x_{n}\\) (dados ordenados); o 1\\(^{o}\\) e 3\\(^{o}\\) quartis; a dispersão (intervalo interquartílico: \\(d_{q}=(Q_{3} - Q_{1})\\)); um limite superior: \\(LS=Q_{3} + 1,50.d_{q}\\); um limite inferior: \\(LI=Q_{1} - 1,50.d_{q}\\); os valores mínimo e máximo observados (caso não existam valores superiores aos limites LI e LS); ou as observações mais extremas, situadas fora dos limites LI e LS (que podem ou não ser outliers , dados atípicos). min=min(alturas) q1=1.635 q2=1.675 med=mean(alturas) q3=1.755 max=max(alturas) iq=q3-q1 ls=q3+1.5*iq li=q1-1.5*iq head(sort(alturas,TRUE)) #2.00 1.95 &gt;&gt;1.93&lt;&lt; 1.86 1.85 1.84 ## [1] 2.00 1.95 1.93 1.86 1.85 1.84 tail(sort(alturas,TRUE)) # 1.56 1.55 1.54 1.47 1.44 &gt;&gt;1.41&lt;&lt; ## [1] 1.56 1.55 1.54 1.47 1.44 1.41 boxplot(alturas, main=&quot;Boxplot do conjunto de dados de alturas&quot;, ylim=c(1.2, 2.1)) lines( y=c(1.47, 1.47), x=c(0.6,1), col=&quot;blue&quot;) text(x=0.60, y=1.47-0.05, &quot;Delimitador inferior do bigode=1,47&quot;, col = &quot;blue&quot;, srt=0) lines( y=c(1.93,1.93), x=c(0.6,1), col=&quot;blue&quot;) text(x=0.60, y=1.93+0.05, &quot;Delimitador superior do bigode=1,93&quot;, col = &quot;blue&quot;, srt=0) lines(y=c(med, med), x=c(1,1.4), col=&quot;blue&quot;) text(x=1.4 , y= med+0.05 , &quot;Média=1,6907&quot;, col = &quot;blue&quot;, srt=0) lines(y=c(q1, q1), x=c(1, 1.4), col=&quot;blue&quot;) text(x=1.4, y=q1 -0.05, &quot;Primeiro quartil: Q1=1,635&quot;, col = &quot;blue&quot;, srt=0) lines(y=c(q2, q2), x=c(0.6,1), col=&quot;blue&quot;) text(x=0.60 , y= q2 - 0.05, &quot;Mediana: Q2=1,675&quot;, col = &quot;blue&quot;, srt=0) lines(y=c(q3, q3), x=c(1, 1.4), col=&quot;blue&quot;) text(x= 1.4 , y=q3 + 0.05, &quot;Terceiro quartil: Q3=1,755&quot;, col = &quot;blue&quot;, srt=0) lines(y=c(li,li) , x=c(1.01,1.4) , col=&quot;red&quot;, lty=2) text(x=1.2, y=q1-1.5*iq-0.05 , &quot;Limite inferior teórico: LI=1,455) &quot;, col = &quot;red&quot;, srt=0) lines(y=c(ls,ls) , x=c(1.01,1.4) , col=&quot;red&quot;, lty=2) text(x=1.2, y=q3+1.5*iq +0.05 , &quot;Limite superior teórico: LS=1,935&quot;, col = &quot;red&quot;, srt=0) points (y=1.47, x=1 , col=&quot;green&quot;, cex=1, lwd=5) text(x=1, y=1.47-0.05 , &quot;Última observação dentro do LI: h=1,47 &quot;, col = &quot;green&quot;, srt=0) points (y=1.93, x=1 , col=&quot;green&quot;, cex=1, lwd=5) text(x=1, y=1.93+0.05 , &quot;Última observação dentro do LS: h=1,93 &quot;, col = &quot;green&quot;, srt=0) Figure 3.25: Box-plot de um rol de valores com Distribuição Normal (média 20 e variãncia 5 "],["introdução-ao-cálculo-de-probabilidades.html", "Capítulo 4 Introdução ao cálculo de probabilidades", " Capítulo 4 Introdução ao cálculo de probabilidades Seria bom começar o capítulo sobre teoria das probabilidades, dando uma definição concisa, simples e intuitiva, todavia formalmente rigorosa. Infelizmente, isto não será possível. Se por um lado, uma definição rigorosa de probabilidade requer um aparato matemático sofisticado e é bem pouco intuitiva; por outro lado as definições simples e frequentemente encontradas são tautológicas como: Probabilidade é um número que quantifica, uma medida da informação disponível sobre a possibilidade de ocorrência de um determinado evento quando ainda não se sabe se ele ocorrerá ou não. Essa definição é ``circular’’ ( definiendum = definien ) uma vez que se vale de um sinônimo de probabilidade (possibilidade) para se auto definir. Todavia ela nos introduz dois conceitos que iremos usar como ponto de partida: probabilidade refere-se a experimentos de resultado incerto (aleatórios); que probabilidade é uma quantidade. "],["introdução-histórica.html", "4.1 Introdução histórica", " 4.1 Introdução histórica Os estudos de probabilidade surgiram no século XVII, motivados por questões práticas relacionadas a jogos de azar e decisões econômicas. Uma das situações que estimulou essas discussões foi o problema apresentado pelo Cavaleiro de Méré (Chevalier de Méré), que envolvia jogos de azar com dados. Ele levantou duas questões principais sobre a probabilidade de certos resultados ao lançar dados, que acabaram influenciando o desenvolvimento da teoria probabilística. A primeira questão envolvia o lançamento de um dado seis vezes, onde Méré acreditava que havia uma alta chance de obter pelo menos um “6”. Sua intuição estava correta: a probabilidade de não obter um “6” em seis lançamentos consecutivos é \\((\\frac{5}{6})^{6}\\) , aproximadamente 33%, o que significa que a chance de obter pelo menos um “6” é de cerca de 67%. O segundo problema que Méré trouxe era mais intrigante e envolvia o lançamento de dois dados 24 vezes. Ele acreditava que deveria obter pelo menos um duplo “6”, mas errou em sua previsão. A probabilidade de não obter um duplo “6” em 24 lançamentos consecutivos é \\((\\frac{35}{36})^{24}\\), aproximadamente 51%, ou seja, a chance de obter um duplo “6” é apenas cerca de 49%, e não tão alta quanto ele esperava ao observar os resultados do jogo. Essa discrepância entre intuição e realidade levou Méré a buscar ajuda com Pascal, e a subsequente troca de ideias com Fermat. Foi nessa correspondência entre Blaise Pascal e Pierre de Fermat em 1654, na qual discutiam problemas de divisão de apostas em jogos interrompidos, que se estabeleceu a base para o conceito de probabilidade esperada. A formalização desses estudos avançou no século XVIII com a publicação da obra Ars Conjectandi (1713) de Jacob Bernoulli, que introduziu a lei dos grandes números. Essa lei estabelece que, com um número crescente de experimentos, a frequência observada de um evento tende a se aproximar de sua probabilidade verdadeira, fornecendo assim uma base teórica sólida para a análise de fenômenos aleatórios. Outro avanço significativo veio com Abraham de Moivre, que em The Doctrine of Chances (1718) aplicou a teoria da probabilidade ao estudo de distribuições estatísticas, introduzindo a curva normal para modelar variáveis aleatórias. Ele também formalizou o conceito de esperança matemática, essencial para a análise de risco e a tomada de decisões em situações de incerteza. No século XIX, Pierre-Simon Laplace sistematizou a teoria da probabilidade em sua obra Théorie Analytique des Probabilités (1812), onde ele introduziu a regra de Bayes, expandindo a aplicação da probabilidade para áreas como astronomia e ciências sociais. Sua abordagem permitiu que a probabilidade fosse utilizada para fazer inferências sobre eventos desconhecidos com base em informações prévias. A evolução da teoria da probabilidade levou, no século XX, à sua formalização por meio da teoria da medida. Esta teoria, desenvolvida por matemáticos como Andrey Kolmogorov na década de 1930, deu à probabilidade um arcabouço rigoroso dentro da matemática, utilizando conceitos de medida para definir a probabilidade como uma função que mapeia eventos (subconjuntos de um espaço amostral) para valores numéricos entre 0 e 1. Esse formalismo ficou conhecido como modelo axiomático da probabilidade. Esses axiomas são a base para o desenvolvimento de modelos probabilísticos consistentes e robustos, que hoje são amplamente utilizados em áreas como finanças, física, estatística e inteligência artificial. Figure 4.1: Astralagus (um dos ossos que compõem o calcanhar, usado no Egito antigo como um dado rudimentar) "],["conceitos-essenciais.html", "4.2 Conceitos essenciais", " 4.2 Conceitos essenciais 4.2.1 Experimentos determinísticos e experimentos aleatórios Aleatório provem do latim: aleatorium: fato cujo desfecho depende de um acontecimento futuro e incerto, resultado da sorte ou acaso, acidental. Probabilidade deriva do latim: probabilitas: qualidade do que se pode comprovar, de probabilis: o que pode passar por um teste, provável e de probare: provar, testar, examinar. Ao contrário de um experimento determinístico, cujo resultado pode ser previamente determinado como : como a reação de dois átomos de hidrogênio com um átomo de oxigênio: \\(2H_{2}+O_{2} \\to 2H_{2}O\\) e a distância \\(S\\) percorrida no vácuo sob velocidade constante \\(V\\) e sem atrito num intervalo de tempo \\(t\\): \\(S = V \\times t\\) o conceito de experimento aleatório é o que estabelece que seu resultado não pode ser previsto com certeza como em: o lançamento de um dado. O resultado pode ser qualquer número inteiro de 1 a 6 e a medição da altura de uma pessoa selecionada aleatoriamente. Os resultados observados apresentam variações mesmo quando esses experimentos são repetidos indefinidamente e sob as mesmas condições; todavia, é possível estabelecer um conjunto cujos elementos compõem todos os possíveis resultados: qualquer número inteiro de 1 a 6: e o conjunto de possíveis resultados é finito e qualquer valor em um intervalo contínuo por exemplo, entre 1,50 m e 2,00 m (com infinitas possibilidades dentro desse intervalo). 4.2.2 O espaço amostral A primeira coisa que fazemos quando começamos a pensar sobre a probabilidade de ocorrência de um certo resultado em um experimento aleatório é tentar listar todos os resultados com possibilidade de ocorrência. Esses resultados formam um conjunto a que denominamos de espaço amostral que, usualmente, é representado pela letra grega maiúscula \\(\\Omega\\). Para que \\(\\Omega\\) seja considerado o espaço amostral desse experimento aleatório ele precisa apresentar duas propriedades: apenas um de seus elementos ocorre cada vez que realizamos o experimento aleatório; e, pelo menos um dos possíveis resultados ocorre sempre que realizarmos o experimento aleatório. Essas condições indicam que os elementos de \\(\\Omega\\) são mutuamente exclusivos e exaustivos. 4.2.2.1 Espaços aleatórios discretos são finitos ou contáveis (infinito numerável) pode-se atribuir uma probabilidade para cada resultado Exemplo 1: Experimento aleatório: lançar um dado e contar o número de pontos na face que ficar exposta para cima Espaço amostral finito: \\(\\Omega = \\{1,2,3,4,5,6\\}\\) Exemplo 2: Experimento aleatório: lançar dois dados e contar o número de pontos nas faces que ficarem expostas para cima Espaço amostral finito: \\(\\Omega = \\{2,3,4,5,6,7,8,9,10,11,12\\}\\) Exemplo 3: Experimento aleatório: lançar uma moeda e contar o número de lançamentos necessários até se obter uma “cara” Espaço amostral infinito contável: \\(\\Omega = \\{1,2,3,4,5, \\dots, k, \\dots\\}\\) Exemplo 4: Experimento aleatório: lançar um dado até se obter um “6” Espaço amostral infinito contável: \\(\\Omega = \\{1,2,3,4,5, \\dots, k, \\dots\\}\\) Um espaço amostral consiste então da enumeração (finita ou infinita contável) de todos os possíveis resultados de serem obtidos em um experimento aleatório. Cada um dos possíveis resultados de um experimento aleatório é chamado de um elemento desse espaço amostral. Assim, para o espaço amostral \\(\\Omega\\), seus elementos serão representados por letras gregas minúsculas \\(\\omega_{n}\\) \\[ \\Omega = \\{\\omega_{1}, \\omega_{2}, \\omega_{3}, ..., \\omega_{n}, \\dots \\} \\] 4.2.2.2 “Espaços” aleatórios contínuos não são contáveis: os resultados possíveis formam um intervalo contínuo de valores probabilidade de um resultado específico é zero: como existem infinitos resultados possíveis, a probabilidade de um valor específico é 0 probabilidades são atribuídas a intervalos: a probabilidade de um evento é associada à extensão do intervalo (comprimento, área, volume, etc.) que o evento ocupa. Exemplo 1: Experimento aleatório: a altura de uma pessoa aleatoriamente sorteada Intervalo amostral: \\(\\Omega = [1,5 ; 2,0 ]\\) m Exemplo 2: Experimento aleatório: o peso de uma pessoa aleatoriamente sorteada Intervalo amostral: \\(\\Omega = [10 ; 100 ]\\) kg Exemplo 3: Experimento aleatório: o teor de um minério por quuilo de uma amostra de solo extraída de um local aleatório Intervalo amostral: \\(\\Omega = [0,001 ; 0,01]\\) gramas 4.2.2.3 Espaços amostrais equiprováveis e não equiprováveis Se todos os elementos que compõem um espaço amostral finito de um experimento aleatório possuem a mesma probabilidade de ocorrência é dito que o espaço amostral desse experimento aleatório é equiprovável (com a mesma probabilidade para todos os seus elementos). Exemplo 1 Experimento aleatório: lançar um dado e contar o número de pontos na face que ficar exposta para cima Espaço amostral finito: \\(\\Omega = \\{1,2,3,4,5,6\\}\\) Probabilidades: \\(P(1)=\\frac{1}{6},P(2)=\\frac{1}{6},\\\\P(3)=\\frac{1}{6},P(4)=\\frac{1}{6},\\\\P(5)=\\frac{1}{6},P(6)=\\frac{1}{6}\\) &gt; Exemplo 2 Experimento aleatório: lançar dois dados e contar o número de pontos nas faces que ficarem expostas para cima Espaço amostral finito: \\(\\Omega = \\{2,3,4,5,6,7,8,9,10,11,12\\}\\) Probabilidades: \\(P(2)=\\frac{1}{36},P(3)=\\frac{2}{36},P(4)=\\frac{3}{36},P(5)=\\frac{4}{36},\\\\P(6)=\\frac{5}{36},P(7)=\\frac{6}{36}, P(8)=\\frac{5}{36}, P(9)=\\frac{4}{36},\\\\ P(10)=\\frac{3}{36}, P(11)=\\frac{2}{36}, P(12)=\\frac{1}{36}\\) Cada um dos elementos que compõem o espaço amostral (a soma dos valores numéricos das faces no lançamento de um dado por duas vezes) poderá resultar de diferentes combinações de valores. A Tabela 4.1 apresenta todas as combinações possíveis de serem obtidas, bem como as proporções em relação ao total para cada elemento do espaço amostral. Table 4.1: Quadro dos possíveis resultados de um experimento aleatório: somas dos valores numéricos das faces no lançamento de um dado por duas vezes Soma Possíveis combinações de resultados nos lançamentos Frequência \\(n_{i}\\) Proporção \\(f_{i}\\) (primeiro,segundo) 2 (1,1) 1 \\(\\frac{1}{36}\\) 3 (1,2); (2,1) 2 \\(\\frac{2}{36}\\) 4 (1,3); (2,2); (3,1) 3 \\(\\frac{3}{36}\\) 5 (1,4); (2,3); (3,2); (4,1) 4 \\(\\frac{4}{36}\\) 6 (1,5); (2,4); (3,3); (4,2); (5,1) 5 \\(\\frac{5}{36}\\) 7 (1,6); (2,5); (3,4); (4,3); (5,2); (6,1) 6 \\(\\frac{6}{36}\\) 8 (2,6); (3,5); (4,4); (5,3); (6,2) 5 \\(\\frac{5}{36}\\) 9 (3,6); (4,5); (5,4); (6,3) 4 \\(\\frac{4}{36}\\) 10 (4,6); (5,5); (6,4) 3 \\(\\frac{3}{36}\\) 11 (5,6); (6, 5) 2 \\(\\frac{2}{36}\\) 12 (6,6) 1 \\(\\frac{1}{36}\\) Totais 36 \\(1\\) As probabilidades de ocorrência de cada um os elementos desse espaço amostral são diferentes e, por essa razão é dito que o espaço amostral desse experimento aleatório tem elementos não equiprováveis. 4.2.3 Evento Define-se como evento de interesse um subconjunto finito do espaço amostral, composto por um ou mais de seus elementos que satisfazem o enunciado estabelecido no experimento aleatório proposto. A expressão evento de interesse (também chamado de sucesso) refere-se, no contexto do cálculo de probabilidades, à ocorrência de um resultado desejado durante a realização de um experimento aleatório. Frequentemente, eventos de interesse são representados por letras maiúsculas do alfabeto romano e podem ser acompanhados de uma notação explicativa, como \\(E(\\dots)\\). Por exemplo, considere um experimento aleatório que consiste em lançar um dado uma única vez. Um possível evento de interesse pode ser: E(obtenção do número 2) (\\(E(2)\\)) e, nesse contexto pode ser a obtenção do número 2 como resultado. Podemos ter variados tipos de eventos de interesse como: simples ou compostos; certos ou impossíveis; dependentes ou independentes ; mutuamente exclusivos ; complementares; 4.2.3.1 Diagramas de Venn para representar o espaço amostral e eventos de interesse Em muitos dos problemas de probabilidade, o evento de interesse pode se definido como associações de dois ou mais eventos formados, por sua vez, por um ou mais elementos do espaço amostral do experimento aleatório. Uniões, interseções e complementos são algumas dessas associações que, doravante, serão muito utilizados. Por essa razão, a representação do espaço amostral e esses eventos por meio de Diagramas de Venn pode ajudar a compreensão de um problema de cálculo probabilidade. Figure 4.2: John Venn, 1834–1923 4.2.3.1.1 União \\(A \\cup B\\) Sejam \\(A\\) e \\(B\\) dois eventos de interesse definidos sobre o espaço amostral \\(\\Omega=\\{1,2,3,4,5,6\\}\\) (lançamento de um dado) tais que \\(A=\\{1,2,3\\}\\) e \\(B=\\{2,4,6\\}\\). Um evento de interesse \\(E\\) expresso como a união desses dois outros, representado por \\(E=(A \\cup B)\\), será o subconjunto do espaço amostral \\(\\Omega\\) que contém os elementos que pertençam a \\(A\\), ou a \\(B\\) ou a ambos. Desse modo, \\(E=A \\cup B=\\{1,2,3,4,6\\}\\) e o Diagrama de Venn correspondente será: Figure 4.3: União: \\(A \\cup B\\) Na realização desse experimento aleatório (lançar um dado) o evento de ineteresse \\(E\\) ocorrerá quando qualquer um dos resultados for um elemento pertencente a \\(A\\), ou a \\(B\\) ou a ambos. 4.2.3.1.2 Interseção \\(A \\cap B\\) Um evento de interresse \\(E\\) definido como a interseção dos eventos \\(A\\) e \\(B\\) anteriormente definodos, representado por \\(E=(A \\cap B)\\), será o subconjunto do espaço amostral \\(\\Omega\\) que contém todos os elementos que pertençam a ambos os eventos A e B simultaneamente. Desse modo, \\(E=(A \\cap B) =\\{2\\}\\) e o Diagrama de Venn correspondente será: Figure 4.4: Interseção: \\(A \\cap B\\) Na realização desse experimento aleatório (lançar um dado) o evento de interesse \\(E\\) ocorrerá apenas quando o resultado for um elemento simultaneamente pertencente a \\(A\\) e \\(B\\) . Quando o evento de interesse é definido pela interseção de dois outros, todavia esssa interseção é vazia, representa-se \\(E\\) como \\[ E(A \\cap B) = \\varnothing \\] 4.2.3.1.3 Complemmento \\(A^{c}\\) Um evento de intersse pode também ser definido como o complemento de outros como, por exemplo, de \\(A\\), sendo representado representado por \\(E=(A^{c})\\) (ou \\(E=(\\stackrel{-}{A})\\)). Desse modo, \\(E=(A^{c}) =\\{4,5,6\\}\\) e o Diagrama de Venn correspondente será: Figure 4.5: Complementar \\(A^{c}\\) De modo análogo, para \\(E=(B^{c})=\\{1,3,5\\}\\) e o Diagrama de Venn correspondente será : Figure 4.6: Complementar de B Figure 4.7: Diagramas de Venn 4.2.3.2 Eventos simples e eventos compostos O evento de interesse (\\(E(2)\\)) definido no experimento aleatório anterior (obter o número 2) é formado por apenas um elemento do espaço amostral. Eventos formados por apenas um elemento do espaço amostral são denominados de evento simples. \\[ \\Omega = \\{1; 2; 3; 4; 5; 6\\}\\\\ E(2) = \\{2\\} \\] Admita agora o mesmo experimento aleatório todavia definindo como evento de interesse (Eobter-se um número par. Um evento de interesse assim definido é um evento composto uma vez que é formado por mais de um elemento do espaço amostral: \\[ \\Omega = \\{1; 2; 3; 4; 5; 6\\}\\\\ E(par) = \\{2; 4; 6\\} \\] Outro exemplo, a partir de um experimento aleatótrio que consiste em se lançar uma moeda duas vezes, cujo espaço amostral é representado por um conjunto composto por quatro elementos \\[ \\Omega = \\{(\\text{Cara}, \\text{Coroa}),(\\text{Coroa}, \\text{Cara}),(\\text{Cara}, \\text{Cara}), (\\text{Coroa}, \\text{Coroa})\\} \\] Se definirmos como evento de interesse na realização desse experimento aleatório obter-se \\(E=\\{(Cara, Cara)\\}\\), o evento \\(E\\) será um evento simples pois é formado por apenas um elemento do espaço amostral. Se, por outro lado, definimos como sucesso obter-se \\(E_{1}=\\{(Cara, Coroa) \\text{ ou } (Coroa, Cara)\\}\\), o evento \\(E_{1}\\) será um evento composto pois é formado por dois elementos do espaço amostral. Se codificarmos Cara=1 e Coroa=0, podemos representar num plano \\(XY\\) o espaço amostral \\(\\Omega\\) desse experimento aleatório e o evento de sucesso \\(E_{1}\\) Figure 4.8: Representação gráfico do espaço amostral do experimento aleatório e do evento de interesse definido 4.2.3.3 Eventos certos e eventos impossíveis Um evento de interesse \\(G\\), definido sobre o espaço amostral \\(\\Omega\\), em que \\(G = \\Omega\\), expressa que qualquer elemento de \\(\\Omega\\) satisfaz o evento \\(G\\), ou seja, qualquer um dos possíveis resultados do experimento aleatório corresponde ao evento. Um evento de interesse assim definido ocorrerá com certeza, razão pela qual tais eventos são denominados eventos certos. Por outro lado, se definirmos um evento de interesse \\(I\\) que não contém resultados pertencentes a \\(\\Omega\\) — o espaço amostral, ou seja, todos os resultados possíveis — como, por exemplo, obter o número 7 no lançamento de um dado de seis faces, esse evento será impossível de ocorrer. Eventos assim definidos são chamados de eventos impossíveis. 4.2.3.4 Eventos independentes Dois eventos são considerados independentes quando a probabilidade de ocorrência de um evento de interesse em um determinado experimento aleatório não é influenciada pelo resultado prévio de outro evento. Em outras palavras, a ocorrência de um evento não altera a probabilidade do outro. Caso contrário, esses eventos são classificados como dependentes ou condicionados. Este conceito será explorado em maior detalhe em seções posteriores. 4.2.3.5 Eventos mutuamente exclusivos Dois eventos que nunca poderão ocorrer simultaneamente são ditos mutuamente exclusivos. No experimento do lançamento da moeda por uma vez, nunca observaremos, simultaneamente, dois eventos como \\(E=\\{(Cara)\\}\\) e \\(F=\\{(Coroa)\\}\\). Um evento assim definido teria sua interseção vazia \\[ G=(E \\cap F) = \\varnothing \\] e, por essa razão, sua probabilidade será \\(P(G)=P(E \\cap F)=0\\). 4.2.3.6 Eventos complementares Definido um evento de interesse qualquer pode-se observar apenas dois resultados: ocorrer; não ocorrer o sucesso. Ou seja, um ou outro deverá forçosamente ocorrer. Chama-se de evento complementar (\\(E^{c}\\) ou \\(\\stackrel{-}{E}\\)) a um evento (\\(E\\)) e sua probabilidade de sucesso será: \\[ P(E^{c}) = 1 - P(E) \\] Se a probabilidade de sucesso de que ele ocorra for \\(P(E)=p\\) e a de que ele não ocorra for \\(P(E^{c}= q)\\) vê-se que a soma dessas quantidades deverá ser \\(p + q =1\\), novamente antecipando um dos postulados do conceito axiomático de probabilidade. 4.2.4 Probabilidade 4.2.4.1 Conceito clássico ou a priori Sob uma visão intuitiva, a probabilidade como uma medida da informação que temos sobre a possibilidade de ocorrência de um evento aleatório, pode ser definida como a medida numérica expressa em termos relativos (percentuais), obtida pela razão (proporção) entre o número de eventos favoráveis (sucessos) pelo número total de eventos prováveis no experimento (espaço amostral). Esse conceito de probabilidade é denominado clássico ou a priori, baseado em um conhecimento prévio ou uma crença subjetiva sobre a probabilidade de um evento ocorrer. Por exemplo, um jogador de cartas pode ter uma crença a priori de que a probabilidade de uma carta ser um ás é de 1 em 13, independentemente do número de baralhos no jogo A distribuição de frequências é um instrumento importante para a análise da variabilidade de experimentos aleatórios e, em particular, as frequências relativas são estimativas das probabilidades. \\[ P(E)= \\frac{\\text{número de resultados de interesse (sucessos)}}{\\text{número total de resultados possíveis no espaço amostral}} \\] Com o estabelecimento de suposições adequadas, um modelo teórico de probabilidade pode ser empregado sem a realização a priori do experimento aleatório, reproduzindo de modo razoável a distribuição das frequências quando o experimento é realizado. Consideremos o exemplo do experimento que consiste em se lançar um dado e observar o valor numérico de sua face. As suposições que deveriam ser estabelecidas a priori são: só pode ocorrer uma das seis faces; e, o dado utilizado não possui viés algum (não favorece face alguma). Como todos os \\(N\\) resultados do espaço amostral apresentam uma mesma probabilidade de ocorrência, então a proporção teórica de ocorrência de qualquer um desse resultados poderá ser apresentado na forma vista na na forma vista na Tabela 4.2. \\[ P(E)= \\frac{1}{N} \\] Table 4.2: Distribuição das proporções teóricas do um experimento aleatório: lançamento de um dado Face 1 2 3 4 5 6 Total Proporção teórica \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) 1 Sendo equiprováveis todos os elementos do espaço amostral, todos terão a mesma probabilidade de ocorrência que será: \\[\\begin{align*} P(E) = &amp; \\frac{1}{N} \\\\ = &amp; \\frac{1}{6} \\\\ = &amp; \\frac{1}{6} \\end{align*}\\] Por essa razão sabe-se, a priori a probabilidade de ocorrência de qualquer evento ao se realizar esse tipo de experimento aleatório uma única vez. 4.2.4.2 Conceito frequentista ou a posteriori Todavia, se realizarmos o experimento aleatório anterior apenas algumas, tal regularidade poderá não ser comprovada: as frequências observadas (as quantidades obtidas para cada um dos valores numéricos das faces) apresentarão uma grande irregularidade diferindo das frequência teóricas definidas. Observa-se que os resultados das frequências observadas irá se estabilizar, aproximando-se das frequências teóricas, à medida que se repete esse experimento um número suficientemente grande de vezes. A definição frequencial (a posteriori): 1- refere-se à probabilidade empírica observada a posteriori; 2- tem por objetivo estabelecer um modelo adequado à interpretação de alguns tipos de experimentos aleatórios; e, 3- é a base para se formular um modelo teórico de distribuição de probabilidades como os que serão abordados mais adiante. Ao se repetir o experimento aleatório um grande número de vezes ( \\(n\\) tendendo a infinitas vezes), a quantidade de vezes que um determinado resultado foi verificado dividida por o número de repetições realizadas (\\(n\\)) irá se aproximar de sua proporção teórica. É o que se denomina como regularidade estatística dos resultados por essa propriedade não mais se necessita que os eventos sejam equiprováveis. Formalmente conhecida como Lei Fraca dos Grandes Números (um dos pilares da teoria da probabilidade, foi formalizada pelo matemático suíço Jakob Bernoulli em 1713) e estabelece uma convergência para a probabilidade: à medida que o número de ensaios independentes de um experimento aleatório aumenta, a frequência relativa dos sucessos observados tende a se aproximar da probabilidade teórica \\[ P\\left(E\\right)=\\underset{n\\to \\infty }{lim}{\\frac{F(E)}{n}} \\] onde: \\(P(E)\\) é a probabilidade de ocorrência do evento \\(E\\); \\(F(E)\\) é a frequência observada do evento \\(E\\) (o número de vezes que ele ocorre em n repetições); e, \\(n\\) é o número de repetições do experimento. Jakob Bernoulli in 1713 4.2.4.2.1 Simulações As simulações desempenham um papel fundamental no entendimento prático dos conceitos probabilísticos, permitindo a reprodução de experimentos aleatórios em larga escala. Por meio de simulações, podemos verificar empiricamente a convergência das frequências observadas para as frequências teóricas discutidas nos conceitos anteriores. Elas fornecem uma ferramenta poderosa para ilustrar a regularidade estatística dos resultados, especialmente em situações em que realizar o experimento real seria impraticável ou custoso. Ao simular o lançamento de um dado, por exemplo, é possível observar como a frequência relativa de qualquer face começa a se aproximar da probabilidade teórica (\\(P(\\cdot)=\\frac{1}{6}\\)) à medida que aumentamos o número de repetições. # Função para lançar o dado n vezes e calcular a frequência de uma face específica lancar_dado &lt;- function(n, face_escolhida) { # Definindo as faces do dado faces &lt;- 1:6 # Realizando n lançamentos lancamentos &lt;- sample(faces, n, replace = TRUE) # Calculando a frequência observada da face escolhida frequencia &lt;- sum(lancamentos == face_escolhida) / n * 100 # Exibindo a frequência em percentual cat(&quot;A frequência observada da face&quot;, face_escolhida, &quot;foi de&quot;, frequencia, &quot;%\\n&quot;) } lancar_dado(10, 3) ## A frequência observada da face 3 foi de 10 % lancar_dado(10000, 3) ## A frequência observada da face 3 foi de 16.6 % Ao simular o lançamento de um dado, por exemplo, é possível observar como as frequências relativas de todas as faces começam a se aproximar das probabilidades teóricas (\\(P(1)=P(2)=P(3)=P(4)=P(5)=P(6)=\\frac{1}{6}\\)) à medida que aumentamos o número de repetições. library(ggplot2) lanca_dado &lt;- function(numero_de_lancamentos) { # Gere os lançamentos do dado lancamentos &lt;- sample(1:6, numero_de_lancamentos, replace = TRUE) # Crie um data frame com os resultados dados &lt;- data.frame(Face = lancamentos) # Contagem das ocorrências de cada face contagem &lt;- table(dados$Face) # Crie um gráfico de barras com o número de lançamentos no título grafico &lt;- ggplot(data = data.frame(Face = names(contagem), Contagem = as.numeric(contagem)), aes(x = Face, y = Contagem)) + geom_bar(stat = &quot;identity&quot;) + labs(x = &quot;Face do Dado&quot;, y = &quot;Contagem&quot;) + ggtitle(paste(&quot;Lançamento de um Dado por:&quot;, numero_de_lancamentos, &quot;vezes&quot;)) + theme_minimal() # Exiba o gráfico print(grafico) } lanca_dado(10) Figure 4.9: Histograma das frequências observadas em 10 lançamentos de um dado justo, evidenciando a variabilidade significativa nas frequências relativas, mesmo que todos os resultados sejam igualmente prováveis. lanca_dado(10000) Figure 4.10: Histograma das frequências observadas em 10.000 lançamentos de um dado justo, ilustrando a convergência assintótica das frequências relativas de cada resultado para sua probabilidade teórica, considerando que todos os resultados são igualmente prováveis. Ao simular o lançamento de dois dados, por exemplo, é possível observar como as frequências relativas de todas as possíveis somas das faces começam a se aproximar das probabilidades teóricas: \\[ P(2) = P(12) = \\frac{1}{36}, \\\\ P(3) = P(11) = \\frac{2}{36}, \\\\ P(4) = P(10) = \\frac{3}{36}, \\\\ P(5) = P(9) = \\frac{4}{36}, \\\\ P(6) = P(8) = \\frac{5}{36}, \\\\ P(7) = \\frac{6}{36} \\] à medida que aumentamos o número de repetições. library(ggplot2) lanca_dois_dados &lt;- function(numero_de_lancamentos) { # Gere os lançamentos dos dois dados dado1 &lt;- sample(1:6, numero_de_lancamentos, replace = TRUE) dado2 &lt;- sample(1:6, numero_de_lancamentos, replace = TRUE) # Calcule a soma dos dois dados somas &lt;- dado1 + dado2 # Crie um data frame com os resultados dados &lt;- data.frame(Soma = somas) # Contagem das ocorrências de cada soma contagem &lt;- table(dados$Soma) # Crie um data frame com a proporção de cada soma dados_grafico &lt;- data.frame( Soma = as.numeric(names(contagem)), Contagem = as.numeric(contagem), Proporcao = as.numeric(contagem) / numero_de_lancamentos ) # Probabilidades teóricas de cada soma prob_teoricas &lt;- c(1/36, 2/36, 3/36, 4/36, 5/36, 6/36, 5/36, 4/36, 3/36, 2/36, 1/36) somas_teoricas &lt;- 2:12 prob_teoricas_formatadas &lt;- paste0(&quot;(&quot;, round(prob_teoricas * 100, 2), &quot;%)&quot;) # Adicione as probabilidades teóricas ao eixo x como rótulos labels_eixo_x &lt;- paste(somas_teoricas, prob_teoricas_formatadas) # Crie o gráfico de barras com as frequências observadas e as probabilidades teóricas no eixo x grafico_somas &lt;- ggplot(data = dados_grafico, aes(x = Soma, y = Proporcao)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) + geom_text(aes(label = scales::percent(Proporcao, accuracy = 0.1)), vjust = -0.5) + # Exibe as proporções acima das barras scale_x_continuous(breaks = somas_teoricas, labels = labels_eixo_x) + # Define os rótulos com soma e probabilidade scale_y_continuous(labels = scales::percent) + # Formata o eixo y como porcentagem labs(x = &quot;Soma dos Dados (Probabilidade Teórica)&quot;, y = &quot;Proporção das Observações (%)&quot;) + ggtitle(paste(&quot;Lançamento de Dois Dados por:&quot;, numero_de_lancamentos, &quot;vezes&quot;)) + theme_minimal() # Exiba o gráfico print(grafico_somas) } lanca_dois_dados(10) Figure 4.11: Histograma das frequências observadas em 10 lançamentos de dois dados justos, evidenciando a diferença significativa das frequências relativas observadas em relação às probabilidades teóricas de cada resultado possível. lanca_dois_dados(10000) Figure 4.12: Histograma das frequências observadas em 10.000 lançamentos de dois dados justos ilustrando a convergência assintótica das frequências relativas observadas para as probabilidades teóricas de cada resultado possível. 4.2.4.3 Conceito axiomático Esta abordagem é baseada em um conjunto de axiomas matemáticos que definem as propriedades básicas de probabilidades. A probabilidade é definida como uma função de conjuntos que atribui a cada conjunto de eventos um número entre 0 e 1, satisfazendo os axiomas matemáticos de probabilidade. Essa abordagem permite que as probabilidades sejam definidas formalmente e usadas para cálculos matemáticos. Um axioma é uma premissa considerada necessariamente evidente e verdadeira, fundamento de uma demonstração, porém ela mesma indemonstrável, originada, segundo a tradição racionalista, de princípios inatos da consciência ou, segundo os empiristas, de generalizações da observação empírica. Admita \\(P\\) uma função que opera sobre o espaço \\(\\Omega\\); isto é, uma função que associa uma quantidade \\(P(\\Omega)\\) a cada elemento \\(\\omega\\) \\(\\in\\) \\(\\Omega\\). Figure 4.13: Representação gráfica da função \\(P(\\Omega)\\) Essa função \\(P\\) será uma função de probabilidade se, e somente se, satisfizer a três axiomas (postulados: conceitos iniciais necessários à construção ou aceitação de uma teoria) estabelecidos por Andrey Kolmogorov (1933). Figure 4.14: Andrey Nikolaevich Kolmogorov (1903-1987) Kolmogoroff afirmou que uma Teoria das probabilidades poderia ser desenvolvida a partir de axiomas, da mesma forma que a geometria e a álgebra, e a considerou como caso especial da Teoria da medida e integração desenvolvida por Lebesgue, Borel e Fréchet. Ele estabeleceu como postulados as propriedades comuns das noções de probabilidade clássica e frequentista que, desta forma, viraram casos particulares da definição axiomática. 4.2.4.3.1 Postulado do intervalo A probabilidade de qualquer \\(E\\) é um número real entre 0 e 1 (pode-se entender isso como uma convenção, onde então se estabelece a medida da probabilidade é um número positivo e que qualquer evento pode ter probabilidade de, no máximo, 1). Esse postulado está plenamente de acordo com a interpretação frequentista de probabilidade. \\[ P(E) \\ge 0 \\text{ (não negatividade e,)}\\\\ \\text{mais especificamente, }\\\\ 0 \\hspace{0.5cm} \\le P(E) \\hspace{0.5cm} \\le 1 \\] 4.2.4.3.2 Postulado da certeza (normalização) O segundo postulado refere-se à probabilidade do evento certo ser igua a 1. No que diz respeito à interpretação frequentista, uma probabilidade de 1 implica que o evento em questão ocorrerá 100% do tempo ou, em outras palavras, que é certo que ele ocorra (como, p. exemplo, um experimento aleatório de se lançar dois dados e somar o valor de suas faces o evento certo poderia ser definido como observar uym valor menor que 13 ou maior que 2) \\[ P(\\Omega) = 1 \\] 4.2.4.3.3 Postulado da aditividade para eventos mutuamente exclusivos (aditividade) \\[ P\\left(\\bigcup _{n=1}^{\\infty }{\\omega}_{n}\\right)=\\sum _{n=1}^{\\infty }P\\left({\\omega}_{n}\\right) \\] para qualquer sequência de eventos mutuamente exclusivos \\(\\{\\omega_{1}, \\omega_{2}, \\omega_{3}, ..., \\omega_{n}, ...\\}\\) (isto é, tal que \\(\\omega_{i} \\cap \\omega_{j} \\varnothing\\) se \\(i \\neq j\\)) Tomando o terceiro postulado no caso mais simples, isto é, para dois eventos mutuamente exclusivos \\(\\omega_{1}\\) e \\(\\omega_{2}\\), pode ser facilmente visto que é satisfeito pela interpretação frequentista. Se um evento ocorrer, digamos, 28% das vezes, outro evento ocorrerá 39%, e os dois eventos não podem ocorrer ao mesmo tempo (ou seja, são mutuamente exclusivos), então um ou outro evento} ocorrerão em 28 + 39 = 67% das vezes. Assim, o terceiro postulado é satisfeito, e o mesmo tipo de argumento se aplica quando há mais de dois eventos mutuamente exclusivos. Recapitulando 1- foi definido o conceito de experimento aleatório como sendo aquele cujos resultados não podem ser determinados com certeza antes de sua realização; 2- foi definido o conceito de espaço amostral de um experimento aleatório como sendo o conjunto de todos os possíveis resultados que ele pode apresentar; 3- foi definido que um evento de interesse é um subconjunto do espaço amostral no qual estamos particularmente interessados; 4- foi definida uma função que tem como domínio o espaço amostral e associa uma quantidade (entre 0 e 1) a cada elemento do espaço amostral; e, por fim, 5- estabelecemos que se essa função atende a três postulados então ela será uma medida da probabilidade de ocorrẽncia de cada evento do espaço amostral em questão. Assim, quando uma função \\(P\\) associa uma quantidade \\(P(\\Omega)\\) a um evento \\(\\omega\\) e \\(P(\\Omega)\\) atende aos três axiomas anteriormente estabelecidos, diz-se que que ela é a função de probabilidade de \\(\\Omega\\). "],["probabilidade-da-união-de-eventos.html", "4.3 Probabilidade da união de eventos", " 4.3 Probabilidade da união de eventos Considerem o espaço amostral de um experimento que consiste no lançamento de um dado honesto: \\(S=\\{1,2,3,4,5,6\\}\\) e admitam alguns eventos constituídos sobre esse espaço mostral, abaixo descritos: \\[ E_{1}=\\{par\\},\\\\ E_{2}=\\{ímpar\\},\\\\ E_{3}=\\{1,2,3\\},\\\\ E_{4}=\\{4,5,6\\},\\\\ E_{5}=\\{\\ge4\\},\\\\ E_{6}=\\{\\le5\\}.\\\\ \\] A partir desses eventos podemos propros novos eventos de interesse a partir de uniões (conectivo \\(\\cup\\)) de dois (ou mais) dos eventos originais como, por exemplo, \\[ H_{a}=E_{1} \\cup E_{2},\\\\ H_{b}=E_{1} \\cup E_{4},\\\\ H_{c}=E_{2} \\cup E_{3},\\\\ H_{d}=E_{2} \\cup E_{5},\\\\ H_{e}=E_{4} \\cup E_{6},\\\\ H_{f}=E_{3} \\cup E_{5}. \\]. No experimento aleatório estabelecido: lembrando que a união de dois conjuntos é o conjunto formado pelos elementos que estão em um, no outro ou em ambos, e pensando em probabilidade como a razão do “número de resultados favoráveis” pelo “número de resultados possíveis” (conceito a priori) podemos fcimente verificar que as probabilidades de ocorrência desses eventos são: \\[ P(H_{a})=1\\\\ P(H_{b})=P(H_{c})=P(H_{d})=P(H_{e})=\\frac{1}{3}\\\\ P(H_{f})=0\\\\ \\] Considerem agora a Tabela 4.3, de dupla entrada, na qual vemos a distribuição dos alunos de uma escola conforme seu sexo e o curso: Table 4.3: Distribuição da quantidade de alunos segundo seu sexo e curso escolhido Curso Sexo Masculino (M) Feminino (F) Total Matemática pura (M) 70 40 110 Matemática aplicada (A) 15 15 30 Estatística (E) 10 20 30 Computação (C) 20 10 30 Total 115 85 200 Essa tabela nos possibilita calcular a probabilidade de ocorrência de diversos eventos de interesse que desejemos estabelecer. Exemplo: seja o experimento aleatório de se escolher, aleatoriamente, um estudante qualquer desses quatro cursos. Assim, se definimos nosso evento de interesse \\(M\\) como sendo M:sexo masculino, a probabilidade de sucesso (que o indivíduo sorteado aleatoriamente seja do sexo masculino) será: \\[ P(M) = \\frac{115}{200} \\] Exemplo: se nosso evento de interesse \\(A\\) como sendo \\(A:\\) curso de matemática aplicada , a probabilidade de sucesso (que o indivíduo sorteado aleatoriamente seja do curso de matemática aplicada será): \\[ P(A) = \\frac{30}{200} \\] A partir dos eventos de interesse anteriormente estabelecidos, podemos definir outros eventos na forma de uniões (\\(\\cup\\)) e interseções (\\(\\cap\\)): uma união entre os dois eventos de interesse anteriores \\(A\\) e \\(M\\) é representada por \\(A \\cup M\\) (alternativamente lê-se também ou) e representa um evento onde pelo menos um dos dois eventos básicos pode ocorrer: ou \\(A\\), ou \\(M\\) ou ambos; e, uma interseção dos dois eventos de interesse anteriores \\(A\\) e \\(M\\) é representada por \\(A \\cap M\\) (alternativamente lê-se também e) e representa um evento onde os dois eventos básicos devem ocorrer: \\(A\\) e \\(M\\). Exemplo: se definimos nosso evento de interesse (\\(P(A \\cap M)\\)) como sendo sexo masculino e cursando matemática aplicada. Facilmente podemos visualizar na Tabela 4.3 que apenas 15 alunos do curso do evento de interesse (matemática aplicada) são do sexo do segundo evento de interesse (masculino), em relação a todo espaço amostral e assim: \\[ P(A \\cap M) = \\frac{15}{200} \\]. Exemplo: consideremos agora o evento de interesse (\\(P(A \\cup M)\\)) como sendo sexo masculino ou cursando matemática aplicada. Na Tabela 4.3 temos as duas probabilidades marginais: \\(P(A)=\\frac{30}{200}\\) (curso: matemática aplicada); e, 2- \\(P(M)=\\frac{115}{200}\\) (sexo masc). Poderíamos intuir equivocadamente que: \\[ P(A \\cup M) = P(A) + P(M) = \\frac{30}{200} + \\frac{115}{200} = \\frac{145}{200} \\] Tal raciocínio é errado pois iria considerar por duas vezes os alunos do sexo masculino. Uma fração da quantidade global (115) de alunos do sexo masculino já considera aqueles que estão matriculados no curso de matemática aplicada (15). É preciso subtrair da soma das probabilidades marginais essa parcela em comum que é a interseção dos dois eventos básicos. A resposta correta será: \\[ P(A \\cup M) = P(A) + P(M) - P(A \\cap M) = \\frac{30}{200} + \\frac{115}{200} -\\frac{15}{200} = \\frac{130}{200} \\]. Portanto, para quaisquer eventos de intersse \\(A\\) e \\(B\\), podemos estabelecer uma regra geral da pobabilidade da união de dois eventos quaiquer como: \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\] Se \\(A\\) e \\(B\\) forem mutuamente exclusivos, a interseção entre eles será vazia (\\(A \\cap B =\\varnothing\\)) e, assim, essa probabiidade é zero. Nessa situação, a probabilidade de \\(P(A \\cup B)\\) fica reduzida a uma regra particular para a adição de probabilidades de eventos mutuamente exclusivos: \\[ P(A \\cup B) = P(A) + P(B) \\] Relembrando o que se denomina como regularidade estatística dos resultados : \\[ P\\left(E\\right)=\\underset{n\\to \\infty }{lim}{\\frac{F(E)}{n}} \\] # Função para simular uma população maior mantendo proporções simPop &lt;- function(tamanho_populacao) { # Proporções conforme a tabela prop &lt;- data.frame( Curso = c(&quot;Matemática pura&quot;, &quot;Matemática aplicada&quot;, &quot;Estatística&quot;, &quot;Computação&quot;), Masculino = c(70, 15, 10, 20), Feminino = c(40, 15, 20, 10), Total = c(110, 30, 30, 30) ) # Calculando as proporções relativas prop$propM &lt;- prop$Masculino / prop$Total prop$propF &lt;- prop$Feminino / prop$Total # Função para gerar amostra de acordo com as proporções gerar_amostra &lt;- function(curso, propM, propF, total, tamanho_populacao) { n_curso &lt;- round((total / sum(prop$Total)) * tamanho_populacao) sexo &lt;- sample(c(&quot;M&quot;, &quot;F&quot;), n_curso, replace = TRUE, prob = c(propM, propF)) data.frame(Curso = rep(curso, n_curso), Sexo = sexo) } # Gerando a população para cada curso populacao &lt;- do.call(rbind, lapply(1:nrow(prop), function(i) { gerar_amostra(prop$Curso[i], prop$propM[i], prop$propF[i], prop$Total[i], tamanho_populacao) })) return(populacao) } # Gerando uma população de 10.000 indivíduos popSim &lt;- simPop(100000) table(popSim$Curso, popSim$Sexo) ## ## F M ## Computação 4989 10011 ## Estatística 10104 4896 ## Matemática aplicada 7511 7489 ## Matemática pura 19998 35002 # Selecionar uma amostra de 200 com reposição (simular a tabela) ordem1=c(&quot;Matemática pura&quot;, &quot;Matemática aplicada&quot;, &quot;Estatística&quot;, &quot;Computação&quot;) ordem2=c(&quot;M&quot;, &quot;F&quot;) amostPop=popSim[sample(1:nrow(popSim), 200, replace = TRUE), ] tab=table(factor(amostPop$Curso, levels = ordem1), factor(amostPop$Sexo, levels = ordem2)) tab=addmargins(tab) colnames(tab)=c(&quot;M&quot;, &quot;F&quot;, &quot;Total&quot;) rownames(tab)=c(&quot;Matemática pura (M)&quot;, &quot;Matemática aplicada (A)&quot;,&quot;Estatística (E)&quot;,&quot;Computação (C)&quot;,&quot;Total&quot;) tab ## ## M F Total ## Matemática pura (M) 76 44 120 ## Matemática aplicada (A) 11 14 25 ## Estatística (E) 10 18 28 ## Computação (C) 16 11 27 ## Total 113 87 200 # Calcular a probabilidade de ser do sexo &quot;Masculino&quot; na amostra pMasc&lt;- mean(amostPop$Sexo == &quot;M&quot;) pMasc ## [1] 0.565 # Calcular a probabilidade de cursar &quot;Matemática aplicada&quot; na amostra pMatAp &lt;- mean(amostPop$Curso == &quot;Matemática aplicada&quot;) pMatAp ## [1] 0.125 # Calcular a probabilidade de cursar &quot;Matemática aplicada&quot; -E- ser do sexo &quot;Masculino&quot; na amostra pMatAp_and_Masc &lt;- mean(amostPop$Curso == &quot;Matemática aplicada&quot; &amp; amostPop$Sexo == &quot;M&quot;) pMatAp_and_Masc ## [1] 0.055 # Calcular a probabilidade de cursar &quot;Matemática aplicada&quot; -OU- ser do sexo &quot;Masculino&quot; na amostra pMatAp_or_Masc &lt;- mean(amostPop$Curso == &quot;Matemática aplicada&quot; | amostPop$Sexo == &quot;M&quot;) pMatAp_or_Masc ## [1] 0.635 Exemplo: Uma população é composta por 20 pessoas que consomem o produto A, 30 pessoas que consomem o produto B e 50 pessoas que consomem o produto C . Um pesquisador de mercado seleciona aleatoriamente uma pessoa desta população. Sabendo que uma pessoa não consome mais de um produto ao mesmo tempo, qual a probabilidade de ter sido selecionada uma pessoa que consome os produtos A ou C? Solução: Definindo os eventos de interesse e as probabilidades associadas: 1- \\(E_{A}=\\text{consumidor do produto A}\\): \\(P(E_{A}=\\frac{20}{100}\\)); 2- \\(E_{B}=\\text{consumidor do produto B}\\): \\(P(E_{B}=\\frac{30}{100}\\)); e, 3- \\(E_{C}=\\text{consumidor do produto C}\\): \\(P(E_{C}=\\frac{50}{100}\\)). Pela regra geral da probabilidade da união de dois eventos quaiquer sabemos que: \\[ P(E_{A} \\cup E_{C}) = P(E_{A}) + P(E_{C}) - P(E_{A} \\cap E_{C}) \\] Como foi estabelecido no enunciado que uma pessoa não consome mais de um produto ao mesmo tempo (esses eventos são, portanto, mutuamente exclusivos: \\(E_{A} \\cap E_{C}=\\varnothing\\)) a probabilidade pedida será: \\[\\begin{align*} P(E_{A} \\cup E_{C}) &amp; = P(E_{A}) + P(E_{C}) - P(E_{A} \\cap E_{C}) \\\\ &amp; = \\frac{20}{10} + \\frac{50}{100} - 0 \\\\ &amp; = \\frac{70}{100} \\\\ &amp; = 0,70 \\end{align*}\\] "],["probabilidade-de-eventos-condicionados.html", "4.4 Probabilidade de eventos condicionados", " 4.4 Probabilidade de eventos condicionados Admita dois eventos definidos sobre o exprimento aleatório de se sortear uma carta de um baralho: \\(A: \\{1, 2,3,4,5,6,7,8,9,10,J,Q,K\\}_{vermelho}\\): cartas vermelhas, e \\(B: \\{J, Q, K\\}_{qualquer}\\): cartas de figuras. A probabilidade de se sortear aleatoriamente uma carta vermelha é \\(P(A)=26/52=0,5\\) e a probabilidade de escolher uma carta de figura é \\(P(B)=12/52=0,23\\). Se nosso interesse é agora determinar a probabilidade de um evento definido como uma vermelha e de figura ao mesmo tempo ou seja, uma carta que está simultaneamente nos conjuntos \\(A\\) e \\(B\\), estamos então interessados na probabilidade da interseção (conectivo \\(\\cap\\)) desses dois eventos: \\(P(A \\cap B)\\). A interseção acaba impondo restrições no espaço amostral inicial (todas as 52 cartas do baralho): olhando-se por um prisma vemos que o espaço amostral agora é reduzido para \\(A\\) (apenas as 26 cartas vermelhas) e, dentro desse novo espaço amostral, estamos interessados nos elementos \\(B\\) (apenas cartas de figura); do mesmo modo se olharmos por outro prisma, quando o espaço amostral agora é reduzido para \\(B\\) (apenas as 12 cartas de figuras) e, dentro desse novo espaço amostral, estamos interessados nos elementos \\(A\\) (apenas cartas vermelhas); A probabilidade de um evento \\(A\\) condicionada à ocorrência prévia de um outro evento \\(B\\), pode ser entendida como a fração de \\(A\\) dentro de \\(B\\), ou seja: \\[ P(\\text{ocorreu A, ocorrer B}) = \\frac{ P(A \\cap B)}{ P(A)}\\\\ P(\\text{foi sorteada uma carta vermelha, sortear-se uma figura})= \\frac{6}{12}=0,50 \\] \\[ P(\\text{ocorreu B, ocorrer A}) = \\frac{ P(B\\cap A)}{ P(B)}\\\\ P(\\text{foi sorteada uma figura, sortear-se uma carta vermelha})= \\frac{6}{12}=0,50 \\] Reescrevendo-se \\[ P(A \\cap B) = P(\\text{ocorreu A, ocorrer B}) \\times P(A)\\\\ P(B \\cap A) = P(\\text{ocorreu B, ocorrer A}) \\times P(B)\\\\ P(A \\cap B)=P(B \\cap A)\\\\ \\] Dois eventos \\(A\\) e \\(B\\) definidos sobre um experimento aleatório qualquer são ditos condicionados quando a ocorrência prévia de um deles impõe uma restrição no espaço amostral do segundo. A probabilidade de um evento qualquer \\(A\\) condicionada a um segundo evento \\(B\\) é representada como \\(P(A|B)\\). A barra vertical pode ser “lida” adotando-se termos correlatos que facilitam o entendimento da relação existente, tais como : probabilidade de \\(A\\) posto que ocorreu \\(B\\); probabilidade de \\(A\\) admitindo-se que ocorreu \\(B\\); probabilidade de \\(A\\) considerando-se que ocorreu \\(B\\), A regra geral da probabilidade de dois eventos condicionados estabelece que: \\[\\begin{align*} P(A|B) &amp; = \\frac{ P(A\\cap B)}{ P(B)} \\\\ P(B|A) &amp; = \\frac{ P(B\\cap A)}{ P(A)} \\end{align*}\\] sendo \\(P(B)&gt;0\\) e \\(P(A)&gt;0\\) nas expressões acima. Exemplo: Consideremos a Tabela 4.3 que apresenta informações cruzadas do sexo dos alunos e seus respectivos cursos. Vamos definir os eventos Fem:sexo feminino e Est: cursar estatística. Como calcular a probabilidade condicionada de nosso evento de interesse P(Fem|Est) (a probabilidade de um aluno aleatoriamente escolhido ser do sexo feminino, dado que ele cursa estatística)? \\[\\begin{align*} P(Fem|Est) &amp; = \\frac{ P(Fem \\cap Est)}{ P(Est)} \\\\ &amp; = \\frac{20}{30} = \\frac{2}{3} \\end{align*}\\] Esse cálculo é facilmente entendido observando-se as celulas da distribuição de frequências na Tabela 4.3. Exemplo: Considerem a Tabela 4.4 que relaciona a ida à praia de uma certa pessoa às condições climáticas do dia. Table 4.4: Condicionamento de passeios à praia em relação às condições climáticas observadas Dia 1 2 3 4 5 6 7 8 9 10 Foi à praia? N S N S S S N N S S Fez sol? N S N S N S S N S S Baseado nos dados coletados responda: 1- Qual a probabilidade dessa pessoa ir à praia? 2- Sabendo-se que fez Sol, qual a probabilidade dessa pessoa ir à praia? 3- Os eventos ir à praia e fazer Sol são independentes ou condicionados? Da Tabela 4.4 extraímos as seguintes probabilidades: \\[\\begin{align*} P(IP) &amp; = \\frac{6}{10}= 0,60 \\\\ P(FS) &amp; = \\frac{6}{10}= 0,60 \\\\ P(IP \\cap FS) &amp; = \\frac{5}{10} \\\\ &amp; = 0,50 \\end{align*}\\] A partir delas podemos calcular a seguinte probabilidade condicionada: \\[\\begin{align*} P(IP|FS) &amp; = \\frac{ P(IP \\cap F)}{ P(FS)} \\\\ &amp; = \\frac{5}{6} \\\\ &amp; = 0,83 \\end{align*}\\] A probabilidade dessa pessoa ir à praia (\\(P(IP)\\)) é 0,60; mas quando faz Sol a probabilidade (\\(P(IP|FS)\\)) dela aumenta para 0,83. Assim, os eventos \\(IP\\) e \\(FS\\) são condicionados: essa pessoa vai à praia 60% dos dias analisados; mas, quando faz sol, ela vai em 83% dos dias (a presença de Sol altera a probabilidade dela ir à praia). Exemplo: Em uma cidade existem 15.000 usuários de telefonia, dos quais 10.000 possuem telefones fixos, 8.000 telefones móveis e 3.000 telefones fixos e móveis. Seja o experimento aleatório de uma operadora de telefone móvel selecionar uma pessoa dessa cidade para oferecer uma promoção do tipo “Fale Grátis de seu Móvel para seu Fixo”. Responda: 1- Sorteando-se aleatoriamente um cliente dessa operadora, se soubermos antecipadamente que ele tem telefone móvel, qual a probabilidade de esse cliente tenha telefone fixo também? 2- Sabendo-se que ele tem telefone fixo, qual a probabilidade de ele tenha telefone móvel também? O espaço amostral de todos esses possíveis eventos pode ser ilustrado pelo diagrama de Venn abaixo: Figure 4.15: Diagrama de Venn do espaço amostral Do diagrama apresentado na Figura4.15 podemos extrair imediatamente as probabilidades pedidas: \\(P(F|M)\\) (probabilidade de ter uma linha fixa sabendo que possui um telefone móvel); e, \\(P(M|F)\\) (probabilidade de ter uma linha móvel sabendo que possui um telefone fixo): \\[\\begin{align*} P(F|M) &amp; = \\frac{n(MF)}{n(M)}\\\\ &amp; =\\frac{3000}{8000}\\\\ &amp; = 0,375 \\end{align*}\\] e \\[\\begin{align*} P(M|F) &amp; = \\frac{n(MF)}{n(F)} \\\\ &amp; =\\frac{3000}{10000} \\\\ &amp; = 0,300 \\end{align*}\\] Mas também podemos calcular as probablidades do modo como explicado no começo desta sessão. Definindo-se os eventos \\(F:\\) telefone fixo e \\(M:\\) telefone móvel, a primeira pergunta pede \\(P(F|M)\\):probabilidade de ter um telefone fixo sabendo que ele tem um telefone móvel: \\[\\begin{align*} P(F|M) &amp; = \\frac{P(F \\cap M)}{P(M)} \\\\ &amp; = \\frac{ \\frac{3000}{15000} }{\\frac{8000}{15000} }\\\\ &amp; = 0,375. \\end{align*}\\] A segunda pede \\(P(M|F)\\): probabilidade de ter um telefone móvel sabendo que ele tem um telefone fixo: \\[\\begin{align*} P(M|F) &amp; = \\frac{P(M \\cap F)}{P(F)} \\\\ &amp; = \\frac{ \\frac{3000}{15000} }{\\frac{10000}{15000} } \\\\ &amp; = 0,300 \\end{align*}\\] Exemplo: Considere a Tabela 4.5 onde são expostos os resultados de uma pesquisa relacionada ao gosto pela prática de tênis entre alunos e alunas. Definindo-se os eventos \\(A\\):“gostar de tênis” e \\(B\\):“ser do sexo feminino”, calcule as probabilidade pedidas ao se sortear, aleatoriamente, uma das pessoas pesquisadas. 1- Qual a probabilidade de que goste de tênis (\\(P(T)\\))? 2- Qual probabilidade de que não goste de tênis (\\(P(T^{c})\\))? 3- Qual a probabilidade de que seja do sexo feminino ou goste de tênis: (\\(P(F \\cup T)\\))? 4- Sabendo-se que foi sorteada uma aluna, qual a probabilidade de que goste de tênis (\\(P(T|F))\\)? 5- Verifique se os eventos \\(T\\): “gostar de tênis” e \\(F\\):“ser do sexo feminino” são condicionados ou independentes (\\(P(T \\cap F) \\stackrel{?}{=} P(T) \\times P(F)\\))) Table 4.5: Distribuição da quantidade de alunos segundo seu sexo e a preferência por tênis Sexo Curso Masculino (M) Feminino (F) Total Gostam de tênis (T) 400 200 600 Não gostam de tênis (NT) 50 50 100 Total 450 250 700 Exemplo: Imagine que um jogador está treinando cobranças de pênaltis. Historicamente a probabilidade de acertar uma cobrança, supondo que acertou a anterior é de 60%. Mas, se ele tiver errado a anterior a probabilidade de acertar cai para 30%. Construa a distribuição de probabilidades do número de acertos em 3 tentativas de cobrança. Figure 4.16: Diagrama em árvore das três repetições dependentes de um pênalti A seguir vemos a cadeia de eventos necessária para que cada contagem de gols se verifique: \\[\\begin{align*} \\text{0 GOL} &amp; = [E_{1} \\cap E_{2} \\cap E_{3}] \\\\ \\text{1 GOL} &amp; = \\{ [A_{1} \\cap E_{2} \\cap E_{3}] \\cup [E_{1} \\cap A_{2} \\cap E_{3}] \\cup [E_{1} \\cap E_{2} \\cap A_{3}] \\}\\\\ \\text{2 GOLS} &amp; = \\{ [A_{1} \\cap A_{2} \\cap E_{3}] \\cup [E_{1} \\cap A_{2} \\cap A_{3}] \\cup [A_{1} \\cap E_{2} \\cap A_{3}] \\}\\\\ \\text{3 GOLS} &amp; = [A_{1} \\cap A_{2} \\cap A_{3}] \\\\ \\end{align*}\\] A probabilidade associada a cada contagem de gols será: \\[\\begin{align*} P(\\text{0 GOL}) &amp; = P[E_{1} \\cap E_{2} \\cap E_{3}] \\\\ P(\\text{1 GOL}) &amp; = P\\{ [A_{1} \\cap E_{2} \\cap E_{3}] \\cup [E_{1} \\cap A_{2} \\cap E_{3}] \\cup [E_{1} \\cap E_{2} \\cap A_{3}] \\}\\\\ P(\\text{2 GOLS}) &amp; = P\\{ [A_{1} \\cap A_{2} \\cap E_{3}] \\cup [E_{1} \\cap A_{2} \\cap A_{3}] \\cup [A_{1} \\cap E_{2} \\cap A_{3}] \\}\\\\ P(\\text{3 GOLS}) &amp; = P[A_{1} \\cap A_{2} \\cap A_{3}] \\\\ \\end{align*}\\] A partir do enunciado, podemos deduzir as probabilidades de cada um dos eventos: \\[\\begin{align*} P(A_{1}) &amp; = 0,50 \\\\ P(E_{1}) &amp; = 0,50\\\\ P(A_{i+1}|A_{i}) &amp; =0,60 \\text{ logo, pelo complementar, } P(E_{i+1}|A_{i})=0,40\\\\ P(A_{i+1}|E_{i}) &amp; =0,30 \\text{ logo, pelo complementar } P(E_{i+1}|E_{i})=0,70\\\\ \\end{align*}\\] \\[\\begin{align*} P(\\text{0 GOL}) &amp; = P[E_{1} \\cap E_{2} \\cap E_{3}] \\\\ P(\\text{0 GOL}) &amp; = P[E_{1}] \\times P[E_{2}|E_{1}] \\times P[E_{3}|E_{2}] \\\\ P(\\text{0 GOL}) &amp; = 0,50 \\times 0,70 \\times 0,70 \\\\ P(\\text{0 GOL}) &amp; = 0,245 \\end{align*}\\] \\[\\begin{align*} P(\\text{3 GOLS}) &amp; = P[A_{1} \\cap A_{2} \\cap A_{3}] \\\\ P(\\text{3 GOLS}) &amp; = P[A_{1}] \\times P[A_{2}|A_{1}] \\times P[A_{3}|A_{2}] \\\\ P(\\text{3 GOLS}) &amp; = 0,50 \\times 0,60 \\times 0,60 \\\\ P(\\text{3 GOLS}) &amp; = 0,18 \\end{align*}\\] \\[\\begin{align*} P(\\text{1 GOL}) &amp; = P\\{ [A_{1} \\cap E_{2} \\cap E_{3}]\\} + P\\{ [E_{1} \\cap A_{2} \\cap E_{3}]\\} + P\\{[E_{1} \\cap E_{2} \\cap A_{3}]\\}\\\\ P(\\text{1 GOL}) &amp; = P[A_{1}] \\times P[E_{2}|A_{1}] \\times P[E_{3}|E_{2}] +\\\\ &amp; P[E_{1}] \\times P[A_{2}|E_{1}] \\times P[E_{3}|A_{2}] +\\\\ &amp; P[E_{1}] \\times P[E_{2}|E_{1}] \\times P[A_{3}|E_{2}]\\\\ P(\\text{1 GOL}) &amp; = 0,50 \\times 0,40 \\times 0,70 +\\\\ &amp; 0,50 \\times 0,30 \\times 0,40 +\\\\ &amp; 0,50 \\times 0,70 \\times 0,30 \\\\ P(\\text{1 GOL}) &amp; =0,305 \\end{align*}\\] \\[\\begin{align*} P(\\text{2 GOLS}) &amp; = P\\{ [A_{1} \\cap A_{2} \\cap E_{3}]\\} + P\\{ [E_{1} \\cap A_{2} \\cap A_{3}]\\} + P\\{[A_{1} \\cap E_{2} \\cap A_{3}]\\}\\\\ P(\\text{2 GOLS}) &amp; = P[A_{1}] \\times P[A_{2}|A_{1}] \\times P[E_{3}|A_{2}] +\\\\ &amp; P[E_{1}] \\times P[A_{2}|E_{1}] \\times P[A_{3}|A_{2}] +\\\\ &amp; P[A_{1}] \\times P[E_{2}|A_{1}] \\times P[A_{3}|E_{2}]\\\\ P(\\text{2 GOLS}) &amp; = 0,50 \\times 0,60 \\times 0,40 +\\\\ &amp; 0,50 \\times 0,30 \\times 0,60 +\\\\ &amp; 0,50 \\times 0,40 \\times 0,30 \\\\ P(\\text{2 GOLS}) &amp; =0,27 \\end{align*}\\] "],["dependência-e-independência-de-eventos.html", "4.5 Dependência e independência de eventos", " 4.5 Dependência e independência de eventos Pela regra geral da probabilidade de dois eventos eventos condicionados: \\[\\begin{align*} P(A|B) &amp; = \\frac{ P(A\\cap B)}{ P(B)} \\\\ P(B|A) &amp; = \\frac{ P(B\\cap A)}{ P(A)} \\end{align*}\\] Como a probabilidade de interseção não se altera (\\(P(A\\cap B)=P(B\\cap A)\\)), podemos reescrever essas duas expressões: \\[\\begin{align*} P(A \\cap B) &amp; = P(A|B) \\times P(B) \\\\ P(A\\cap B) &amp; = P(B|A) \\times P(A) \\end{align*}\\] com \\(P(B)&gt;0\\) e \\(P(A)&gt;0\\) nas expressões acima. Se os eventos \\(A\\) e \\(B\\) são guardam nenhuma relação de condicionamento eles são chamadas de eventos independentes. Equivale dizer que \\(P(A|B)=P(A)\\) (ou \\(P(B|A)=P(B)\\)), a probabilidade de \\(A\\) não se altera pela prévia ocorrência de \\(B\\) (ou a de \\(B\\) pelo de \\(A\\)). Portanto, dois eventos são denominados independentes se, e somente se: \\[ P (A \\cap B)= P(A) \\times P(B) \\] Independência e correlação: se duas variáveis aleatórias são independentes não há associação de natureza alguma entre elas, inclusive a linear, um caso particular de correlação. Todavia uma correlação linear nula não implica em independência posto existirem várias outras formas outras de relacionamento (quadrática, cúbica, ). Figure 4.17: Independência implica em ausência de qualquer tipo de associação (a recíproca não se aplica 4.5.1 Demonstração clássica de independência Uma bolsa contém 5 bolas vermelhas e 5 azuis. Nós removemos uma bola aleatória da bolsa, registramos sua cor e a colocamos de volta na sacola. Em seguida, removemos outra bola aleatória da bolsa e registramos sua cor. Qual é a probabilidade de a primeira bola ser vermelha ? Qual é a probabilidade de a segunda bola ser azul? Qual é a probabilidade de a primeira bola ser vermelha e a segunda bola azul? A primeira bola retirada foi uma bola vermelha e a segunda bola azul; esses eventos foram independentes ? Solução: Probabilidade em se retirar uma bola vermelha em primeiro lugar: Há 10 bolas das quais 5 são vermelhas . A probabilidade de se retirar uma bola vermelha será: \\[ P(1^{a} vermelha)= \\frac{5}{10}= \\frac{1}{2} \\] Probabilidade em se retirar uma bola azul em segundo lugar: O enunciado do experimento assegura que após a retirada da primeira bola ela é devolvida ao sacola; por essa razão, ao se retirar a segunda bola, há novamente 10 bolas no total, das quais 5 são azuis. A probabilidade de se retirar uma bola azul será: \\[ P(2^{a} azul)= \\frac{5}{10}= \\frac{1}{2} \\] Probabilidade da primeira bola retirada ser vermelha e a segunda ser azul: Ao se retirar duas bolas do sacola há quatro possíveis combinações de resultados. Nós podemos obter: 1- uma vermelha e depois outra vermelha; 2- uma vermelha e depois uma azul; 3- uma azul e depois uma vermelha; ou, 4- uma azul e depois outra azul; Queremos saber a probabilidade do segundo resultado após termos obtido uma bola vermelha na primeira seleção. Como existem 5 bolas vermelhas e 10 bolas no total, existem \\(\\frac{5}{10}\\) possibilidades de obter uma bola vermelha primeiro. Agora nós colocamos a primeira bola de volta, então há novamente 5 bolas vermelhas e 5 bolas azuis na sacola. Portanto, há \\(\\frac{5}{10}\\) possibilidades de obter uma segunda bola azul se a primeira bola for vermelha . Isso significa que existem: \\(\\frac{5}{10} \\times \\frac{5}{10}= \\frac{25}{100}\\) possibilidades de se obter uma bola vermelha em primeiro lugar e uma bola azul em segundo. Então, a probabilidade associada será de \\(\\frac{1}{4}\\). A primeira bola retirada foi uma bola vermelha e a segunda bola azul. Esses dois eventos são independentes? Esses eventos serão independentes se, e somente se: \\[ P (A \\cap B)= P(A) \\times P(B) \\] \\[\\begin{align*} P(1^{a} vermelha) &amp; = \\frac{5}{10}= \\frac{1}{2}\\\\ P(2^{a} azul) &amp; = \\frac{5}{10}= \\frac{1}{2}\\\\ P(1^{a} vermelha,2^{a} azul) &amp; = \\frac{25}{100} = \\frac{1}{4}\\\\ \\end{align*}\\] Como \\(\\frac{1}{4}=\\frac{1}{2} \\times \\frac{1}{2}\\), os eventos são independentes. Figure 4.18: Ilustração do experimento aleatório sob a condição de reposição 4.5.2 Demonstração clássica de dependência E se, ao retirarmos a primeira bola, não a devolvêssemos ao sacola? Admitamos agora que o enunciado de nosso problema passou a ser: Uma bolsa contém 5 bolas vermelhas e 5 azuis. Nós removemos uma bola aleatória da bolsa, registramos sua cor e não a colocamos de volta na sacola. Em seguida, removemos outra bola aleatória da bolsa e registramos sua cor. 1- Qual é a probabilidade de a primeira bola ser vermelha ? 2- Qual é a probabilidade de a segunda bola ser azul? 3- Qual é a probabilidade de a primeira bola ser vermelha e a segunda bola azul? 4- A primeira bola retirada foi uma bola vermelha e a segunda bola azul; esses eventos foram independentes ? Solução: \\(1^{a}\\) Etapa: analisar todos os possíveis resultados Probabilidade da primeira bola retirada ser vermelha e a segunda ser azul: Ao se retirar duas bolas do sacola há quatro possíveis combinações de resultados. Nós podemos obter: uma vermelha e depois outra vermelha; uma vermelha e depois uma azul; uma azul e depois uma vermelha ; ou, uma azul e depois outra azul. Queremos saber a probabilidade do segundo resultado após termos obtido uma bola vermelha na primeira seleção. Como existem 5 bolas vermelhas e 10 bolas no total, existem \\(\\frac{5}{10}\\) maneiras de obter uma bola vermelha primeiro. Entretanto, nessa nova situação, nós não colocamos a primeira bola de volta, então haverá apenas 4 bolas vermelhas e 5 bolas azuis na sacola. Haverá \\(\\frac{4}{9}\\) maneiras de obter uma segunda bola vermelha se a primeira bola for vermelha . Isso significa que existem: \\(\\frac{5}{10} \\times \\frac{4}{9}= \\frac{20}{90}\\) maneiras de se obter uma bola vermelha em primeiro lugar e uma bola vermelha em segundo. Então, a probabilidade associada será de \\(\\frac{2}{9}\\); Haverá \\(\\frac{5}{9}\\) maneiras de obter uma segunda bola azul se a primeira bola for vermelha . Isso significa que existem: \\(\\frac{5}{10} \\times \\frac{5}{9}= \\frac{25}{90}\\) maneiras de se obter uma bola vermelha em primeiro lugar e uma bola azul em segundo. Então, a probabilidade associada será de \\(\\frac{5}{18}\\); Haverá \\(\\frac{5}{9}\\) maneiras de obter uma segunda bola vermelha se a primeira bola for azul. Isso significa que existem: \\(\\frac{5}{10} \\times \\frac{5}{9}= \\frac{25}{90}\\) maneiras de se obter uma bola azul em primeiro lugar e uma bola vermelha em segundo. Então, a probabilidade associada será de \\(\\frac{5}{18}\\). Haverá \\(\\frac{4}{9}\\) maneiras de obter uma segunda bola azul se a primeira bola for azul. Isso significa que existem: \\(\\frac{5}{10} \\times \\frac{4}{9}= \\frac{20}{90}\\) maneiras de se obter uma bola azul em primeiro lugar e uma bola azul em segundo. Então, a probabilidade associada será de \\(\\frac{2}{9}\\); Resumo das probabilidades calculadas: 1 -uma vermelha e depois outra vermelha : \\(\\frac{2}{9}\\); 2- uma vermelha e depois uma azul: \\(\\frac{5}{18}\\); 3- uma azul e depois uma vermelha : \\(\\frac{5}{18}\\); e, 4- uma azul e depois outra azul: \\(\\frac{2}{9}\\). \\(2^{a}\\) Etapa: analisar a possibilidade de se obter uma bola vermelha na primeira extração: uma vermelha e depois outra vermelha : \\(\\frac{2}{9}\\); uma vermelha e depois uma azul: \\(\\frac{5}{18}\\). A probabilidade total de se obter uma bola vermelha na primeira extração será: \\[ P(1^{a} vermelha)= \\frac{2}{9} + \\frac{5}{18} = \\frac{1}{2} \\] \\(3^{a}\\) Etapa: analisar a possibilidade de se obter uma bola azul na segunda extração: uma vermelha e depois uma azul: \\(\\frac{5}{18}\\); uma azul e depois outra azul: \\(\\frac{2}{9}\\). A probabilidade total de se obter uma bola azul na segunda extração será: \\(P(2^{a} azul)= \\frac{5}{18} + \\frac{2}{9} = \\frac{1}{2}\\) \\(4^{a}\\) Etapa: analisar a possibilidade de se obter uma bola vermelha e em seguida azul: uma vermelha e depois outra azul: \\(\\frac{5}{18}\\); \\(5^{a}\\) Etapa: Esses dois eventos são independentes? Esses eventos serão independentes se, e somente se: \\[ P (A \\cap B)= P(A) \\times P(B) \\] \\[\\begin{align*} P(1^{a} vermelha) &amp; = \\frac{2}{9} + \\frac{5}{18} = \\frac{1}{2} \\\\ P(2^{a} azul) &amp; = \\frac{5}{18} + \\frac{2}{9} = \\frac{1}{2} \\\\ P(1^{a} vermelha,2^{a} azul) &amp; = \\frac{5}{18} \\\\ \\end{align*}\\] Como \\(\\frac{5}{18} \\neq \\frac{1}{2} \\times \\frac{1}{2}\\), os eventos não são independentes. Figure 4.19: Ilustração do experimento aleatório sob a condição de não reposição "],["probabilidade-de-eventos-independentes-regra-da-cadeia.html", "4.6 Probabilidade de eventos independentes (regra da cadeia)", " 4.6 Probabilidade de eventos independentes (regra da cadeia) Se \\(E_{1}\\), \\(E_{2}\\), …, \\(E_{n}\\) são eventos independentes entre si, então: Para que isso se verifique, a independência entre cada um e todos os eventos deve se verificada. Numa situação de três eventos, por exemplo, teríamos que observar: \\[ P (E_{1} \\cap E_{2})= P(E_{1}) \\times P(E_{2}) \\] \\[ P (E_{1} \\cap E_{3})= P(E_{1}) \\times P(E_{3}) \\] \\[ P (E_{2} \\cap E_{3})= P(E_{2}) \\times P(E_{3}) \\] \\[ P (E_{1} \\cap E_{2} \\cap E_{3} )= P(E_{1}) \\times P(E_{2}) \\times P(E_{3}) \\] Exemplo: considere o experimento aleatório de se lançar dois dados e obter o valor 1 no primeiro deles e 5 no segundo (defina os eventos \\(E_{1}= \\text{sair face 1}\\) e \\(E_{5}=\\text{sair face 5}\\)). Solução: Quando lançamos dois dados o resultado obtido em um deles (o valor numérico da face) não condiciona ou altera o resultado obtido no outro: os resultados são são independentes. Desse modo, sendo \\(P(E_{1})=\\frac{1}{6}\\) e \\(P(E_{5})=\\frac{1}{6}\\): \\[\\begin{align*} P(E_{1} \\cap E_{5}) &amp; = \\frac{1}{6} \\times \\frac{1}{6}\\\\ &amp; = \\frac{1}{36}. \\end{align*}\\] Exemplo: Uma empresa que compra produtos de dois fabricantes diferentes (Fabricante 1 e Fabricante 2}) adquiriu 168 unidades do primeiro e 84 do segundo. Sabendo que 8 unidades fabricadas pelo primeiro fornecedor não atenderam às especificações e apenas 4 do segundo, verifique se o fato de uma amostra ter atendido às especificações independe de ter sido produzida pelo Fabricante 1. Solução: Para a primeira verificação pedida defina os eventos \\(Fab1:\\) ter sido produzida pelo Fabricante 1, \\(Aprov:\\) ter atendido às especificações e \\(Fab2:\\) ter sido produzida pelo Fabricante 2. Na sequência podemos calcular as seguintes probabilidades: \\[\\begin{align*} P(Fab1) &amp; = \\frac{168}{252} \\\\ &amp; = 0,6666 \\\\ P(Aprov) &amp; = \\frac{240}{252} \\\\ &amp; = 0,9523 \\\\ P(Fab1 \\cap Aprov) &amp; = \\frac{160}{252} \\\\ &amp; = 0,6349 \\end{align*}\\] Se o fato de uma amostra ter sido aprovada independe de ter sido produzida pelo Fabricante 1 então \\(P(Aprov|Fab1) = P(Aprov)\\): \\[\\begin{align*} P(Aprov|Fab1) &amp; = \\frac{P(Aprov \\cap Fab1)}{P(Fab1)} \\\\ &amp; = \\frac{0,6349}{0,6666} \\\\ &amp; = 0,9523. \\end{align*}\\] Como \\(P(Aprov|Fab1) = P(Aprov)\\), verifica-se que o fato de uma amostra aleatoriamente sorteada entre as peças do fabricante 1 não condiciona sua aprovação. Exemplo: A probabilidade de um consumidor (\\(C_{1}\\)) ficar satisfeito com o desempenho de certa marca de produto é de 25%. A probabilidade de um outro consumidor (\\(C_{2}\\)) ficar satisfeito com a mesma marca é de 40%. Admitamos que os dois consumidores irão consumir o produto num mesmo momento e de forma independente (incomunicáveis). Qual a probabilidade de os dois consumidores ficarem satisfeitos simultaneamente? Solução: As probabilidades individuais dos consumidores 1 e 2 ficarem satisfeitos com o desempenho da marca do produto são: \\[\\begin{align*} P(C_{1}) &amp; = 0,25\\\\ P(C_{2}) &amp; = 0,40 \\end{align*}\\] A probabilidade de ambos ficarem satisfeitos, dado que o enunciado afirma que esses eventos são independente será: \\[\\begin{align*} P(C_{1} \\cap C_{2}) &amp; = 0,25 \\times 0,40\\\\ &amp; = 0,10. \\end{align*}\\] "],["teorema-de-bayes.html", "4.7 Teorema de Bayes", " 4.7 Teorema de Bayes Figure 4.20: Thomas Bayes (1702 - 1761) Admita o espaço amostral de um experimento baseado no sorteio aleatório de um estudante de uma escola, com dois possíveis resultados quanto ao sexo: Figure 4.21: Espaço amostral Considere agora um evento definido nesse espaço amostral como sendo ``ter um carro’’: Figure 4.22: Espaço amostral As interseções desse evento com os elementos do espaço amostral são: Figure 4.23: Espaço amostral Pela regra da probabilidade condicionada temos que \\[ P(C|F) = \\frac{P(C \\cap F)}{ P(F)}\\\\ P(C \\cap F) = P(C|F) P(F) \\] e, de modo equivalente, \\[ P(C|M) = \\frac{P(C \\cap M)}{ P(M)}\\\\ P(C \\cap M) = P(C|M) P(M) \\] A probabilidade de se ter um carro é dada pela regra da união de eventos que, nesse caso são disjuntos e assim: \\[ P(C) = P(C \\cap M) \\cup P(C \\cap F)\\\\ P(C) = P(C|M) P(M) + P(C|F) P(F) \\] Sorteado aleatoriamente um estudante da escola verificou-se possuir um carro. Qual a probabilidade de que seja do sexo feminino (\\(P(F|C)\\))? \\[ P(F|C) = \\frac{P(F \\cap C)}{ P(C)}\\\\ P(F \\cap C) = P(F|C) P(C) \\\\ \\] Pela igualdade \\(P(C \\cap F)=P(F \\cap C)\\): \\[ P(C \\cap F) = P(C|F) P(F) \\\\ P(F \\cap C) = P(F|C) P(C) \\] substituindo-se na expressão acima chega-se a: \\[\\begin{align*} P(C \\cap F) &amp; =P(F \\cap C)\\\\ P(C|F).P(F) &amp; = P(F|C).P(C)\\\\ P(F|C) &amp; = \\frac{P(C|F)P(F)}{P(C)} \\end{align*}\\] uma relação entre duas probabilidades inversamente condicionadas conhecida como Teorema de Bayes. Adimita então serem dados: ``M’’: ser do sexo masculino: \\(P(M)=0,65\\); ``F’’: ser do sexo feminino: \\(P(F)=0,35\\). ``C’’: possuir um carro: \\(P(C|M)=0,30\\) \\(P(C|F)=0,18\\). A probabilidade de se ter carro (\\(P(C)\\)) resulta de união de dois únicos e possíveis eventos condicionados ao sexo e disjuntos. Assim: \\[\\begin{align*} P(C) &amp; = P(C \\cap M) \\cup P(C \\cap F)\\\\ P(C) &amp; = [P(M).P(C|M)] \\cup [P(F).P(C|F)] \\\\ P(C) &amp; = [0,65 . 0,30] + [0,35 . 0,18] \\\\ P(C) &amp; = 0,258\\\\ \\end{align*}\\] e podemos calcular \\(P(F|C)\\): \\[\\begin{align*} P(F|C) &amp; = \\frac{P(F).P(C|F)}{P(C)}\\\\ P(F|C) &amp; = \\frac{0,35 . 0,18 }{ 0,258}\\\\ P(F|C) &amp; = 0,2442\\\\ \\end{align*}\\] A probabilidade de que um estudante aleatoriamente sorteado nessa escola e sabendo-se a priori que possui um carro ser do sexo feminino é de 24,42%. Para um espaço amostral mais amplo, de modo geral consideremos, inicialmente o diagrama da Figura 4.24 onde \\(\\Omega\\) é o espaço amostral de um experimento aleatório qualquer: Figure 4.24: Espaço amostral Admita que \\(E_{1}\\), \\(E_{2}\\), \\(E_{3}\\) e \\(E_{4}\\) formem a partição do espaço amostral \\(\\Omega\\) (seus elementos são mutuamente exclusivos) como exposto na Figura 4.25 Figure 4.25: Espaço amostral e suas partições E seja \\(B\\) um evento qualquer em \\(\\Omega\\) como ilustrado na Figura 4.26 Figure 4.26: Evento definido sobre o espaço amostral Delimitemos as interseções do evento \\(B\\) com as partições \\(E_{1}\\), \\(E_{2}\\), \\(E_{3}\\) e \\(E_{4}\\) do espaço amostral \\(\\Omega\\), como ilustrado na Figura 4.27 Figure 4.27: Interseções das partições do espaço amostral com o evento B Isso pode ser estendido, em uma forma geral, para \\(i=1, \\dots, n\\) partições como ilustrado na Figura 4.28 Figure 4.28: Interseções das n partições do espaço amostral com o evento B Na representação esquemática da Figura 4.28 podemos identificar: 1- \\(E_{1}\\), \\(E_{2}\\), , \\(E_{i}\\), , \\(E_{n}\\) constituem-se em partições do espaço amostral \\(\\Omega\\); 2- Todas as partições são mutuamente exclusivas: \\(E_{i} \\cap E_{j} = \\varnothing\\),\\(\\forall\\) \\(i \\neq j\\) (a interseção de quaisquer partições é vazia); 3- Sendo vazias as interseções entre quaisquer partições, o espaço amostral \\(\\Omega\\) será a simples união de todas elas: \\(\\Omega = E_{1} \\cup E_{2} \\cup E_{3} \\cup E_{4}\\cup \\dots \\cup E_{i} \\dots \\cup E_{n}\\); e, 4- B é um evento qualquer definido sobre as partições de \\(\\Omega\\) São conhecidas as probabilidades de ocorrência de cada um dos elementos do espaço amostral \\(\\Omega\\): \\[ P(E_{1}); P(E_{2}); P(E_{3}); \\dots;P(E_{i}); \\dots; P(E_{n}) \\] e também as probabilidades do evento \\(B\\) condicionadas a cada elemento do espaço amostral: \\[ P(B|E_{1}); P(B|E_{2});\\dots;P(B|E_{i});\\dots; P(B|E_{n}) \\] A probabilidade de ocorrência do evento B é dada pela soma das probabilidades de cada uma de suas interseções com os elementos do espaço amostral \\(\\Omega\\), uma vez que essas interseções são disjuntas entre si: \\[\\begin{align*} P(B) &amp; = P(E_{1} \\cap B) \\cup P(E_{2} \\cap B) \\cup \\dots P(E_{i} \\cap B) \\cup \\dots P(E_{n} \\cap B) \\\\ P(B) &amp; = \\sum _{i=1}^{n}P\\left({E}_{i}\\cap B\\right) \\end{align*}\\] Pela Regra do produto de eventos condicionados, a probabilidade de ocorrência do evento B posto ter ocorrido um evento \\(E_{i}\\) é: \\[\\begin{align*} P(B|E_{i}) &amp; = \\frac{P(E_{i}\\cap B)}{P(E_{i})} \\\\ P(E_{i}\\cap B) &amp; = P(E_{i}) \\times P(B|E_{i}) \\end{align*}\\] com \\(P(E) &gt; 0\\) Aplicando-se na expressão anteriormente desenvolvida da probabilidade de ocorrência do evento B teremos: \\[\\begin{align*} P(B) &amp; = P(E_{1} \\cap B) \\cup P(E_{2} \\cap B) \\cup \\dots \\cup P(E_{i} \\cap B) \\cup \\dots \\cup P(E_{n} \\cap B) \\\\ P(B) &amp; = P(E_{1}) \\times P(B|E_{1}) + P(E_{2}) \\times P(B|E_{2}) + \\\\ &amp; \\dots +P(E_{i}) \\times P(B|E_{i}) + \\\\ &amp; \\dots + P(E_{n}) \\times P(B|E_{n}) \\end{align*}\\] Portanto a probabilidade total do evento B em \\(\\Omega\\) é dada pelo somatório: \\[ P(B) = \\sum _{i=1}^{n}\\left[P\\left({E}_{i}\\right)\\cdot P\\left(B|{E}_{i}\\right)\\right] \\] Pela Regra do produto de eventos condicionados a probabilidade de ocorrência de um evento \\(E_{i}\\) posto ter ocorrido o evento \\(B\\) é: \\[\\begin{align*} P(E_{i}|B) &amp; = \\frac{P(E_{i} \\cap B)}{P(B)} \\\\ P(E_{i} \\cap B) &amp; = P(B) \\times P(E_{k}|B) \\\\ P(B) &amp; = \\frac{P(E_{i}\\cap B)}{P(E_{k}|B)} \\end{align*}\\] com \\(P(B) &gt; 0\\) Pela igualdade dos dois modos de se expressar a probabilidade total do evento \\(B\\) desenvolvidos: \\[ P(B) = \\frac{P(E_{i}\\cap B)}{P(E_{i}|B)} \\] e \\[ P(B) = \\sum _{i=1}^{n}\\left[P\\left({E}_{i}\\right)\\cdot P\\left(B|{E}_{i}\\right)\\right] \\] tem-se \\[ \\frac{P(E_{i}\\cap B)}{P(E_{i}|B)}=\\sum _{i=1}^{n}\\left[P\\left({E}_{i}\\right)\\cdot P\\left(B|{E}_{i}\\right)\\right] \\] Rearranjando-se em termos da expressão anterior para exprimir a probabilidade de ocorrência de um evento \\(E_{i}\\) posto ter ocorrido o evento \\(B\\) chegamos a: \\[ P(E_{i}|B) = \\frac{P(E_{i}\\cap B)}{\\sum _{i=1}^{n}\\left[P\\left({E}_{i}\\right)\\cdot P\\left(B|{E}_{i}\\right)\\right]} \\] Sendo \\[ P(E_{i} \\cap B) = P(B) \\times P(E_{i}|B) \\] a expressão anterior pode ser reescrita como: \\[ P(E_{i}|B) = \\frac{ P(E_{i}) \\times P(B|E{i}) }{ \\sum _{i=1}^{n}\\left[P\\left({E}_{i}\\right)\\times P\\left(B|{E}_{i}\\right)\\right] } \\] uma forma mais geral do Teorema de Bayes. O Teorema de Bayes é também chamado de Teorema da probabilidade a posteriori ao permitir que se calcule \\(P(E_{i}|B)\\) em termos da ocorrência \\(P(B|E_{i})\\) É, de certo modo, uma conjugação do teorema na probabilidade total e da regra do produto de probabilidades. O denominador: \\[ P(B)= \\sum _{i=1}^{n}\\left[P\\left({E}_{i}\\right)\\times P\\left(B|{E}_{i}\\right)\\right] \\] é a denominada probabilidade marginal de ocorrência do evento \\(B\\) no espaço amostral \\(\\Omega\\) composto por \\(n\\) elementos (partições). Na expressão do Teorema de Bayes: \\(P(E_{k}|B)\\) é a denominada probabilidade a posteriori do evento \\(E_{k}\\) condicionada pela ocorrência anterior do evento B; \\(P(E_{k})\\) é a denominada probabilidade a priori do evento \\(E_{k}\\); \\(P(B|E_{k})\\) é a denominada probabilidade a posteriori do evento \\(B\\) condicionada pela ocorrência anterior do evento \\(E_{k}\\); \\(P(E_{i})\\) é a denominada probabilidade a priori de cada evento \\(E_{i}\\); \\(P(B|E_{i})\\) é a denominada probabilidade a posteriori do evento \\(B\\) condicionada pela ocorrência anterior de cada evento \\(E_{i}\\). Exemplo: Constatou-se que o aumento nas vendas de um certo produto comercializado por uma empresa num mês pode ocorrer somente por uma das quatro causas mutuamente exclusivas a seguir: 1- ação de marketing; 2- propaganda; 3- flutuações na economia do país; ou, 4- efeitos sazonais. A probabilidade de haver uma ação da empresa no mês focada para o marketing é de 40%; e para propaganda é de 30%; as probabilidades de ocorrerem flutuações na economia do país é de 20% e de efeitos sazonais é de 10%. Uma pesquisa mostrou que a probabilidade de haver um aumento nas vendas do produto devido a uma ação de marketing é de 7%; devido à publicidade, de 7,5%, por flutuações na economia do país, de 3% e por sazonalidade de 2%. Em um determinado mês a empresa observou um considerável incremento nas vendas. Qual seria sua causa mais provável? Qual a probabilidade de incremento das vendas em um certo mês? Inicialmente definimos um experimento aleatório como sendo “qual fato ocorreu no mês”. Não sabemos qual fato ocorreu, mas sabemos que as possibilidades são apenas 4 (marketing, propaganda, flutuações na economia ou efeitos sazonais). Podemos então conceber que esses fatos são elementos do espaço amostral do experimento aleatório: pois são eventos exaustivos e exclusivos: não pode ocorrer mais de um ao mesmo tempo e ao menos um ocorrerá. Assim esse espaço amostral é composto pelos seguintes “elementos” e suas probabilidaes são tiradas do enunciado: \\(E_{1}\\) o elemento “Ação de marketing” \\(\\therefore \\rightarrow\\) \\(P(E_{1})=0,40\\); \\(E_{2}\\) o elemento “Ação de propaganda” \\(\\therefore \\rightarrow\\) \\(P(E_{2})=0,30\\); \\(E_{3}\\) o elemento “Flutuações na economia” \\(\\therefore \\rightarrow\\) \\(P(E_{3})=0,20\\); ou, \\(E_{4}\\) o elemento “Sazonalidade” \\(\\therefore \\rightarrow\\) \\(P(E_{4})=0,10\\). Chamemos de \\(B\\) ao evento “ocorrer um incremento nas vendas”, um evento construído sobre os elementos do espeço amostral e que apresenta diferentes probabilidades a depender de qual elemento do espaço amostral ocorreu (a probabilidade de \\(B\\) está condicionada aos elementos do espaço amstral). Da leitura do enunciado extraímos as probabilidades de ocorrência de cada um dos eventos influenciadores: As probabilidades condicionadas de ocorrer um incremento das vendas (\\(B\\)) pela ocorrência anterior de cada um dos elementos do espaço amostral (posto ter ocorrido o evento \\(E_{i}\\)) também são tiradas do enunciado: \\(P(B|E_{1}) = 0,07\\) ; \\(P(B|E_{2}) = 0,075\\); \\(P(B|E_{3}) = 0,03\\); e, \\(P(B|E_{4}) = 0,02\\). Para responder à indagação do problema (“Qual a causa mais provável?”) podemos invertê-la e reformulá-la: “Qual a probabilidade de ter ocorrido cada um dos quatro eventos (\\(E_{1}\\), \\(E_{2}\\), \\(E_{3}\\), \\(E_{4}\\)) posto (dado) ter ocorrido um incremento nas vendas? Calculemos para cada um deles usando o Teorema de Bayes: \\[ P(E_{i}|B) = \\frac{ P(E_{i}) \\times P(B|E{i}) }{ \\sum _{i=1}^{n}\\left[P\\left({E}_{i}\\right)\\times P\\left(B|{E}_{i}\\right)\\right] } \\] Probabilidade da empresa ter realizado uma ação de marketing, posto ter ocorrido um incremento nas vendas de seu produto: \\[\\begin{align*} P(E_{1}|B) &amp; = \\frac{ P(E_{1}) \\times P(B|E{1}) }{ \\sum _{i=1}^{4}\\left[P\\left({E}_{i}\\right)\\times P\\left(B|{E}_{i}\\right)\\right] } \\\\ P(E_{1}|B) &amp; = \\frac{0,40 \\times 0,07} { (0,40 \\times 0,07) + (0,30 \\times 0,075) +(0,20 \\times 0,03) +(0,10 \\times 0,02) } \\\\ P(E_{1}|B) &amp; = 0,4786 \\\\ \\end{align*}\\] Probabilidade da empresa ter realizado propaganda, posto ter ocorrido um incremento nas vendas de seu produto: \\[\\begin{align*} P(E_{2}|B) &amp; = \\frac{ P(E_{2}) \\times P(B|E{2}) }{ \\sum _{i=1}^{4}\\left[P\\left({E}_{i}\\right)\\times P\\left(B|{E}_{i}\\right)\\right] } \\\\ P(E_{2}|B) &amp; = \\frac{0,30 \\times 0,075} { (0,40 \\times 0,07) + (0,30 \\times 0,075) +(0,20 \\times 0,03) +(0,10 \\times 0,02) } \\\\ P(E_{2}|B) &amp; = 0,3846 \\end{align*}\\] Probabilidade da empresa ter ocorrido flutuações na economia, posto ter ocorrido um incremento nas vendas de seu produto: \\[\\begin{align*} P(E_{3}|B) &amp; = \\frac{ P(E_{3}) \\times P(B|E{3}) }{ \\sum _{i=1}^{4}\\left[P\\left({E}_{i}\\right)\\times P\\left(B|{E}_{i}\\right)\\right] } \\\\ P(E_{3}|B) &amp; = \\frac{0,20 \\times 0,03} { (0,40 \\times 0,07) + (0,30 \\times 0,075) +(0,20 \\times 0,03) +(0,10 \\times 0,02) } \\\\ P(E_{3}|B) &amp; = 0,1026 \\end{align*}\\] Probabilidade da empresa ter ocorrido efeitos sazonais, posto ter ocorrido um incremento nas vendas de seu produto: \\[\\begin{align*} P(E_{4}|B) &amp; = \\frac{ P(E_{4}) \\times P(B|E{4}) }{ \\sum _{i=1}^{4}\\left[P\\left({E}_{i}\\right)\\times P\\left(B|{E}_{i}\\right)\\right] } \\\\ P(E_{4}|B) &amp; = \\frac{0,10 \\times 0,02} { (0,40 \\times 0,07) + (0,30 \\times 0,075) +(0,20 \\times 0,03) +(0,10 \\times 0,02) } \\\\ P(E_{4}|B) &amp; = 0,03419 \\end{align*}\\] Respostas: 1- Os cálculos indicam que o evento mais provável pelo incremento das vendas observado naquele mês foi o de uma ação de marketing; 2- A probabilidade de incremento das vendas em um determinado mês como resultado dos quatro possíveis eventos indicados é o próprio denominador do Teorema de Bayes: 0,0585. Exemplo: Considere 5 urnas, cada uma delas contendo 6 bolas. Duas dessas urnas (urnas tipo \\(C_{1}\\)) possuem 3 bolas brancas em seu interior. Duas outras (urnas tipo \\(C_{2}\\)) possuem 2 bolas brancas em seu interior e a última (urnas tipo \\(C_{3}\\)) possui 6 bolas brancas em seu interior (cf. Figura 4.29). Figure 4.29: Cinco urnas cada uma com 6 bolas em cores de diferentes quantidades da cor branca Escolhida aleatoriamente uma urna retira-se uma bola. Qual a probabilidade da urna escolhida ter sido a urna \\(C_{3}\\) sabendo-se que a bola retirada foi branca? Desejamos determinar \\(P(C_{3} | Branca)\\) Da leitura do enunciado extraímos as seguintes informações: \\[\\begin{align*} P(C_{1}) &amp; = \\frac{2}{5} \\\\ P(C_{2}) &amp; = \\frac{2}{5} \\\\ P(C_{3}) &amp; = \\frac{1}{5} \\\\ P(Branca | C_{1}) &amp; = \\frac{1}{2} \\\\ P(Branca | C_{2}) &amp; = \\frac{1}{3} \\\\ P(Branca | C_{3}) &amp; = 1 \\end{align*}\\] \\[\\begin{align*} P(C_{3} | Branca) &amp; = \\frac{ P(C_{3}) \\times P(Branca | C_{3}) }{ \\sum_{i=1}^{3}\\left[P\\left({C}_{i}\\right)\\times P\\left(Branca | {C}_{i}\\right)\\right] } \\\\ P(C_{3} | Branca) &amp; = \\frac{ 0,20 \\times 1,00} { (0,40 \\times 0,50 ) + (0,40 \\times 0,33 ) +(0,20 \\times 1,00)} \\\\ P(C_{3} | Branca) &amp; = 0,375 \\end{align*}\\] "],["teoremas-da-teoria-das-probabilidades.html", "4.8 Teoremas da Teoria das probabilidades", " 4.8 Teoremas da Teoria das probabilidades 4.8.1 Teorema 01 Se \\(E\\) é um evento num espaço discreto \\(\\Omega\\), então \\(P(E)\\) é igual à soma das probabilidades de ocorrência de todos os elementos do espaço amostral que satisfazem ao evento de interesse \\(E\\) . Sejam \\(E_{1},E_{2},E_{3},\\dots\\) a sequência finita ou infinita de eventos que satisfazem ao evento de interesse \\(E\\). Assim, \\(E = E_{1} \\cup E_{2} \\cup E_{3}...\\). Como \\(E_{1},E_{2},E_{3},\\dots\\) são eventos mutuamente exclusivos, pelo terceiro postulado das probabilidades teremos: \\[ P(E) = P(E_{1}) + P(E_{2}) + P(E_{3}) + ... \\] Exemplo: Lançamento de uma moeda duas vezes Espaço amostral dos possíveis eventos (resultados): \\(\\Omega = \\{(cara, cara), (cara, coroa), (coroa, cara), (coroa, coroa)\\}\\) Evento de interesse \\(E\\): obter ao menos uma cara Eventos que satisfazem: \\(E_{1} =\\{(cara, cara)\\}\\); \\(E_{2} =\\{(cara, coroa)\\}\\); \\(E_{3} =\\{(coroa, cara)\\}\\) A probabilidade de \\(E\\) (\\(P(E)\\))será a soma das probabilidades dos eventos que o satisfazem: \\[ P(E) = P(E_{1}) + P(E_{2}) + P(E_{3}) = \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} = \\frac{3}{4} \\] 4.8.2 Teorema 02 Se um experimento aleatório pode ter \\(N\\) resultados possíveis e equiprováveis e um evento \\(E\\) pode ter \\(n\\) resultados que o satisfazem, então \\(P(E) = \\frac{n}{N}\\). Sejam \\(E_{1},E_{2},E_{3},\\dots,E_{N}\\) os resultados do espaço amostral \\(\\Omega\\), cada um deles equiprovável (\\(P(E_{i} =\\frac{1}{N}\\)). Se \\(E\\) é a união de \\(n\\) desses eventos mutuamente exclusivos, pelo terceiro postulado das probabilidades teremos: \\[\\begin{align*} P(E) &amp; = P(E_{1}) + P(E_{2}) + P(E_{3}) + ... P(E_{n}) \\\\ P(E) &amp; = \\frac{1}{N} + \\frac{1}{N} +\\frac{1}{N} +...+\\frac{1}{N} \\\\ P(E) &amp; = \\frac{n}{N} \\end{align*}\\] 4.8.3 Teorema 03 Se \\(E\\) e \\(E^{c}\\) são eventos complementares no espaço amostra \\(\\Omega\\) então \\(P(E^{c}) = 1 - P(E)\\). Sendo os eventos \\(E\\) e \\(E^{c}\\) mutuamente exclusivos e também sendo \\(E \\cup E^{c} = \\Omega\\), considerando-se que \\(P(\\Omega) = 1\\), pelos segundo e terceiro postulados tem-se: \\[\\begin{align*} P(\\Omega) &amp; = 1 \\\\ 1 &amp; = P(E \\cup E^{c}) \\\\ 1 &amp; = P(E) + P(E^{c}) \\end{align*}\\] 4.8.4 Teorema 04 \\(P(\\varnothing)=0\\) Sendo \\(\\Omega\\) e \\(\\varnothing\\) são mutuamente exclusivos e, como de acordo com a definição de um espaço vazio \\(\\Omega \\cup \\varnothing = \\Omega\\), pelo terceiro postulado tem-se: \\[\\begin{align*} P(\\Omega) &amp; = P(\\Omega \\cup \\varnothing)\\\\ P(\\Omega) &amp; = P(\\Omega) + P(\\varnothing)\\\\ P(\\Omega) - P(\\Omega) &amp; = P(\\varnothing)\\\\ P(\\varnothing) &amp; =0 \\end{align*}\\] 4.8.5 Teorema 05 Se \\(A\\) e \\(B\\) são eventos em um mesmo espaço amostral \\(\\Omega\\) e \\(A \\subset B\\) então \\(P(A) \\le P(B)\\). Se \\(A \\subset B\\) então pode-se escrever: \\(B = A \\cup (A^{c} \\cap B)\\) (verifica-se pelo correspondente diagrama de Venn). Como \\(A\\) e \\(A^{c}\\cap B\\) são mutuamente exclusivos, pelo terceiro postulado tem-se: \\[\\begin{align*} P(B) &amp; = P(A) + P(A^{c}\\cap B) \\\\ P(A) &amp; = P(B) - P(A^{c}\\cap B) \\end{align*}\\] 4.8.6 Teorema 06 A probabilidade de qualquer evento \\(E\\) em \\(\\Omega\\) está compreendida entre \\(0 \\le P(E) \\le 1\\). Estando \\(\\varnothing \\subset E \\subset \\Omega\\) e considerando-se o Teorema 5 tem-se: \\[ P(\\varnothing) \\le P(E) \\le P(\\Omega) \\\\ 0 \\le P(E) \\le 1 \\] 4.8.7 Teorema 07 Para dois eventos quaisquer em \\(\\Omega\\), \\(A\\) e \\(B\\) tem-se que: \\(P( A \\cup B ) = P(A) + P(B) - P(A \\cap B)\\). Sejam as seguintes probabilidades para esses eventos mutuamente exclusivos: $P(A B) = a $; \\(P(A \\cap B^{c}) = b\\); e, \\(P(A^{c} \\cap B) = c\\). \\[\\begin{align*} P ( A \\cup B) &amp; = a + b + c \\\\ P ( A \\cup B) &amp; = (a + b) + (c + d) - a \\\\ P ( A \\cup B) &amp; = P(A) + P(B) - P(A \\cap B) \\end{align*}\\] 4.8.8 Teorema 08 Para três eventos quaisquer em \\(\\Omega\\), \\(A\\), \\(B\\) e \\(C\\) tem-se que: \\[\\begin{align*} P( A \\cup B \\cup C ) &amp; = \\\\ &amp; = P(A) + P(B) +P(C) - \\\\ &amp; P(A \\cap B) - P(A \\cap C) - P(B \\cap C) + \\\\ &amp; P(A \\cap B \\cap C) \\end{align*}\\] Escrevendo-se \\(A \\cup B \\cup C\\) como \\(A \\cup (B \\cup C)\\) e usando o Teorema 7 duas vezes (uma para \\(P[A \\cup (B \\cup C)]\\) e a outra para \\(P( B \\cup C)\\) tem-se: \\[\\begin{align*} P( A \\cup B \\cup C) &amp; = P[ A \\cup (B \\cup C)] \\\\ P( A \\cup B \\cup C) &amp; = P(A) + P( B \\cup C) - P [A \\cap (B \\cup C)]\\\\ P( A \\cup B \\cup C) &amp; = P(A) + P(B) + P(C) - P (B \\cap C) - P [A \\cap (B \\cup C)] \\end{align*}\\] Pela lei distributiva tem-se: \\[\\begin{align*} P [A \\cap (B \\cup C)] &amp; = P[ (A \\cap B) \\cup (A \\cap C ) ]\\\\ P [A \\cap (B \\cup C)] &amp; = P(A \\cap B) + P(A \\cap C) - P[ ( A \\cap B) \\cap (A \\cap C)] \\\\ P [A \\cap (B \\cup C)] &amp; = P(A \\cap B) + P(A \\cap C) - P( A \\cap B \\cap C) \\end{align*}\\] Chega-se a : \\[\\begin{align*} P( A \\cup B \\cup C ) &amp; = \\\\ &amp; P(A) + P(B) +P(C) - P(A \\cap B) - P(A \\cap C) - P(B \\cap C) +\\\\ &amp; P(A \\cap B \\cap C) \\end{align*}\\] "],["introdução-a-variáveis-aleatórias.html", "Capítulo 5 Introdução a variáveis aleatórias", " Capítulo 5 Introdução a variáveis aleatórias Ao realizar um experimento aleatório frequentemente estamos interessados mais em alguma função do resultado do que no próprio resultado em si: Ao lançar dois dados, podemos estar interessados na soma “7”, sem nos importar se isso foi decorrente de (1,6),(2,5),(3,4),(4,3),(5,2) ou (6,1). De forma semelhante, ao lançar três vezes uma moeda, podemos estar interessados no número total de “2 caras” (KK) que ocorre, sem nos preocuparmos se isso decorreu da sequência (K,K,C),(K,C,K) ou (C,K,K). Da mesma forma, ao planejar uma família de três filhos, podemos estar interessados em ter exatamente 2 filhos do sexo masculino (MM), sem nos importar se isso resultou de (F,M,M), (M,F,M) ou (M,M,F). Essas quantidades de interesse, ou mais formalmente, essas funções de valor real definidas no espaço amostral (o conjunto de todos os possíveis resultados do experimento), são conhecidas como variáveis aleatórias. Em outras palavras, uma função \\(X\\) (geralmente representada por uma letra maiúscula) que associa cada elemento \\(\\omega\\) pertencente ao espaço amostral \\(\\Omega\\) um número real é denominada, de forma mais precisa, como uma variável aleatória ou função estocástica. \\[ X (\\Omega) \\to \\mathcal{R}_{X} \\text{, estando } \\mathcal{R}_{X}\\subseteq \\mathcal{R} \\] Figure 5.1: Variável aleatória O domínio dessa função (\\(X\\)) é o conjunto de todos os possíveis valores numéricos de interesse do experimento aleatório e seu contradomínio está em \\(\\mathbb{R}\\). Exemplo 1: considere o espaço amostral do experimento aleatório relacionado ao sexo do bebê em três gestações bem sucedidas: \\(\\Omega=\\{\\omega_{1}:(FFF), \\omega_{2}:(FFM),\\omega_{3}:(FMF), \\omega_{4}:(MFF),\\\\ \\omega_{5}:(FMM), \\omega_{6}:(MFM), \\omega_{7}:(MMF), \\omega_{8}:(MMM)\\}.\\) Se estivermos interessados no número de nascimentos do sexo masculino, podemos definir a função \\(X\\) para associar cada elemento \\(\\omega_{i}\\) em \\(\\Omega\\) a um valor \\(x_{i} \\in \\mathbb{R}\\) que apresentará os seguintes resutados: \\(X(FFF)=0; X(FFM)=1; X(FMF)=1; X(MFF)=1;\\\\ X(FMM)=2; X(MFM)=2; X(MMF)=2; X(MMM)=3\\) \\(\\mathcal{R}_{X}=\\{x_{1}=0; x_{2}=1; x_{3}=2; x_{4}=3\\}\\subseteq \\mathcal{R}\\) Exemplo 2: considere o espaço amostral do experimento aleatório relacionado à sobrevivência de paciente ao final de 1 dia em uma UTI com quatro leitos: \\(\\Omega=\\{(0,0,0,0),(0,0,0,1),(0,0,1,0),(0,0,1,1),\\\\ (0,1,0,0),(0,1,0,1),(0,1,1,0),(0,1,1,1),(1,0,0,0),\\\\ (1,0,0,1),(1,0,1,0),(1,0,1,1),(1,1,0,0),(1,1,0,1),\\\\ (1,1,1,0),(1,1,1,1)\\}.\\) Cada elemento do espaço amostral é uma sequência de quatro valores binários \\((x_{1},x_{2},x_{3},x_{4})\\), onde: \\(x_{i}=0\\) indica que o paciente no leito \\(i\\) sobreviveu e \\(x_{i}=1\\) caso contrário. Se estivermos interessados no número de falecimentos podemos definir a função \\(X\\) para associar cada elemento \\(\\omega_{i}\\) em \\(\\Omega\\) a um valor \\(x_{i} \\in \\mathbb{R}\\) que apresentará os seguintes resultados: \\(X(0000)=0; X(0001)=1; X(0010)=1; \\dots ; X(1111)=4\\) \\(\\mathcal{R}_{X}=\\{x_{1}=0; x_{2}=1; x_{3}=2; x_{4}=3; x_{5}=4\\}\\subseteq \\mathcal{R}\\) Existem dois tipos de variáveis aleatórias: discretas e contínuas: Os valores possíveis de uma variável aleatória discreta pertencem a um conjunto finito ou infinito enumerável, como \\(\\{0,1,2, \\dots \\}\\). Exemplos incluem: o número de acidentes em uma semana; o número de partículas αα emitidas por uma fonte radioativa em um intervalo de tempo; ou o número de casos de uma doença em um mês. Os valores possíveis de uma variável aleatória contínua pertencem a um intervalo contínuo de números reais, como \\([a,b]\\), \\([0,\\infty)\\) ou \\(-\\infty, \\infty)\\). Exemplos incluem: o peso ou a altura de um grupo de pessoas; o tempo de vida de uma lâmpada; o tempo de reação a um estímulo; a concentração de álcool em um certo volume de sangue, a temperatura mínima no inverno Antártico. A função aletória \\(X\\) não irá, em geral, simplesmente relacionar os possíveis resultados de interesse do experimento aleatório a números reais. Mas sim computar algumas informações úteis como a probabilidade de sua verificação. "],["função-massa-de-probabilidade-probability-mass-function---pmf.html", "5.1 Função massa de probabilidade (Probability Mass Function - PMF)", " 5.1 Função massa de probabilidade (Probability Mass Function - PMF)   Considere \\(X\\) uma variável aleatória discreta com o contradomínio \\(x_{1},x_{2},x_{3}, \\dots x_{n} \\dots\\). Uma função (de distribuição) de probabilidade \\(f(x)\\) é assim denominada se, aplicada a cada um dos possíveis valores \\(x_{i}\\) da variável aleatória \\(X\\), resultar em sua probabilidade de ocorrência. Assim, para \\(x = x_{i}\\), \\(f(x_{i}) = P(X=x_{i})=p(x{i})\\).   Para que essa função \\(f(x)\\) possa ser considerada uma função de distribuição de probabilidade, ela precisa necessariamente atender às seguintes condições:   Postulado do intervalo: \\[ 0 \\leq f(x_{i}) \\leq 1 \\] para qualquer \\(x_{i} \\in \\mathcal{R}_{X}\\), onde \\(\\mathcal{R}_{X}\\) é o contradomínio de \\(X\\). Postulado do evento certo: \\[ \\sum _{i=1}^{n}f\\left(x_{i}\\right) = 1. \\]   Equivale afirmar que a probabilidade de ocorrência de um dos valores que a variável aleatória pode assumir está sempre compreendida no intervalo \\(0 \\leq P(X = x_{i}) \\leq 1\\). E mais, que a soma das probabilidades de todos os possíveis valores de \\(X\\) será 1. Observação: somente se a soma acma for finita há a posibilidade de se ter probabilidades \\(p(x_{i})\\) iguais para todos \\(x_{i}\\)   Exemplo: Suponha que uma moeda seja lançada duas vezes e que \\(X\\) seja a variável aleatória que represente o número de \\(caras\\) verificado. Defina o espaço amostral, associe para cada evento possível o valor da variável aleatória e definda uma função discreta de probabilidade correspondente.   O espaço amostral desse experimento é S = {(cara,cara), (cara,coroa), (coroa,cara), (coroa,coroa)} e a tabela abaixo relaciona o número de caras (o valor da variável aleatória \\(X\\)) associado a cada evento possível desse experimento:   Ponto amostral (cara,cara) (cara,coroa) (coroa,cara) (coroa,coroa) X 2 1 1 0   As probabilidades de ocorrência de cada um desses eventos é:   \\[\\begin{align*} P(cara,cara) &amp; = \\frac{1}{4} \\\\ P(cara,coroa) &amp; = \\frac{1}{4}\\\\ P(coroa,cara) &amp; = \\frac{1}{4} \\\\ P(coroa,coroa) &amp; = \\frac{1}{4}\\\\ \\end{align*}\\]   Para definir uma função discreta de distribuição de probabilidade deveremos associar a cada valor que a variável aleatória \\(X\\) assume sua correspondente probabilidade de ocorrência. \\[\\begin{align*} P(X=0) &amp; = P(coroa,coroa) = \\frac{1}{4} \\\\ P(X=1) &amp; = P[(cara,coroa) \\cup (coroa,cara)] \\\\ &amp; = P(cara,coroa) + P(coroa,cara)\\\\ &amp; = \\frac{1}{4} + \\frac{1}{4} \\\\ &amp; = \\frac{1}{2} \\\\ P(X=2) &amp; = P(cara,cara) = \\frac{1}{4} \\end{align*}\\]   Função discreta de probabilidades da variável aleatória X xk 0 1 2 P(X = xk) = f(xk) 1/4 1/2 1/4 # Valores possíveis para o número de caras x &lt;- c(0, 1, 2) # Probabilidades associadas (calculadas com o modelo binomial) p &lt;- c(0.25, 0.5, 0.25) # Criando o gráfico de barras barplot( p, names.arg = x, xlab = &quot;x&quot;, ylab = expression(f(x)), col = &quot;gray&quot;, ylim = c(0, 1), main = &quot;Probabilidade de observar caras ao lançar 2 moedas&quot; ) # Adicionando linhas no eixo y para reforçar a interpretação abline(h = seq(0, 1, by = 0.25), col = &quot;lightgray&quot;, lty = 2)   Uma função de distribuição cumulativa \\(F(x)\\) para uma variável aleatória \\(X\\) exprime a probabilidade de que a variável aleatória \\(X\\) assuma um valor menor ou igual a determinado \\(x\\) , sendo definida por: \\[ F(x) = P(X \\leq x) \\]   Propriedades:   1- \\(0 \\leq F(x) \\leq 1\\) (os valores da função estão no intervalo de probabilidade); 2- \\(F(x)\\) é não decrescente: \\(F(x) \\leq F(y)\\) se \\(x \\leq y\\) (a probabilidade cumulativa nunca diminui); - \\(F(- \\infty) = \\underset{x\\to -\\infty }{lim}F\\left(x\\right)=0\\) (a probabilidade cumulativa no limite inferior é zero); 4- \\(F(+ \\infty) = \\underset{x\\to \\infty }{lim}F\\left(x\\right)=1\\) (a probabilidade cumulativa no limite superior é um).   Relação com a Função de Probabilidade: Para uma variável aleatória discreta \\(X\\), a função massa de probabilidade \\(f(x)\\) pode ser derivada da função de distribuição cumulativa \\(F(x)\\). Especificamente, para todo \\(x\\) em \\((-\\infty, \\infty)\\)): \\[ f(x) = P(X = x) = F(x) - F(x^-), \\] onde \\(F(x^-)\\) é o valor de \\(F(x)\\) imediatamente antes de \\(x\\), considerando a natureza discreta da variável. Além disso, a definição cumulativa da função \\(F(x)\\) para valores discretos \\(x\\) pode ser escrita como: \\[ F\\left(x\\right)=P\\left(X\\le x\\right)=\\sum _{u\\le n}f\\left(u\\right), \\] em que \\(u\\) são os valores possíveis da variável aleatória \\(X\\). Equivale dizer que é a soma sobre todos os valores \\(u\\) assumidos por \\(X\\) para os quais \\(u \\leq x\\).   Se \\(X\\) é discreta e assume um número finito de valores \\(x_{1},x_{2}, \\dots, x_{n}\\), então sua função de probabilidade cumulativa \\(F(x)\\) será dada por: \\[ F(x) = \\begin{cases} 0, &amp; \\text{se } -\\infty &lt; x &lt; x_{1}, \\\\ f(x_{1}), &amp; \\text{se } x_{1} \\leq x &lt; x_{2}, \\\\ f(x_{1}) + f(x_{2}), &amp; \\text{se } x_{2} \\leq x &lt; x_{3}, \\\\ \\vdots &amp; \\\\ f(x_{1}) + f(x_{2}) + \\dots + f(x_{n}), &amp; \\text{se } x_{n} \\leq x &lt; \\infty. \\end{cases} \\] Essa expressão mostra que \\(F(x)\\) é obtida pela soma cumulativa das probabilidades associadas aos valores discretos de \\(X\\), até um ponto \\(x\\).   Exemplo: Suponha que uma moeda seja lançada duas vezes e que \\(X\\) seja a variável aleatória que represente o número de caras verificado. Especifique sua função de probabilidade cumulativa dessa variável aleatória e apresente seu gráfico.   xk 0 1 2 P(X = xk) = f(xk) 1/4 1/2 1/4   Sua função de probabilidade cumulativa é dada por:     O gráfico de sua função de probabilidade cumulativa é:   # Valores possíveis para o número de caras x &lt;- c(0, 1, 2) # Probabilidades associadas (calculadas com o modelo binomial) p &lt;- c(0.25, 0.5, 0.25) # Probabilidade acumulada p_cumulative &lt;- cumsum(p) # Criando o gráfico de probabilidade acumulada com linhas verticais plot( x, p_cumulative, type = &quot;s&quot;, # Tipo &quot;steps&quot; para probabilidade acumulada xlab = &quot;x&quot;, ylab = expression(F(x)), ylim = c(0, 1), xlim = c(0, 3), # Expandido para incluir margens main = &quot;Função de distribuição acumulada (CDF)&quot;, col = &quot;blue&quot;, lwd = 2, axes = FALSE # Desabilita os eixos padrão para customização ) # Adicionando linhas verticais para conectar os valores segments( x0 = x[-1], # Exclui o primeiro valor (X=0) y0 = c(0, p_cumulative[-length(p_cumulative)])[-1], # Exclui o valor inicial (F(0)) x1 = x[-1], y1 = p_cumulative[-1], # Exclui o valor inicial (F(0)) col = &quot;red&quot;, lty = 2 # Linhas tracejadas ) # Adicionando bolinhas abertas nos limites inferiores (apenas X &gt; 0) points(x[-1], c(0, p_cumulative[-length(p_cumulative)])[-1], pch = 1, col = &quot;blue&quot;, cex = 1.5) # Adicionando bolinhas fechadas no limite superior de cada degrau points(x, p_cumulative, pch = 16, col = &quot;blue&quot;, cex = 1.5) # Customizando o eixo X axis(1, at = 0:3, labels = 0:3) # Customizando o eixo Y com frações axis(2, at = c(0, 0.25, 0.5, 0.75, 1), labels = c(&quot;0&quot;, &quot;1/4&quot;, &quot;1/2&quot;, &quot;3/4&quot;, &quot;1&quot;), las = 1) # Adicionando grades para facilitar a interpretação grid(nx = NULL, ny = NULL, col = &quot;lightgray&quot;, lty = &quot;dotted&quot;) "],["função-de-densidade-de-probabilidade-probability-density-function---pdf.html", "5.2 Função de densidade de probabilidade (Probability Density Function - PDF)", " 5.2 Função de densidade de probabilidade (Probability Density Function - PDF) Considerem os espaços amostrais a seguir (\\(\\Omega_{1},\\Omega_{2},\\Omega_{3},\\Omega_{4},\\Omega_{5}\\)) representativos de 4 experimentos aleatórios e admitam também que todos os eventos possíveis são equiprováveis.   Figure 5.2: Diferentes espaços amostrais de um experimento aleatório (por razões gráficas desprezem o espaço fora dos círculos   Interpretem o último deles como um espaço amostral formado por \\(\\infty\\) pontos amostrais.   Os eventos que compõem os quatro primeiros espaços amostrais são variável aleatória discretas. Discretas pois permitem a contagem dos possíveis valores (finitos ou infinitos contáveis) aleatórios que o experimento pode assumir. Mas no quinto espaço amostral temos incontáveis possibilidades.   Um espaço amostral com essa característica é representativo de uma variável aleatória contínua.   Sendo todos os eventos representados nos espaços amostrais equiprováveis, comparemos as probabilidades associadas a cada um desses possíveis resultados. Em \\(\\Omega_{1}\\), \\(P(\\omega_{1})=1\\) Em \\(\\Omega_{2}\\), \\(P(\\omega_{1})=P(\\omega_{2})=P(\\omega_{3})=P(\\omega_{4})=0,50\\) Em \\(\\Omega_{3}\\), \\(P(\\omega_{1})=P(\\omega_{2})=...=P(\\omega_{16})=0,0625\\) Em \\(\\Omega_{4}\\), \\(P(\\omega_{1})=P(\\omega_{2})=...=P(\\omega_{64})=0,015625\\) Em \\(\\Omega_{5}\\), \\(P(\\omega_{n}) \\rightarrow 0\\), à medida que o número de eventos \\(n \\rightarrow \\infty\\) A probabilidade individual de qualquer evento do quinto espaço amostral ocorrer \\(\\rightarrow 0\\). Por essa razão, no caso de variáveis aleatórias contínuas, não faz sentido falar em uma probabilidade pontual exata, associada a um resultado específico. Isso ocorre porque, para qualquer valor particular, a probabilidade é sempre igual a zero. Experimentos aleatórios nos quais os possíveis resultados assumem valores resultantes de processos de mensuração tais como, por exemplo, rendas, pesos, velocidades, tempos, comprimentos, pertencentes aos números Reais, podem ser adequadamente modelados por variáveis aleatórias contínuas. Para estes uma função densidade de probabilidade é definida de modo a retornar a probabilidade de ocorrência associada a um intervalo de valores, posto a probabilidade de ocorrência de um valor aleatório contínuo específico \\(x\\) tender a zero: \\(P(X=x) \\to 0\\). A função \\(f(x)\\) é uma função densidade de probabilidade para a variável aleatória contínua \\(X\\) se atende às seguintes condições relacionadas aos axiomas da probabilidade: 1- \\(f(x) \\geq 0\\), para todo \\(x \\in (-\\infty, \\infty)\\); 2- A integral da função sobre todo o domínio é igual a 1: \\[ \\underset{-\\infty }{\\overset{\\infty }{\\int }}f\\left(x\\right)dx = 1 \\]. Se \\(X\\) é uma variável aleatória contínua, a probabilidade de \\(X\\) assumir qualquer valor exato é \\(P(X = x) = 0\\). No entanto, a probabilidade intervalar de \\(X\\) estar entre dois valores distintos, \\(a\\) e \\(b\\), é dada por: \\[ P(a &lt; X &lt; b) = \\int_{a}^{b} f(x) \\, dx. \\] Graficamente, a interpretação de uma função densidade de probabilidade contínua é representada pela área sob a curva da função \\(f(x)\\), delimitada pelos valores de interesse \\(a\\) e \\(b\\). Essa área corresponde à probabilidade de \\(X\\) estar no intervalo \\((a, b)\\). Figure 5.3: A área sob a curva de uma função de probabilidade de uma variável contínua entre dois valores quaisquer é a probabilidade de se observar valores entre esses dois pontos Como \\(f(x) \\geq 0\\), a curva da função densidade de probabilidade estará acima do eixo \\(x\\) e a totalidade da área será igual a \\(1\\), conforme \\[ \\underset{-\\infty }{\\overset{\\infty }{\\int }}f\\left(x\\right)dx = 1. \\] Para estes uma função densidade de probabilidade é definida de modo a retornar a probabilidade de ocorrência associada a um intervalo de valores, posto a probabilidade exata de ocorrência de um valor aleatório contínuo específico \\(x\\) tender a zero \\(P(X=x) \\to 0\\). A função \\(f(x)\\) é uma função densidade de probabilidade para a variável aleatória contínua \\(X\\) se atende às seguintes condições relacionadas aos axiomas da probabilidade: Para tornar o conceito de densidade mais compreensível admita a função densidade de probabilidade (fdp) a seguir e sua representação gráfica na Figura 5.4 \\[ f(X=x)= \\begin{cases} 2 . x \\hspace{0.6cm} \\text{para } 0 \\le x \\le 1 \\\\ 0, \\hspace{0.9cm} \\text{para qualquer outro x}\\\\ \\end{cases} \\] Figure 5.4: A área definida por (ODA) equivale à probabilidade de \\(f(X=x)\\) no intervalo \\(0 \\le x \\le 0,50\\) é notadamente menor que a área definida por (ABCD) equivalente à probabilidade de \\(f(X=x)\\) no intervalo \\(0,5 \\le x \\le 1\\). Tendo os intervalos [0;0,50] e [0,50; 1,00] igual amplitude, depreende-se que uma fdp é uma função indicadora da concentração massa (probabilidade) nos possíveis valores de \\(X\\) A função de distribuição cumulativa, definida como \\(F(x) = P(X \\leq x)\\), também terá a forma de uma curva, crescente, que aumenta continuamente de \\(0\\) para \\(1\\). Essa característica reflete o fato de que, ao longo do eixo \\(x\\), a probabilidade cumulativa acumula todos os valores de \\(f(x)\\) até \\(x\\), de modo que: \\(F(x)\\) é não decrescente (\\(F(x_1) \\leq F(x_2)\\) para \\(x_{1} \\leq x_{2}\\)); \\(F(x)\\) atinge valores-limite: \\[ \\lim_{x \\to -\\infty} F(x) = 0 \\quad \\text{e} \\quad \\lim_{x \\to \\infty} F(x) = 1. \\] Assim, \\(F(x)\\) descreve graficamente a probabilidade acumulada até qualquer valor \\(x\\), reforçando a relação entre a densidade de probabilidade e a probabilidade cumulativa. Figure 5.5: Função de probabilidade cumulativa Exemplo: Seja a seguinte função e verifique se a função \\(f(x)\\) pode ser a função de densidade de probabilidade da variável aleatória contínua \\(X\\) e determine qual a probabilidade associada a valores compreendidos no intervalo \\(0 \\leq X \\leq \\frac{1}{2}\\). A resolução deste exemplo será feita de um modo geométrico. Figure 5.6: A probabilidade de se observar valores entre 0 e 1/2 é igual à area sob a função densidade de probabilidade entre esses dois valores Verificações para se aceitar a função como uma função de densidade de probabilidade para a variável aleatória \\(X\\): \\[ f(x) \\geq 0 \\] e, \\[ \\underset{-\\infty }{\\overset{\\infty }{\\int }}f\\left(x\\right)dx = 1 \\] Resp.: Atende às duas condições (não assume valores menores que zero e a área sob a reta dessa função é unitária) Cálculo da probabilidade para o intervalo \\(0 \\leq X \\leq \\frac{1}{2}\\) a partir da área do triângulo hachurado (\\(\\frac{base \\times altura}{2}\\)): \\[ P ( 0 \\leq X \\leq \\frac{1}{2}) = \\frac{1}{2} \\times (\\frac{1}{2} \\times 1 ) = \\frac{1}{4} \\] Exemplo: Verifique se as funções a seguir atendem os pressupostos necessários para ser uma função densidade de probabilidade (assuma que toda \\(f(x)=0\\) para valores fora dos intervalos especificados): 1- \\(f(x)=3x\\) para \\(0 \\le x \\le 1\\); 2- \\(f(x)=\\frac{x^{2}}{2}\\) para \\(x \\ge 0\\); 3- \\(f(x) = \\frac{(x-3)}{2}\\) para \\(3 \\le x \\le 5\\); 4- \\(f(x)=2\\) para \\(0 \\le x \\le 2\\); 5- \\[ f(X=x)= \\begin{cases} \\frac{(2+x)}{4}, \\hspace{0.6cm} \\text{para } -2 \\le x \\le 0 \\\\ \\frac{(2-x)}{4}, \\hspace{0.6cm} \\text{para } 0 \\le x \\le 2\\\\ \\end{cases} \\] 6- \\(f(x)=- \\pi\\) para \\(-\\pi &lt; x &lt; 0\\) Os gráficos das funções densidade de probabilidade são: Figure 5.7: A área definida por \\(f(x)\\) no intervalo \\(0 \\le x \\le 1\\) é maior que 1. Por essa razão não pode ser uma fdp Figure 5.8: A área definida por \\(f(x)\\) no intervalo \\(x \\ge 0\\) é maior que 1. Por essa razão não pode ser uma fdp Figure 5.9: Os valores assumidos por \\(f(x)\\) são \\(\\ge 0\\) e a área definida por f(x) o intervalo \\(3 \\le x \\le 5\\) é igual a 1. Por essa razão pode ser uma fdp Figure 5.10: A área definida por \\(f(x)\\) no intervalo \\(0 \\le x \\le 2\\) é maior que 1. Por essa razão não pode ser uma fdp Figure 5.11: Os valores assumidos por \\(f(x)\\) são \\(\\ge 0\\) e a área definida por \\(f(x)\\) nos intervalos \\(-2 \\le x \\le 0\\) e \\(0 \\le x \\le 2\\) é igual a 1. Pode ser uma fdp Figure 5.12: Os valores assumidos por f(x) são \\(&lt;0\\). Por esa razão não pode ser uma fdp. Exemplo: A dureza \\(X\\) de uma peça de aço pode ser entendida como sendo uma variável aleatória contínua uniforme no intervalo \\((50,70)\\) da escala Rockwel. Calcule a esperança e a variâcia dessa variável aleatória e a probabilidade de que uma peça tenha dureza entre 55 e 60? Definindo a variável aleatória contínua \\(X:X \\sim U(50,70)\\): \\[ f(X=x)= \\begin{cases} \\frac{1}{70-50}=\\frac{1}{20}, \\hspace{0.6cm} \\text{para } 50 \\le x \\le 70 \\\\ 0, \\hspace{1cm} \\text{para qualquer outro x}\\\\ \\end{cases} \\] Sua esperança e a variância são: Esperança: \\(E(X) = \\mu = \\frac{(70+50)}{2}=60\\); e, Variância: \\(Var(X) = \\frac{(70-50)^{2} }{12}=33,33\\). Figure 5.13: Os valores assumidos por \\(f(x)\\) são \\(\\ge 0\\) e a área definida por \\(f(x)\\) no intervalo \\(50 \\le x \\le 70\\) é igual a 1. Por essa razão pode ser uma fdp. A probabilidade pedida equivale à área \\(P(60 \\le x \\le 55) = (60-55) . 0,05=0,25\\). "],["esperança-e-variância-de-uma-variável-aleatória-discreta.html", "5.3 Esperança e variância de uma variável aleatória discreta", " 5.3 Esperança e variância de uma variável aleatória discreta Coletando-se dados podemos analisá-los, por exemplo, em termos de sua distribuição, pelas estatísticas da média e variância. De maneira análoga procedemos com variáveis aleatórias (discretas ou contínuas) onde dispomos das probabilidades de ocorrência associadas a cada um dos valores (discretos ou infinitos numeráveis) que ela pode assumir. A esperança matemática (valor esperado ou expectância) de uma variável aleatória discreta é dada pela somatória do produto de cada um dos valores que ela pode assumir pela probabilidade associada a cada um desses valores. Seja \\(X\\) uma variável aleatória discreta que pode assumir os valores \\(x_{1},x_{2}, \\dots x_{n}\\); e sejam \\(P_{1},P_{2}, \\dots, P_{n}\\) as respectivas probabilidades associadas às suas ocorrências. A esperança da variável \\(X\\), denotada por \\(E(X)\\) será: \\[ E\\left(X\\right)=\\sum _{i=1}^{n}{x}_{i}.{P}_{i} \\] Com n sendo o número de possíveis resultados que a variável \\(X\\) pode assumir. A expressão anterior é semelhante àquela usada para se calcular a média para frequências de dados sendo que agora, no lugar de se utilizar a frequência relativa a cada dado observado, temos as probabilidades dadas por um modelo teórico pressuposto. Algumas propriedades envolvendo a esperança:   1- Se \\(c\\) é uma contante qualquer, então: \\(E(c) = c\\) (\\(c \\in \\mathbb{R}\\)); 2- Se \\(c\\) é uma contante qualquer, então: \\(E(c X) = c . E(X)\\) (\\(c \\in \\mathbb{R}\\)); 3- Se \\(c\\) é uma contante qualquer, então: \\(E(X \\frac{+}{-} c) = E(X) \\frac{+}{-} c\\) (\\(c \\in \\mathbb{R}\\)); 4- Se \\(X\\) e \\(Y\\) são duas variáveis aleatórias quaisquer, então: \\(E(X +/- Y) = E(X) +/- E(Y)\\); 5- Se \\(X\\) e \\(Y\\) são duas variáveis aleatórias independentes quaisquer, então: \\(E(X . Y) = E(X). E(Y)\\). A variância de uma variável aleatória qualquer \\(X\\), denotada por \\(Var(X)\\), será dada por: \\[\\begin{align*} Var\\left(X\\right) &amp; = \\sum_{i=1}^{n} [{x}_{i} - E(X)]^{2}.{P}_{i} \\end{align*}\\] Algumas propriedades envolvendo a variância:   1- Se \\(c\\) é uma contante qualquer, então: \\(Var(c)=0\\) (\\(c\\in\\mathbb{R}\\)); 2- Se \\(c\\) é uma contante qualquer, então: \\(Var(cX)=c^{2}.Var(X)\\) (\\(c\\in\\mathbb{R}\\)); 3- Se \\(X\\) e \\(Y\\) são duas variáveis aleatórias independentes quaisquer, então: \\(Var(X \\pm Y)=Var(X)+Var(Y)\\); 4- Se \\(X\\) e \\(Y\\) são duas variáveis aleatórias quaisquer, então: \\(Var(X \\pm Y)=Var(X)+Var(Y) \\pm 2Cov(X,Y)\\) (também). A covariância (\\(Cov(X,Y)\\)) entre duas variáveis aleatórias quaisquer \\(X\\) e \\(Y\\) é dada por: \\[ Cov \\left(X,Y\\right)= E(XY) - E(X)E(Y) \\] Exemplo: Seja \\(X\\) uma variável aleatória discreta que indica o número de pontos observados na face superior de um dado quando ele é lançado. Calcule a esperança e a variância dessa variável aleatória. Função discreta de distribuição de probabilidades de X xi P(X = xi) 1 1/6 2 1/6 3 1/6 4 1/6 5 1/6 6 1/6 Total 1 \\(E(X) = \\frac{1}{6} . (1+2+3+4+5+6) = 3,50\\)   \\[\\begin{align*} Var(X) &amp; = (1-3,50)^{2}.(\\frac{1}{6}) + (2-3,50)^{2}.(\\frac{1}{6}) +\\\\ &amp; (3-3,50)^{2}.(\\frac{1}{6}) + (4-3,50)^{2}.(\\frac{1}{6}) + (5-3,50)^{2}.(\\frac{1}{6}) + \\\\ &amp; (6-3,50)^{2}.(\\frac{1}{6}) \\\\ &amp; = 2,90 \\end{align*}\\] Exemplo: Uma empresa de caminhões de aluguel possui uma frota composta de 4 veículos. O aluguel é cobrado por diária de uso de um caminhão e a função de distribuição de probabilidade de locações diárias está a seguir especificada. Calcule a esperança e a variância de locação diária dessa empresa. Função discreta de distribuição de probabilidade de locações diárias xi P(X = xi) 0 0,10 1 0,20 2 0,30 3 0,30 4 0,10 \\(E(X) = (0 . 0,10) + (1 . 0,20) + 2 . 0,30) + (3 . 0,30) + (4 . 0,10) = 2,10\\) (caminhões por dia) \\[\\begin{align*} Var(X) &amp; = (0-2,10)^{2}.0,10 + (1-2,10)^{2}.0,20 + (2-2,10)^{2}.0,30 + \\\\ &amp; (3-2,10)^{2}.0,30 + (4-2,10)^{2}.0,10 \\\\ &amp; = 1,29^{1} \\end{align*}\\] \\(^{1}\\): (caminhões por dia)\\(^{2}\\) "],["esperança-e-variância-de-uma-variável-aleatória-contínua.html", "5.4 Esperança e variância de uma variável aleatória contínua", " 5.4 Esperança e variância de uma variável aleatória contínua A esperança e a variância de uma variável aleatória contínua são dadas, respectivamente, por: \\[ E(X) = \\underset{-\\infty }{\\overset{\\infty }{\\int }}x.f\\left(x\\right)dx \\] \\[ Var(X) = E\\left[ (X - E(X))^{2} \\right] = \\underset{-\\infty }{\\overset{\\infty }{\\int }} (x-E(X))^{2}.f\\left(x\\right)dx \\] "],["introdução-a-modelos-teóricos-de-probabilidade.html", "Capítulo 6 Introdução a modelos teóricos de probabilidade", " Capítulo 6 Introdução a modelos teóricos de probabilidade Existem variáveis aleatórias discretas ou contínuas, que apresentam certas características ou padrões de comportamento. Para essas variáveis, com base nesses comportamentos típicos, foram estruturados modelos teóricos de distribuições de probabilidade (variáveis discretas) e de densidade de probabilidade (variáveis contínuas) e derivadas as expressões de suas esperanças e variâncias. "],["modelos-teóricos-de-probabilidade-para-variáveis-aleatórias-discretas.html", "6.1 Modelos teóricos de probabilidade para variáveis aleatórias discretas", " 6.1 Modelos teóricos de probabilidade para variáveis aleatórias discretas 6.1.1 Uniforme Variável aleatória \\(X\\), assumindo valores \\(x_{1}, x_{2} , \\dots, x_{k}\\) tem distribuição uniforme se, e somente se, \\[ P(X=x_{i})=\\frac{1}{k}. \\] para todo \\(i=1, 2, \\dots, k\\). Para uma variável com distribuição uniforme: Esperança: \\(E(X)=\\frac{1}{k}\\sum_{i=1}^{k} x_{i}\\) Variância: \\(VAR(X)=\\frac{1}{k} \\left[ \\sum_{i=1} ^{k} x_{i}^{2}\\times\\frac{( \\sum_{i=1} ^{k} x_{i})^2}{k} \\right]\\) 6.1.2 Bernoulli Variável aleatória com distribuição Bernoulli é uma variável definida por um experimento probabilístico em que os resultados possíveis se resumem a apenas dois: sucesso ou fracasso (ocorrência ou não).   Caracterização de uma variável aleatória \\(X\\) com distribuição de Bernoulli: \\(X\\sim Ber(p)\\) xi Evento P(X = xi) 1 Sucesso p 0 Fracasso q=(1-p) Σ - 1 Para uma variável de Bernoulli: Esperança: \\(E(X)=p\\) Variância: \\(VAR(X)=p(1-p)\\) Exemplo: Seja \\(X\\) uma variável aleatória resultante do lançamento de um dado uma única vez e cujo sucesso está definido como obter a face com 5 pontos. Calcule a probabilidade de sucesso e fracasso, assim como sua variância. xi Evento P(X = xi) (face 5 no lançamento de um dado) 1 Sucesso p=1/6 0 Fracasso q=5/6 Σ - 1   Esperança: \\(E(X)= \\frac{1}{6}\\) Variância: \\(Var(X)= \\frac{5}{36}\\) Admita agora \\(X\\) uma variável aleatória resultante de realização de \\(n\\) tentativas (repetições) de Bernoulli e definindo \\(x\\) como sendo o número de sucessos verificados nessas \\(n\\) tentativas. Desse modo, proporção de sucessos observada após \\(n\\) repetições é expressa como \\(\\frac{x}{n}\\). Se \\(p\\) é a probabilidade de sucesso a cada repetição e se \\(\\epsilon\\) é um número qualquer positivo, tem-se: \\[ \\underset{n\\to \\infty }{lim}P\\left(\\left|\\frac{x}{n}-p\\right|\\ge \\epsilon \\right)=0 \\] A Lei dos grandes números para infinitas repetições de Bernoulli afirma que, após um grande número de repetições (\\(n\\)), a proporção de sucessos observada (\\(\\frac{x}{n}\\)) irá se aproximar da probabilidade teórica da variável aleatória de Bernoulli \\(p\\). 6.1.3 Binomial Variável aleatória com distribuição Binomial é uma variável resultante da repetição de um experimento modelado por uma variável de Bernoulli (isto é, a cada repetição apenas dois resultados podem ocorrer: sucesso ou fracasso). Para que \\(X\\) seja uma variável aleatória com distribuição Binomial: \\(X\\sim b(n,p)\\) é necessário que: - o experimento deve ser realizado um número \\(n\\) finito de vezes; - cada repetição deve ser independente das demais; - cada repetição é, em essência, um ensaio de Bernoulli onde só pode haver dois resultados: sucesso ou fracasso; - a probabilidade de sucesso \\(p\\) em cada repetição é sempre a mesma; e, consequentemente, - a probabilidade de fracasso \\(q=1-p\\) em cada repetição é também a mesma. Considerem o diagrama de árvore ilustrado na Figura 6.1 que representa, esquematicamente, 3 repetições independentes de um evento modelado por uma variável de Bernoulli, com probabilidade individual de sucesso \\(P(X=1)=p\\) e, de fracasso, \\(P(X=0)=1-p=q\\). Figure 6.1: Três repetições independentes de um experimento aleatório modelado por uma variável de Bernoulli Se \\(p\\) é a probabilidade de se verificar sucesso em qualquer uma das \\(n\\) repetições de Bernoulli realizadas no experimento aletório então uma variável aleatória Binomial \\(X\\) definida sobre esse experimento apresentará \\(k\\) sucessos após \\(n\\) repetições independentes e terá a seguinte função de probabilidade: \\[\\begin{align*} f(k) &amp; = P(X=k) \\\\ f(k) &amp; = {C}_{k}^{n}. {p}^{k}. {q}^{(n-k)} \\\\ f(k) &amp; = \\frac{n!}{k!. (n-k)!} . {p}^{k}. {q}^{(n-k)} \\end{align*}\\] Sendo a probabilidade \\(p\\) de sucesso, igual em todas as repetições, então: Esperança: \\(E\\left(X\\right)=\\sum _{i=1}^{n}{x}_{i}. P\\left(X={x}_{i}\\right)=n. p\\) Variância: \\(V\\left(X\\right)=E\\left({X}^{2}\\right)-{\\left[E\\left(X\\right)\\right]}^{2} = n . p . q\\) Exemplo: Numa prova com 6 questões, a probabilidade de que um aluno acerte cada uma delas é de 0,30. Admitindo que a resolução dessas 6 questões é feita de modo independente, qual a probabilidade desse aluno acertar 4 questões? 1- cada questão apresenta apenas duas possibilidades: acertar ou errar; assim, esse experimento aleatório pode seguir o modelo teórico de Bernoulli tendo o evento de sucesso definido como: a chance de acertar uma prova, com probabilidade de ocorrência \\(p=0,30\\); 2- ao se repetir esse experimento \\(n=6\\) (pois este é o número de questões a serem resolvidas) o experimento passa seguir o modelo teórico Binomial pois nos foi assegurada a independência entre cada repetição bem como a constância da probabilidade \\(p\\). A probabilidade de se acertar \\(k=4\\) questões em \\(n-6\\) repetições independentes tendo cada uma uma probabilidade de sucesso \\(p=0,30\\) será então: \\[\\begin{align*} P\\left(X=k\\right) &amp; = {C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=4\\right) &amp; = 15 . 0,30^{4} . 0,70^{(6-4)} \\\\ &amp; = 0,0595 \\end{align*}\\] Conclusão: a probabilidade de um aluno acertar 4 questões das 6 resolvidas, considerando a probabilidade associada ao acerto de cada questão, é de 0,0595. Exemplo: Ainda utilizando a construção teórica desse experimento, admitamos que nosso interesse reside em obter as seguintes probabilidades a ele associadas: 1- probabilidade do aluno não acertar nenhuma questão; 2- probabilidade do aluno acertar todas as questões; 3- probabilidade do aluno acertar no mínimo 2 questões; e a 4- probabilidade do aluno acertar no máximo 2 questões. A resposta aos dois primeiros itens é imediata pela simples aplicação dos dados ao odelo, pois o número de sucessos desejado é \\(k=0\\) no primeiro e \\(k=6\\) no segundo (e \\(p=0,30\\) para todos) . Assim: \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=0\\right) &amp; = 1 . 0,30^{0} . 0,70^{(6-0)} \\\\ &amp; = 0,1176 \\end{align*}\\] \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=6\\right) &amp; = 1 . 0,30^{6} . 0,70^{(6-6)} \\\\ &amp; = 0,000729 \\end{align*}\\] A resposta aos dois últimos itens irá demandar o uso da regra da adição de probabilidades e, como cada evento é disjunto dos demais, essa regra recai sobre a simples adição das probabilidades envolvidas. Ao perguntar qual a probabilidade do aluno acertar no mínimo 2 questões (\\(P(X \\ge 2)\\)) equivale a se perguntar qual a probabilidade do aluno acertar 2 OU 3 OU 4 OU 5 OU 6 questões. Assim, temos como elementos desses eventos de sucesso \\({2, 3, 4, 5, 6}\\). Assim a solução passará pelo cálculo das probabilidades individuais para cada um desses eventos de sucesso que serão simplesmente somadas pois, a ocorrência de cada um desses eventos de sucesso é disjunta dos demais (se ocorrer 2 não ocorre simultaneamente 3). \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=2\\right) &amp; = 15 . 0,30^{2} . 0,70^{(6-2)} \\\\ &amp; = 0,3241 \\end{align*}\\] \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=3\\right) &amp; = 20 . 0,30^{3} . 0,70^{(6-3)} \\\\ &amp; = 0,1852 \\end{align*}\\] \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=4\\right) &amp; = 15 . 0,30^{4} . 0,70^{(6-4)} \\\\ &amp; = 0,0595 \\end{align*}\\] \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=5\\right) &amp; = 6 . 0,30^{5} . 0,70^{(6-5)} \\\\ &amp; = 0,01020 \\end{align*}\\] \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=6\\right) &amp; = 1 . 0,30^{6} . 0,70^{(6-6)} \\\\ &amp; = 0,000729 \\end{align*}\\] Assim, \\(P\\left(X\\ge2\\right)=0,3241+0,1852+0,0595+0,01020+0,00079=0,5797\\) # Defina o número de repetições (n) e o número de sucessos (k) n=6 # Número de repetições k=c(0,1,2,3,4,5,6) # Número de sucessos varia de nenhum (0) até dez (10) acertar as dez questões p=0.30 # A probabilidade em cada repetição de Bernoulli # Probabilidade de k sucessos em n repetições (utilizando a função &#39;dbinom&#39;) probabilidade &lt;- dbinom(k, n, prob = p) # Neste exemplo, 0.5 é a probabilidade de sucesso # Crie uma tabela com duas colunas (Número de Sucessos e Probabilidade) tabela &lt;- data.frame(Número_de_Sucessos = k, Probabilidade = probabilidade) # Exiba a tabela print(tabela) ## Número_de_Sucessos Probabilidade ## 1 0 0.117649 ## 2 1 0.302526 ## 3 2 0.324135 ## 4 3 0.185220 ## 5 4 0.059535 ## 6 5 0.010206 ## 7 6 0.000729 Exemplo: Uma pessoa trabalha em 3 empregos onde desenvolve atividades iguais, sendo remunerada também igualmente nos três lugares. A probabilidade de que o pagamento saia até o 2\\(^{o}\\) dia útil nos três empregos é de 0,85. Qual a probabilidade de apenas um salário sair até o 2\\(^{o}\\) dia útil? 1- a probabilidade de ocorrência do pagamento até o 2\\(^{o}\\) dia útil em cada emprego pode ser modelada por uma variável aleatória de Bernoulli pois apresenta apenas duas possibilidades: ocorrer ou não, cuja probabilidade de sucesso nos foi dada: \\(p=0,85\\); 2- os três empregos podem ser considerados como repetições desse experimento básico; 3- esse experimento final pode ter as probabilidades modeladas por uma variável aleatória Binomial com evento de sucesso definido como chance de se receber apenas um pagamento até o 2\\(^{o}\\) dia útil (\\(k=1\\)) pois consiste na repetição de (\\(n=3\\)) experimentos de Bernoulli independentes e com probabilidade individual constante (\\(p-0,85\\)). A probabilidade de se receber o pagamento até o 2\\(^{o}\\) dia útil em apenas um emprego será dada por: \\[\\begin{align*} P\\left(X=k\\right) &amp; ={C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\\\ P\\left(X=1\\right) &amp; =3 . 0,85^{1} . 0,15^{2} \\\\ &amp; = 0,0574 \\end{align*}\\] Conclusão: a probabilidade desse trabalhador receber apenas um salário até o 2\\(^{o}\\) dia útil do mês é de 0,0574. # Defina o número de repetições (n) e o número de sucessos (k) n=3 # Número de repetições k=c(0,1,2,3) # Número de sucessos varia de nenhum (0) até três (3) receber até o segundo dia nos três empregos p=0.85 # A probabilidade em cada repetição de Bernoulli # Probabilidade de k sucessos em n repetições (utilizando a função &#39;dbinom&#39;) probabilidade = dbinom(k, n, prob = p) # Neste exemplo, 0,856 é a probabilidade de sucesso # Crie uma tabela com duas colunas (Número de Sucessos e Probabilidade) tabela = data.frame(Número_de_Sucessos = k, Probabilidade = probabilidade) # Exiba a tabela print(tabela) ## Número_de_Sucessos Probabilidade ## 1 0 0.003375 ## 2 1 0.057375 ## 3 2 0.325125 ## 4 3 0.614125 6.1.4 Poisson A distribuição de Poisson (assim chamada em homenagem a Siméon Denis Poisson que a descobriu no início do século XIX) é largamente empregada quando se deseja contar o número de eventos raros cuja probabilidade média seja dada em termos de um intervalo de tempo, determinada extensão, área ou volume (uma taxa). Uma variável aleatória discreta \\(X\\) com Distribuição de Poisson é aquela que pode assumir infinitos valores numeráveis (\\(k=0,1,2, .s, \\infty\\)). Sua representação é: \\(X \\sim Pois (\\lambda)\\) e sua função de probabilidade para esses valores é: \\[\\begin{align*} f(k) &amp; = P(X=k) \\\\ &amp; = \\frac{\\lambda^{k}. \\epsilon^{-\\lambda}} {k!} \\end{align*}\\] Com \\(\\epsilon= 2,718\\) (número irracional de Euler). A esperança e a variância de uma variável aleatória discreta com Distribuição de Poisson são dados pelo seu parâmetro \\(\\lambda\\) que expressa o número médio de eventos ocorrendo no intervalo de tempo, ou em uma determinada extensão, área ou volume : Esperança: \\(E(X) = \\lambda\\); Variância: \\(Var(X) = \\lambda\\) Exemplo:Uma central telefônica recebe em média 5 chamadas por minuto. Supondo que a Distribuição de Poisson seja adequada a esse contexto, obter as probabilidade de que essa central não receba chamadas num intervalo de 1 e que receba no máximo duas chamadas em 4 minutos. Dados do problema: 1- \\(\\lambda=\\) é o parãmetro da distribuição de Poisson (a esperança, a média); assim temos \\(\\lambda=5\\) chamadas por minuto (é importante atentar para qual é a unidade associada ao valor do \\(\\lambda\\)); 2- não receber chamada alguma equivale a um \\(k=0\\); 3- na sequência, ao se perguntar sobre a probabilidade de se receber no máximo duas chamadas em 4 minutos equivale a não receber chamada alguma ou uma chamada ou duas chamadas (soma das probabilidades de eventos mutuamente excludentes); 4- mas é necessário reestimar o valor de \\(\\lambda\\) pois agora o intervalo de tempo é de 4 minutos e o valor que nos foi dado é para 1 minuto (o que é feito mediante uma simples regra de três: 5 chamadas em um minuto passam a ser 20 chamadas em quatro minutos) Probabilidade de não receber chamada alguma: \\[\\begin{align*} P(X=k) &amp; = \\frac{\\lambda ^{k}. \\epsilon^{-\\lambda}} {k!} \\\\ P(X=0) &amp; = \\frac{5^{0}. \\epsilon^{-5}} {0!} \\\\ P(X=0) &amp; = \\frac{1 . 0,00673}{1}\\\\ &amp; = 0,00673 \\end{align*}\\] Probabilidade de receber no máximo 2 chamadas em 4 minutos (\\(\\lambda = 20\\) chamadas por 4 minutos): \\[\\begin{align*} P(X=0) &amp; = \\frac{20^{0}. \\epsilon^{-20}} {0!} = 2,061154e-09 \\\\ P(X=1) &amp; = \\frac{20^{1}. \\epsilon^{-20}} {1!} = 4,122307e-08 \\\\ P(X=2) &amp; = \\frac{20^{2}. \\epsilon^{-20}} {2!} = 4,122307e-07 \\\\ \\end{align*}\\] \\(P(X \\le 2)=P(X=0)+P(X=1)+P(X=2)=4,554699e-7\\) # Defina o número de repetições (n) e o número de sucessos (k) k= 0 # Número de sucessos varia de nenhum (0) até infinito l= 5 # A probabilidade de sucesso na proporção dada (área, comprimento, tempo...) 5 chamadas por minuto k1= c(0,1,2) l1= 20 # 5 chamadas em 1 minuto &gt;&gt; 20 chamadas em 20 minutos # Probabilidade de k sucessos em n repetições (utilizando a função &#39;dbinom&#39;) probabilidade = dpois(k, lambda = l) # Neste exemplo, l=5 chamadas por minuto probabilidade1 = dpois(k1, lambda = l1) # Neste exemplo, l=20 chamadas por minuto # Crie uma tabela com duas colunas (Número de Sucessos e Probabilidade) tabela = data.frame(Número_de_Sucessos = k, Probabilidade = probabilidade) tabela1 = data.frame(Número_de_Sucessos = k1, Probabilidade = probabilidade1) # Exiba a tabela print(tabela) ## Número_de_Sucessos Probabilidade ## 1 0 0.006737947 print(tabela1) ## Número_de_Sucessos Probabilidade ## 1 0 2.061154e-09 ## 2 1 4.122307e-08 ## 3 2 4.122307e-07 Exemplo: Um posto de bombeiros recebe em média 3 chamadas por dia. Admitindo que as probabilidades associadas ao recebimento de diferentes números de chamadas podem ser modeladas por uma variável aleatória de Poisson qual seria a probabilidade desse posto receber 4 chamadas em 2 dias? A unidade da esperança dessa variável de Poisson (\\(\\lambda\\)) de chamadas nos foi dada por dia ao passo que a probabilidade pedida está associada a um período de dois dias, exigindo que a esperança \\(\\lambda\\) seja convertida para essa nova unidade (uma simples regra de três: 3 chamadas por dia, então para 2 dias, 6 chamadas). Assim, a probabilidade pedida será: \\[\\begin{align*} P(X=k) &amp; = \\frac{\\lambda ^{k}. \\epsilon^{-\\lambda}} {k!}\\\\ P(X=4) &amp; = \\frac{6^{4}. \\epsilon^{-6}} {4!} \\\\ &amp; = 0,1338 \\end{align*}\\] A figura abaixo ilustra a distribuição acumulada das probabilidades de alguns sucessos para o exemplo em estudo. library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ lubridate 1.9.4 ✔ tibble 3.2.1 ## ✔ purrr 1.0.4 ✔ tidyr 1.3.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::group_rows() masks kableExtra::group_rows() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors prob=c(0.00248, 0.01448, 0.044643, 0.08929, 0.1338, 0.16072, 0.16072, 0.137762, 0.256105) k=c(&quot;k=0&quot;, &quot;k=1&quot;, &quot;k=2&quot;, &quot;k=3&quot;, &quot;k=4&quot;, &quot;k=5&quot;, &quot;k=6&quot;, &quot;k=7&quot;, &quot;soma(k=8,k=9,...,inf)&quot;) legend_title=&quot;Sucessos&quot; nchamadas=data.frame(sucesso = k, proporcao= prob) nchamadas=nchamadas %&gt;% mutate(va_poisson = &quot;Probabilidades segundo o modelo teórico de Poisson&quot;) ggplot(nchamadas, aes(x = va_poisson, y =proporcao, fill = forcats::fct_rev(sucesso))) + geom_col( width = 0.2) + geom_text(aes(label = proporcao),size=3, position = position_stack(vjust = 0.5) ) + theme(legend.position = &quot;right&quot;) + ylab(&quot;Probabilidade acumulada&quot;) + xlab(NULL)+ scale_fill_discrete(name=&quot;Número de sucessos&quot;, labels=rev(c(&quot;k=0&quot;, &quot;k=1&quot;,&quot;k=2&quot;,&quot;k=3&quot;,&quot;k=4&quot;, &quot;k=5&quot;,&quot;k=6&quot;,&quot;k=7&quot;,&quot;soma(k=8,k=9,...,inf)&quot;))) Figure 6.2: Gráfico ilustrativo das probabilidades acumuladas Exemplo: Por um posto de pedágio passam, em média, 5 carros por minuto. Qual a probabilidade de passarem exatamente 3 carros em 1 minuto? \\[\\begin{align*} P(X=k) &amp; = \\frac{\\lambda ^{k}. \\epsilon^{-\\lambda}} {k!} \\\\ P(X=3) &amp; = \\frac{5^{3}. \\epsilon^{-5}} {3!} \\\\ &amp; = 0,1404 \\end{align*}\\] Uma variável aleatória discreta de Poisson modela muito bem eventos raros; ou seja, aqueles que não acontecem com grande frequência para qualquer intervalo considerado (tempo, extensão, área, volume). Trata-se de uma caso de variável Binomial no qual \\(n \\to \\infty\\) e \\(p\\) é pequeno (\\(n \\ge 50\\) e \\(n . p \\le (5)\\)). Nesse cenário pode-se demonstrar que: \\[ lim_{n \\to \\infty} P(X) = {C}_{k}^{n}. {p}^{k}. {q}^{n-k} \\] é igual a: \\[ P(X=k) = \\frac{\\lambda ^{k}. \\epsilon^{-\\lambda}} {k!} \\] Tal aproximação era, tempos atrás (antes da era computacional), bastante útil pois, para um \\(n\\) muito grande o cálculo fatorial era trabalhoso! Nesse contexto pode-se modelar o experimento acima, de modo bem aproximado, por uma variável aleatória de Poisson com \\(\\lambda=n . p\\): \\[ f(k) = P(X=k) = \\frac{n . p^{k}. \\epsilon^{- n . p}} {k!} \\] 6.1.5 Multinomial A distribuição multinomial é uma generalização da distribuição binomial - a qual admite apenas dois resultados: sucesso e fracasso - para as situações de mais de dois valores. Assim como a distribuição binomial, a distribuição multinomial é uma função de distribuição para processos discretos nos quais prevalecem probabilidades fixas para cada valor gerado de modo independente uns dos outros. Admita que \\(X\\) seja uma variável aleatória com distribuição multinomial: \\(X_{k} \\sim multinomial(n , \\boldsymbol{p})\\) que envolve um processo aleatório que possui um conjunto de \\(k\\) possíveis resultados, cada um com sua probabilidade \\(p_{k}\\) definida: \\[ X_{k} = \\begin{cases} X_{1} \\text{ com probabilidade } p_{1} \\\\ X_{2} \\text{ com probabilidade } p_{2} \\\\ \\vdots \\\\ X_{k} \\text{ com probabilidade } p_{k} \\\\ \\end{cases} \\] \\(X_{1}, X_{2}, \\dots, X_{k}\\) são os \\(k\\) possíveis resultados assumidos pela variável aleatória multinomial \\(X_{k}\\) ; \\(\\boldsymbol{p}\\) é o vetor de probabilidades \\(p_{1}, p_{2}, \\dots, p_{k}\\) associadas à ocorrência de cada um dos possíveis resultados da variável aleatória multinomial \\(X_{k}\\); \\(n\\) é o número finito de vezes que o experimento é realizado; \\(n_{1}, n_{2}, \\dots, n_{k}\\) é o número de sucessos observados em cada um dos possíveis resultados que a variável aleatória multinomial \\(X_{k}\\) pode assumir, de tal modo que \\(n_{1} + n_{2} + \\dots + n_{k} = n\\); as probabilidades de sucesso \\(p_{k}\\) em cada uma das repetições são sempre as mesmas: indepenendência de resultados entre as as repetições. A função de distribuição de probabilidades é dada por: \\[\\begin{align*} f(X=(n_{1}, n_{2}, \\dots, n_{k})) &amp; = P(X_{1}=n_{1}; X_{2}=n_{2}, \\dots, X_{k}=n_{k}) \\\\ P(X_{1}=n_{1}; X_{2}=n_{2}, \\dots, X_{k}=n_{k}) &amp; = \\frac{n!}{n_1!n_2!\\cdots n_{k}!}. p_1^{n_1} \\cdot p_2^{n_2} \\dots p_k^{n_k} \\end{align*}\\] A esperança e a variância de uma variável aleatória discreta com Distribuição de multinomial são dadas por: O vetor esperança: \\(E(X_{k}) = n.\\boldsymbol{p}_{k} = \\{n.p_{1};n.p_{2};\\cdots;n.p_{k}; \\}\\); O vetor variância: \\(Var(X_{k}) = n.\\boldsymbol{p}_{k}.(1-\\boldsymbol{p}_{k})= \\{n.p_{1}.(1-p_{1});n.p_{2}.(1-p_{2});\\cdots;n.p_{k}.(1-p_{k})\\}\\). Os valores do vetor da esperança numericamente calculados poderão ser arredondados se o objetivo incluir sua aplicação prática em algum contexto; caso contrário, permanecem como calculados foram. Exemplo: Em uma partida de xadrez dois jogadores têm a probabilidade de vencer, perder ou empatar. A probabilidade do jogador “A” vencer é 0,40, do jogador “B” vencer é 0,35 e do jogo terminar empatado é de 0,25. Calcule a probabilidade de em 12 partidas, observar-se que o jogador “A” ganhou 7 partidas, o jogador “B” venceu 2 partidas e as 3 partidas restantes terminaram em empate. Uma variável aleatória \\(X\\) pode ser definda para modelar a probabilidade dos diferentes resultados possíveis nesse experimento aleatório tal que \\(X_{3} \\sim multinomial (n, \\boldsymbol{p})\\) tal que: \\[ X_{3} = \\begin{cases} X_1\\text{: &quot;A&quot; ganha } \\text{probabilidade } p_{1}=0,40 \\\\ X_2\\text{: &quot;B&quot; ganha } \\text{probabilidade } p_{2}=0,35 \\\\ X_3\\text{: empate entre &quot;A&quot; e &quot;B&quot;} \\text{probabilidade } p_{3}=0,25 \\\\ \\end{cases} \\] \\[\\begin{align*} P(X_{1}=n_{1};X_{2}=n_{2};X_{3}=n_{3}) &amp; = \\frac{n!}{n_1!n_2!n_{3}!} \\cdot p_1^{n_1} \\cdot p_2^{n_2} \\cdot p_3^{n_3}\\\\ P(X_{1}=7;X_{2}=2;X_{3}=3) &amp; = \\frac{12!}{7!2!3!} \\cdot 0,40^{7} \\cdot 0,35^{2} \\cdot 0,25^{3}\\\\ P(X_{1}=7;X_{2}=2;X_{3}=3) &amp; = 0.02483712 \\end{align*}\\] # Defina as probabilidades de sucsesso de cada possível resultado da variável aleatória: p_(k). Admita que existam apenas tês: p1= 0.4 p2= 0.35 p3= 0.25 # O número de repetições &#39;n&#39; é automaticamente definido pelo arranjo de sucessos que se deseja estimar a probabilidade x=c(7,2,3) # Nesse caso deseja-se 7 sucessos do resultado X_1 (jogador 1), 2 do X_2 (jogador 2) e 3 do X_3 (jogador 3) # Probabilidade desse vetor de sucessos (utilizando a função &#39;dmultinom&#39;) probabilidade = dmultinom(x=c(7,2,3), prob = c(0.4,0.35,0.25)) print(probabilidade) ## [1] 0.02483712 "],["modelos-téoricos-do-tempo-de-espera.html", "6.2 Modelos téoricos do tempo de espera", " 6.2 Modelos téoricos do tempo de espera As distribuições do tempo de espera são outra importante classe de problemas associados com a quantidade de tempo que leva para a ocorrência de um evento específico de interesse. Dentro dessa classe de problemas se enquadram duas distribuições bastante conhecidas, são elas: geométrica e Geométrica negativa. 6.2.1 Geométrica A distribuição geométrica é uma distribuição de probabilidade discreta que modela o número de tentativas independentes necessárias para obter o primeiro sucesso em um processo de Bernoulli, onde cada tentativa tem duas possibilidades: sucesso ou fracasso e a probabilidade de sucesso é constante e denotada por \\(p\\) sob as seguintes condicionantes: 1- cada experimento é um ensaio de Bernoulli (só poderá haver dois resultados possíveis: sucesso ou fracasso); 2- cada repetição deve ter seu resultado independente do resuluado das demais; 3- a probabilidade de sucesso (\\(p\\)) é constante para todas as repetições; 4- consequentemente, a probabilidade de fracasso (\\(q=1-p\\)) também o é; e, 5- o experimento é repetido segue até que se verifique o primeiro sucesso.   Considere o experimento aelatório de se lançar uma moeda não honesta, com probabilidade \\(p\\) de ocorrência de Cara e \\((1-p)\\) de ocorrência de Coroa. Se definimos nosso evento de sucesso como sendo obter Cara no lançamento, quantos lançamentos serão necessários para se verificar a ocorrência de sucesso?   Admita uma sequência de \\(n\\) lançamentos: {Coroa, Coroa, …, Coroa, Cara} onde no \\(n-ésimo\\) lançamento verificou-se o sucesso. Assim sendo, podemos definir \\(j=(n-1)\\) como o número de tentativas anteriores fracassadas.   Uma variável aleatória \\(X\\) com Distribuição Geométrica, com parâmetro \\(p\\) (\\(0 \\le p \\le1\\)), é aquela que pode assumir infinitos valores numeráveis (\\(j=0,1,2, .s, \\infty\\)) para a quantidade \\(j\\) de tentativas que precedem o primeiro sucesso, que será observado na tentativa seguinte (\\(j+1\\)). Sua representação é \\(X\\sim geo(p)\\) e sua função de probabilidade é: \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) = p . (1-p)^{j} \\\\ &amp; = P(X=j) = p . q^{j} \\end{align*}\\] O Modelo geométrico pode ser escrito sub uma “forma alternativa”: o número de tentativas \\(n\\) até se observar o primeiro sucesso, agora com \\(x=n=1, 2, ...\\).}. \\[\\begin{align*} f(X=x; p) &amp; = P(X=n) = p . (1-p)^{(n-1)} \\\\ &amp; = P(X=n) = p . q^{(n-1)} \\end{align*}\\]   A esperança e a variância de uma variável aleatória discreta com Distribuição geométrica (\\(X\\sim geo(p)\\)) são: Esperança: \\(E(X) = \\frac{1}{p}\\) Variância: \\(Var(X) = \\frac{(1-p)}{p^{2}} = \\frac{q}{p^{2}}\\). A distribuição geométrica é frequentemente usada em situações em que estamos interessados em calcular quantas tentativas independentes são necessárias até que um evento específico ocorra. Por exemplo, pode ser usada para modelar o número de lançamentos de uma moeda justa até que a primeira cara apareça, ou o número de tentativas até que um cliente faça sua primeira compra em um site de comércio eletrônico. Lembrando que o modelo binomial é aplicado sobre a contagem de número de sucessos \\(k\\) em \\(n\\) tentativas de Bernoulli; ou seja, o número de tentativas \\(n\\) é fixo e o número de sucessos \\(k\\) é aleatório. Já o modelo geométrico estima o número de tentativas \\(j\\) até se observar o primeiro sucesso; isto é, o número de sucessos \\(k\\) é fixo e o número de tentativas \\(j\\) é aleatório. Uma variável aleatória geométrica é definida como o número de tentativas até que o primeiro sucesso fosse encontrado e, como essas tentativas são independentes entre si; ie., a probabilidade \\(p\\) não se altera em razão de terem sido realizadas tentativas anteriores, a contagem do número de tentativas até o próximo sucesso pode ser começada em qualquer tentativa sem alterar a distribuição de probabilidades da variável aleatória. A consequência de usar um modelo geométrico é que o sistema presumivelmente não será desgastado, a probabilidade permanece constante. Nesse sentido à distribuição geométrica é dita faltar qualquer memória. Exemplo: A probabilidade de que um bit transmitido através de um canal digital seja recebido com erro é de 0,1. Considere que as transmissões sejam eventos independentes e o erro relativamente raro. Uma variável aleatória discreta pode ser definida como \\(X\\sim Geo(p)\\). Qual a probabilidade de que o primeiro erro na transmissão de um bit ocorra na quinta transmissão? Uma variável aleatória discreta com Distribuição geométrica pode ser definida para modelar a probabilidade desse experiment aleatório como \\(X\\sim geo(p)\\), onde \\(p\\) é a probabilidade individual de sucesso (no nosso caso, que o bit seja transmitido com erro). Dados do problema: 1- a probabilidade de ocorrência de um sucesso (aqui bem entendido como sendo a transmissão de um bit com erro) é \\(p=0,1\\); e, 2- a probabilidade pedida é a de se observar a ocorrência do primeiro sucesso com 5 repetições (bem entendido aqui que o número de tentativas sem se observar sucesso será \\(j=4\\) e, em \\(j+1=5\\) teremos sucesso). \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) = (1-p)^{j} . p \\\\ P(X=4) &amp; = (1-0,1)^{4} . 0,1 \\\\ P(X=4) &amp; = 0,0656 \\end{align*}\\] A probabilidade de que na quinta transmissão de um bit ocorra um erro é de 6,56%. p = 0.10 n = 4 # Uma vez que na 5 repetição ocorrerá o &#39;sucesso&#39; (o bit será transmitido com erro) dgeom(x = n, prob = p) ## [1] 0.06561 Exemplo: Uma linha de produção está sendo analisada para fins de controle da qualidade das peças produzidas. Tendo em vista o alto padrão requerido, a produção é interrompida para regulagem toda vez que uma peça defeituosa é observada. Se 0,01 é a probabilidade da peça ser defeituosa, determine a probabilidade de ocorrer uma peça defeituosa entre a \\(4^{a}\\) e \\(6^{a}\\) peças produzidas.   Uma variável aleatória discreta com Distribuição geométrica pode ser definida para modelar esse experimento aleatório como \\(X\\sim Geo(p)\\) onde \\(p\\) é a probabilidade individual de sucesso (no caso, a produção de uma peça defeituosa). Pede-se a probabiidade de que essa ocorrência se verifique OU na quarta OU na quinta OU na setxa peça produzida. Dados do problema: a probabilidade de ocorrência de um sucesso (aqui bem entendido como sendo a produção de uma peça defeituosa) é \\(p=0,01\\); e, a probabilidade pedida é a de se observar a ocorrência da produção da primeira peça defeituosa com 4, 5 OU 6 repetições. Assim sendo o número de tentativas sem se ter nenhuma peça produzida com defeito é de \\(3 \\le j \\le 5\\) porque assim, em \\(j+1\\), teremos sucesso na quarta, quinta ou sexta peça produzidas. Considerando-se que os eventos são disjuntos (ocorrerá na quarta, na quinta ou na sexta), probabilidade pedida será: \\[ P(X=j)_{3 \\le j \\le 5}= P(X=3) + P(X=4) + P(X=5) \\] A probabilidade de verificarnos sucesso na \\(4^{a}\\) peça produzida (peça produzida com defeito) será: \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=3) &amp; = (1-0,01)^{3} . 0,01 \\\\ P(X=3) &amp; = 0.00970299 \\end{align*}\\]   A probabilidade de verificarnos sucesso na \\(5^{a}\\) peça produzida (peça produzida com defeito) será: \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=4) &amp; = (1-0,01)^{4} . 0,01 \\\\ P(X=4) &amp; = 0.00960596 \\end{align*}\\]   A probabilidade de verificarnos sucesso na \\(6^{a}\\) peça produzida (peça produzida com defeito) será: \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{k} . p \\\\ P(X=5) &amp; = (1-0,01)^{5} . 0,01 \\\\ P(X=5) &amp; = 0.0095099 \\end{align*}\\] A probabilidade de termos uma peça produzida com defeito na quarta OU na quinta OU na sexta das peças produzidas será: \\[\\begin{align*} P(3 \\le j \\le 5) &amp; = P(X=3) + P(X=4) + P(X=5) \\\\ P(3 \\le j \\le 5) &amp; = 0.00970299 + 0.00960596 + 0.00950990\\\\ P(3 \\le j \\le 5) &amp; = 0.02881885 \\end{align*}\\] A probabilidade de termos uma peça defeituosa na quarta OU na quinta OU na sexta das peças produzidas é de 2,9116%. p = 0.01 n = c(3,4,5) # Uma vez que na 4, 5 e 6 repetições ocorrerão os &#39;sucessos&#39; (a produção de uma peça defeituosa) dgeom(x = n, prob = p) ## [1] 0.00970299 0.00960596 0.00950990 Exemplo 9 A probabilidade de um alinhamento ótico bem sucedido na montagem de produto de armazenamento de dados é de 0,80. Assuma que as tentativas são independentes e responda: 1- Qual é a probabilidade de que o primeiro alinhamento bem sucedido requeira exatamente quatro tentativas? 2- Qual é a probabilidade de que o primeiro alinhamento bem sucedido requeira no máximo quatro tentativas? 3- Qual é a probabilidade de que o primeiro alinhamento bem sucedido requeira ao menos quatro tentativas? Uma variável aleatória discreta com Distribuição geométrica pode ser definida para modelar esse experimento aleatório como \\(X\\sim Geo(p)\\) onde \\(p\\) é a probabilidade individual de sucesso . Dados do problema: a probabilidade de ocorrência de um sucesso (alinhamento ótico bem sucedido na montagem de produto de armazenamento de dados) é \\(p=0,80\\); o item (1) pede a probabilidade de verificar o primeiro sucesso com exatamente quatro repetições; assim, o número de tentativas sem se observar sucesso é \\(j=3\\) (em \\(j+1=4\\) verifica-se sucesso); o item (2) pede a probabilidade de se verificar o primeiro sucesso com no máximo quatro repetições; assim, o número de tentativas sem se observar sucesso é de \\(0 \\le j \\le 3\\) (em \\(j+1\\) teremos sucesso: no primeiro OU no segundo OU no terceiro OU no quarto alinhamentos realizados); e, o item (3) pede a probabilidade de se observar o primeiro sucesso com no mínimo quatro repetições; assim, o número de tentativas sem se observar sucesso é de $3 j $ (em \\(j+1\\) teremos sucesso: no quarto OU* no quinto OU** sexto .s, alinhamentos realizados).   Para o item (1) a probabilidade de termos a ocorrência de um sucesso (ou seja, um alinhamento ótico bem sucedido) na \\(4^{a}\\) montagem será: \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=3) &amp; = (1-0,80)^{3} . 0,80 \\\\ P(X=3) &amp; = 0,0064 \\end{align*}\\] p = 0.80 n = c(3) # Uma vez que na 4 repetição ocorrerá o &#39;sucesso&#39; (um alinhamento ótico bem sucedido) dgeom(x = n, prob = p) ## [1] 0.0064 Para o item (2) considerando-se que as repetições são independentes, a probabilidade pedida será: \\[ P(X=j)_{0 \\le j \\le 3} = P(X=0) + P(X=1) + P(X=2) + P(X=3) \\] \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=0) &amp; = (1-0,80)^{0} . 0,80 \\\\ P(X=0) &amp; = 0,80 \\end{align*}\\] \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=1) &amp; = (1-0,80)^{1} . 0,80 \\\\ P(X=1) &amp; = 0,16 \\end{align*}\\] \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=2) &amp; = (1-0,80)^{2} . 0,80 \\\\ P(X=2) &amp; = 0,032 \\end{align*}\\] \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=3) &amp; = (1-0,80)^{3} . 0,80 \\\\ P(X=3) &amp; = 0,0064 \\end{align*}\\] A probabilidade pedida é de: \\[\\begin{align*} P(X=j)_{0 \\le j \\le 3} &amp; = P(X=0) + P(X=1) + P(X=2) + P(X=3) \\\\ P(X=j)_{0 \\le j \\le 3} &amp; = 0,9984 \\end{align*}\\] p = 0.80 n = c(0,1,2,3) # Uma vez que na 1, 2, 3, e 4 repetições ocorrerão os &#39;sucessos&#39; (alinhamentos óticos bem sucedidos) dgeom(x = n, prob = p) ## [1] 0.8000 0.1600 0.0320 0.0064 Para o item (3) considerando-se que os eventos pedidos são disjuntos a probabilidade pedida deverá ser calculada a partir do complemento da probabilidade total menos os eventos que não são de interesse: \\[ P(X=j)_{3 \\le j \\le \\infty} = 1 - P(X=0) + P(X=1) + P(X=2) \\] \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=0) &amp; = (1-0,80)^{0} . 0,80 \\\\ P(X=0) &amp; = 0,80 \\end{align*}\\] \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=1) &amp; = (1-0,80)^{1} . 0,80 \\\\ P(X=1) &amp; = 0,16 \\end{align*}\\] \\[\\begin{align*} f(X=x; p) &amp; = P(X=j) \\\\ P(X=j) &amp; = (1-p)^{j} . p \\\\ P(X=2) &amp; = (1-0,80)^{2} . 0,80 \\\\ P(X=2) &amp; = 0,032 \\end{align*}\\] A probabilidade é de: \\[\\begin{align*} P(X=j)_{3 \\le j \\le \\infty} &amp; = 1 - P(X=0) + P(X=1) + P(X=2) \\\\ P(X=j)_{3 \\le j \\le \\infty} &amp; = 1 - (0,80 + 0,16 + 0,032) \\\\ P(X=j)_{3 \\le j \\le \\infty} &amp; = 0,008 \\end{align*}\\] 6.2.2 Binomial Negativa A distribuição Binomial Negativa (também conhecida como de Distribuição de Pascal em homenagem ao matemático francês Blaise Pascal) pode ser considerada como uma generalização da variável Geométrica, na qual agora considera-se a situação em que se modelam as probabilidades de se verificar mais de um evento de sucesso em um certo número de repetições. Ao se realizar repetidos experimentos de Bernoulli, uma variável aleatória Binomial Negativa modela as probabilidades de serem observados \\(k\\) sucessos em \\(n\\) repetições. Um experimento que apresenta uma distribuição Binomial Negativa satisfaz aos seguintes pressupostos: 1- cada repetição é um ensaio de Bernoulli (só poderá haver dois resultados possíveis: sucesso ou fracasso); 2- cada repetição não altera a probabilidade original (há independência entre as repetições); 3- portanto, a probabilidade de sucesso (\\(p\\)) em cada repetição é constante; 4- e, consequentemente, a probabilidade de fracasso (\\(q=1-p\\)) em cada repetição também é constante; e, 5- o experimento aleatório prossegue até que sejam verificados \\(k\\) sucessos (na última repetição terá sido observado o k-ésimo sucesso). A notação de uma variável aleatória Binomial Negativa é \\(X\\sim bn(p,k)\\), onde o parâmetro \\(p\\) (\\(0 \\le p \\le1\\)) indica a probabilidade individual de sucesso a cada repetição de Bernoulli e \\(k\\) o número total de sucessos desejado e estabelecido a priori. Sua função discreta de probabilidade, que calcula a probabilidade de se observar o número \\(k\\) sucessos estabelecido a priori em \\(n\\) de ensaios de Bernoulli realizados, é a seguinte: \\[\\begin{align*} f(X=n; (p, k)) &amp; = P(X=n; (p,k) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{(n-k)} \\\\ &amp; = \\frac{(n-1)!}{ (k-1)! \\cdot (n-k)!} \\cdot {p}^{k} \\cdot {q}^{(n-k)} \\end{align*}\\] Como são necessários no mínimo \\(k\\) tentativas para se obter \\(k\\) sucessos, os valores de \\(n={k, k+1, k+2 ...}\\)). A esperança e a variância de uma variável aleatória discreta com Distribuição Binomial Negativa são: Esperança: \\(E(X) = \\frac{k}{p}\\) ; Variância: \\(Var(X) = \\frac{k \\cdot (1-p)}{p^{2}} = \\frac{q \\cdot k}{p^{2}}\\). Uma variável aleatória Binomial é uma contagem de número de sucessos \\(k\\) em \\(n\\) tentativas de Bernoulli; ou seja, o número de tentativas \\(n\\) é predeterminado (fixo) e o número de sucessos \\(k\\) é a variação aleatória. Em \\(n\\) tentativas a probabilidade de se observar \\(k\\) sucessos é calculada pela sua função de distribuição discreta de probabilidades. Uma variável aleatória Binomial Negativa é uma contagem do número de tentativas \\(n\\) de Bernoulli em \\(k\\) sucessos; ou seja, o número de sucessos \\(k\\) é predeterminado (fixo) e o número de tentativas é a variação aleatória. Em \\(n\\) tentativas a probabilidade de se observar \\(k\\) sucessos é calculada pela sua função de distribuição discreta de probabilidades. Exemplo 10: A probabilidade com que um bit transmitido através de um canal digital de transmissão seja recebido com erro é de 0,1 e que as transmissões sejam eventos independentes. Qual a probabilidade de que nas dez primeiras transmissões ocorram quatro erros? Uma variável aleatória discreta com Distribuição Binomial Negativa pode ser definida para modelar esse experimento aleatório, tal que \\(X\\sim bn(p,k)\\) onde \\(p\\) é a probabilidade individual de sucesso e \\(k\\) o número de sucessos estabelecido a priori. Dados do problema: 1- a probabilidade de ocorrência de um sucesso (aqui bem entendido como sendo a recepção errada de um bit transmitido) é \\(p=0,1\\); e, 2- o número de sucessos (aqui bem entendido como sendo a recepção errada de um bit transmitido) está definido em \\(k=4\\). Pede-se a probabilidade de se observar quatro sucessos (\\(k=4\\)) em dez (\\(n=10\\)) transmissões (repetições). A probabilidade de se observar \\(k=4\\) sucessos ao se realizar \\(n=10\\) tentativas de Bernoulli é dada pela função discreta de probabilidade da variável aleatória Binomial Negativa: \\[\\begin{align*} f(X=n; (p, k)) &amp; = P(X=10; (p=0,1; k=4)) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(n-1)!}{ (k-1)! \\cdot (n-k)!} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(10-1)!}{ (4-1)! \\cdot (10-4)!} \\cdot {0,1}^{4} \\cdot {0,9}^{10-4} \\\\ &amp; = \\frac{362880}{ 6 \\cdot 720} \\cdot 0.0001 \\cdot 0.531441 \\\\ &amp; = 0,004464104 \\end{align*}\\] A probabilidade de se observar 4 sucessos em 10 tentativas é de 0,4464104%. k = 4 n = 10 p = 0.10 x = n-k # Entrar com n-k na função pois ela espera o número de falhas dnbinom(x = x , size = k, prob = p) ## [1] 0.004464104 Exemplo 11: Bob é um jogador de basquete de uma escola. Ele é um lançador de arremessos livres e sua probabilidade de acertar é igual a 70%. Durante uma partida qualquer, qual a probabilidade de que Bob acerte seu terceiro arremesso livre na seu quinta tentativa? Uma variável aleatória discreta com Distribuição Binomial Negativa pode ser definida para modelar esse experimento aleatório tal que \\(X\\sim bn(p,k)\\) onde \\(p\\) é a probabilidade individual de sucesso e \\(k\\) o número de sucessos estabelecido a priori sob probabilidade constante e igual a p a cada repetição (p, k são os parâmetros do modelo). Dados do problema: 1- a probabilidade de ocorrência de um sucesso é \\(p=0,70\\), e 2- o número de sucessos =3$. Pede-se a probabilidade de se observar três sucessos (\\(k=3\\)) em cinco (\\(n=5\\)) arremessos(repetições). A probabilidade de se obter \\(k=3\\) sucessos ao se realizar \\(n=5\\) tentativas é dada pela função discreta de probabilidade da variável Binomial Negativa: \\[\\begin{align*} f(X=n; (p; r)) &amp; = P(X=5; (p=0,7; k=3 )) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(n-1)!}{ (k-1)! \\cdot (n-k)!} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(5-1)!}{ (3-1)! \\cdot (5-3)!} \\cdot {0,7}^{3} \\cdot {0,3}^{5-3} \\\\ &amp; = \\frac{24}{ 2 \\cdot 2} \\cdot 0.343 \\cdot 0.09 \\\\ &amp; = 0,18522 \\end{align*}\\] A probabilidade de Bob acertar 3 arremessos em 5 tentativas é de 18,522%. k = 3 n = 5 p = 0.70 x = n-k # Entrar com n-k na função pois ela espera o número de falhas dnbinom(x = x , size = k, prob = p) ## [1] 0.18522 Exemplo 12: Lançamos repetidas vezes uma moeda. Seja \\(X\\) o número de caras até que consigamos sete coroas. Qual é a probabilidade de que o número de caras seja igual a cinco até que consigamos as sete coroas? Uma variável aleatória discreta com Distribuição Binomial Negativa pode ser definida para modelar esse experimento aleatório tal que \\(X\\sim bn(p,k)\\) onde \\(p\\) é a probabilidade individual de sucesso e \\(k\\) o número de sucessos estabelecido a priori sob probabilidade constante e igual a p a cada repetição (p, k são os parâmetros do modelo).   Dados do problema: a probabilidade de ocorrência de um sucesso é \\(p=0,5\\), e, o número de sucessos é \\(k=7\\). Pede-se a probabilidade de se observar sete sucessos (sete Caras) em doze tentativas (sete Caras e cinco Coroas). A probabilidade de se obter \\(k=7\\) sucessos ao se realizar \\(n=12\\) tentativas é dada pela função discreta de probabilidade da variável Binomial Negativa: \\[\\begin{align*} f(X=n; p; r) &amp; = P(X=12; p=0.5; k=7 ) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(n-1)!}{ (r-1)! \\cdot (n-k)!} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(12-1)!}{ (7-1)! \\cdot (12-7)!} \\cdot {0,5}^{7} \\cdot {0,5}^{12-7} \\\\ &amp; = \\frac{39916800}{ 720 \\cdot 120} \\cdot 0.0078125 \\cdot 0.03125 \\\\ &amp; = 462 \\cdot 0.0078125 \\cdot 0.03125 \\\\ &amp; = 0.112793 \\end{align*}\\] A probabilidade de se obter 7 sucessos em 12 tentativas é de 11,28%. k = 7 n = 12 p = 0.50 x = n-k # Entrar com n-k na função pois ela espera o número de falhas dnbinom(x = x , size = k, prob = p) ## [1] 0.112793 Exemplo 13: Considere o tempo para recarregar o flash de uma câmera de celular. Assuma que a probabilidade de que uma câmera instalada no celular durante sua montagem passe no teste seja de 0.80 e que cada câmera é montada de modo que a probabilidade não se altere (independência). Determine as seguintes probabilidades: 1- de que a segunda falha ocorra na décima câmera testada; 2- de que a segunda falha ocorra no teste de quatro ou menos câmeras; e, 3- o valor esperado do número de câmeras testadas para obter a terceira falha. Uma variável aleatória discreta com Distribuição Binomial Negativa pode ser definida para modelar esse experimento aleatório tal que \\(X\\sim bn(p,k)\\) onde \\(p\\) é a probabilidade individual de sucesso e \\(k\\) o número de sucessos estabelecido a priori sob probabilidade constante e igual a p a cada repetição (p, k são os parâmetros do modelo). Dados do problema: se a probabilidade de que a câmera montada no celular passe no teste é \\(0,80\\) a probabilidade de não passar é de (\\(1-0,80\\)) \\(=0,20\\); fica bem entendido aqui que o sucesso é a câmera montada no celular não passar no teste, logo \\(p=0,20\\); no item (1) pede-se a probabilidade de se observar um número de sucessos fixado a priori \\(k=2\\) em \\(n=10\\) câmeras testadas (repetições de Bernoulli); no item (2) pede-se a probabilidade de se observar um número de sucessos também fixado a priori em \\(k=2\\) mas agora no intervalo de \\(n \\le 4\\) câmeras testadas (repetições de Bernoulli); e, o valor esperado para o número de câmeras testadas (repetições de Bernoulli) (\\(n=?\\)) para que se observ \\(k=3\\) sucessos. A probabilidade de se observar \\(k=2\\) sucessos ao se realizar \\(n=10\\) tentativas de Berboulli é dada pela função discreta de probabilidade da variável Binomial Negativa: / \\[\\begin{align*} f(X=n; (p,k)) &amp; = P(X=10; (p=0.2; k=2) ) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(n-1)!}{ (k-1)! \\cdot (n-k)!} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(10-1)!}{ (2-1)! \\cdot (10-2)!} \\cdot {0,2}^{2} \\cdot {0,8}^{10-2} \\\\ &amp; = \\frac{362880}{ 1 \\cdot 40320} \\cdot 0.04 \\cdot 0.1677722 \\\\ &amp; = 9 \\cdot 0.04 \\cdot 0.1677722 \\\\ &amp; = 0.06039799 \\end{align*}\\] A probabilidade de se observar \\(k=2\\) sucessos em \\(n=10\\) tentativas de Bernoulli é de 6,039%. As probabilidades de se observar \\(k=2\\) sucessos ao serem realizadas \\(n \\le 4\\) tentativas é expressa por $P(X=2) P(X=3) P(X=4)=P(X=2)+P(X=3)+P(X=4)), dadas pela função discreta de probabilidade da variável Binomial Negativa aplicada a: \\[\\begin{align*} f(X=n; (p,k)) &amp; = P(X=2; (p=0.2; k=2) ) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(2-1)!}{ (2-1)! \\cdot (2-2)!} \\cdot {0,2}^{2} \\cdot {0,8}^{2-2} \\\\ P(X=2) &amp; = 0,04 \\end{align*}\\] \\[\\begin{align*} f(X=n; (p,k))) &amp; = P(X=3; (p=0.2; k=2)) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(3-1)!}{ (2-1)! \\cdot (3-2)!} \\cdot {0,2}^{2} \\cdot {0,8}^{3-2} \\\\ P(X=3) &amp; = 0,064 \\end{align*}\\] \\[\\begin{align*} f(X=n; (p,k)) &amp; = P(X=4; (p=0.2; k=2) ) = \\binom{n-1}{k-1} \\cdot {p}^{k} \\cdot {q}^{n-k} \\\\ &amp; = \\frac{(4-1)!}{ (2-1)! \\cdot (4-2)!} \\cdot {0,2}^{2} \\cdot {0,8}^{4-2} \\\\ P(X=4) &amp; = 0,0768 \\end{align*}\\] A probabilidade de se obter \\(r=2\\) sucessos em \\(n \\le 4\\) tentativas é de (\\(0,04+0,064+0,0768\\)) 18,08%. O valor esperado (esperança) do número de câmeras testadas para que se observem \\(k=3\\) sucessos é dado \\[\\begin{align*} E(X) &amp; = \\frac{k}{p} \\\\ E(X) &amp; = \\frac{3}{0,2} \\\\ E(X) &amp; =15 \\end{align*}\\] O valor esperado (esperança) do número \\(n\\) de câmeras testadas para que se observem \\(r=3\\) sucessos é 15 "],["modelos-teóricos-de-probabilidade-para-variáveis-aleatórias-contínuas.html", "6.3 Modelos teóricos de probabilidade para variáveis aleatórias contínuas", " 6.3 Modelos teóricos de probabilidade para variáveis aleatórias contínuas Experimentos aleatórios nos quais os possíveis resultados assumem valores resultantes de processos de mensuração tais como, por exemplo, rendas, pesos, velocidades, tempos, comprimentos, pertencentes aos números Reais, podem ser adequadamente modelados por variáveis aleatórias contínuas. 6.3.1 Uniforme A Distribuição Uniforme é uma das distribuições contínuas mais simples de toda a Estatística. Ela se caracteriza por ter uma função densidade contínua em um intervalo fechado \\([a,b]\\). Ou seja, a probabilidade de ocorrência de um certo valor é sempre a mesma. Embora as aplicações desta distribuição não sejam tão abundantes quanto as demais distribuições que discutiremos mais adiante, utilizaremos a Distribuição Uniforme para introduzirmos as funções contínuas e darmos uma noção de como se utiliza a função densidade para determinarmos probabilidades, esperanças e variâncias. Uma variável aleatória \\(X\\) tem Distribuição Uniforme no intervalo \\([a,b]\\), com notação \\(X \\sim U (a, b)\\), se sua função densidade de probabilidade for dada por: \\[ f(X=x)= \\begin{cases} \\frac{1}{b-a}, \\hspace{0.6cm} \\text{para } a \\le x \\le b \\\\ 0, \\hspace{1cm} \\text{para qualquer outro x}\\\\ \\end{cases} \\] A esperança e a variância de uma variável aleatória contínua com Distribuição Uniforme são: Esperança: \\(E(X) = \\frac{(a+b)}{2}\\); e, Variância: \\(Var(X) = \\frac{(b-a)^{2} }{12}\\). A probabilidade para um intervalo \\([c,d]\\) tal que \\(a \\le c &lt; d \\le b\\) será dada por: \\[ \\int_c^d \\frac{1}{(b - a)} \\, dx \\\\ \\frac{1}{(b - a)} \\int_c^d 1 \\, dx \\\\ \\frac{1}{(b - a)} |^{d}_{c} \\\\ \\frac{(d - c)}{(b - a)} \\] 6.3.2 Exponencial   A Distribuição Exponencial é largamente utilizada nas áreas de engenharia, física, computação e biologia para modelar variáveis tais como vida útil de equipamentos, tempos entre falhas (\\(TBF\\)), tempos de sobrevivência de espécies, intervalos de solicitação de recursos por exemplo.   Esta é uma distribuição que se caracteriza por ter uma função de taxa de falha constante, a única com esta propriedade e por essa razão tem tem sido usada extensivamente como um modelo para o tempo de vida de certos produtos e materiais. Uma variável aleatória contínua \\(X\\) que assume valores não negativos segue o modelo teórico Exponencial com parâmetro (taxa) \\(\\lambda\\): \\(X \\sim Exp (\\lambda)\\) . Sua função densidade de probabilidade é dada por: \\[ f(X=x)= \\begin{cases} \\lambda \\cdot \\varepsilon ^{-\\lambda \\cdot x} \\text{, para } x \\ge 0 \\\\ 0 \\text{, para } x &lt; 0\\\\ \\end{cases} \\] Alternativamente com parâmetro (escala): \\(\\alpha=\\frac{1}{\\lambda}\\) e sua sua densidade de probabilidade é dada por: \\[ f(X=x)= \\begin{cases} \\frac{1}{\\alpha} \\cdot \\varepsilon ^{-\\frac{1}{\\alpha} \\cdot x} \\text{, para } x \\ge 0 \\\\ 0 \\text{, para } x &lt; 0\\\\ \\end{cases} \\] Para se calcular probabilidades de uma Distribuição Exponencial torna-se necessária a resolução da integral associada, posto que a análise simplificada de figuras geométricas não mais é possível.   De modo geral temos: \\[\\begin{align*} P( a &lt; X &lt; b) &amp; = \\int_{a}^{b} \\lambda \\cdot \\varepsilon ^{- \\lambda \\cdot x} dx \\\\ P( a &lt; X &lt; b) &amp; = - \\varepsilon^{-\\lambda \\cdot x} \\rvert_{a}^{b} \\\\ P( a &lt; X &lt; b) &amp; = \\varepsilon^{-\\lambda \\cdot a} - \\varepsilon^{-\\lambda \\cdot b} \\\\ \\end{align*}\\] Sua esperança e a variância são: Esperança: \\(E(X) = \\mu = \\frac{1}{\\lambda}=\\alpha\\); e, Variância: \\(Var(X) = \\frac{1}{\\lambda^{2}}=\\alpha^{2}\\). Exemplo: Uma indústria fabrica lâmpadas especiais que ficam em operação continuamente. A empresa oferece a seus clientes a garantia de reposição, caso a lâmpada dure menos de 50 horas. A vida útil dessas lâmpadas pode ser modelada adequadamente através da distribuição Exponencial com parâmetro \\(\\lambda = \\frac{1}{8000}\\). Determine a probabilidade de uma lâmpada necessitar ser trocada pela indústria em razão da garantia oferecida ao cliente. Definindo a variável aleatória contínua \\(T\\) como sendo a vida útil da lâmpada: \\(T \\sim Exp (\\frac{1}{8000})\\) e sua função densidade de probabilidade: \\[ f(T=t)= \\begin{cases} \\frac{1}{8000} \\cdot \\varepsilon ^{- \\frac{1}{8000} \\cdot t} \\text{, para } t \\ge 0 \\\\ 0 \\text{, para } x &lt; 0\\\\ \\end{cases} \\] A probabilidade de que uma lâmpada tenha uma vida útil menor que 50 horas será dada pela integral da fdp no intervalo [0;50]: \\[\\begin{align*} P( 0 &lt; T &lt; 50) &amp; = \\int_{0}^{50} \\lambda \\cdot \\varepsilon ^{- \\lambda \\cdot x} dx \\\\ P( 0 &lt; T &lt; 50) &amp; = - \\varepsilon^{-\\lambda \\cdot x} \\rvert_{0}^{50} \\\\ P( 0 &lt; T &lt; 50) &amp; = \\varepsilon^{- \\frac{1}{8000} \\cdot 0} - \\varepsilon^{- \\frac{1}{8000} \\cdot 50} \\\\ P( 0 &lt; T &lt; 50) &amp; = 1-0,939413063 \\\\ &amp; = 0,006 \\\\ \\end{align*}\\] A probabilidade de que uma lâmpada fabricada por essa empresa tenha uma vida útil menor que 50 h é de 0,006 (proporção de 0,60%), naturalmente muito pequena considerando que a duração média das lâmpadas é de \\(\\mu = \\frac{1}{\\lambda} =\\frac{1}{\\frac{1}{8000}}=8000\\) h, aproximadamente 333 dias (esperança da variável). # Biblioteca necessária library(ggplot2) # Parâmetro lambda (inverso da esperança) lambda &lt;- 1/8000 # horas # A esperança é de 8000 horas ~ 1 ano # Faixa de valores para mostrar a curvatura suave x_values &lt;- seq(0, 50000, length.out = 50) # Função densidade de probabilidade exponencial: f(x) = lambda * exp (-lambda*x) # para lambda maior que zero &gt; X ~ Exp (lambda) density_values &lt;- dexp(x_values, rate = lambda) # Pontos plot_data &lt;- data.frame(x = x_values, density = density_values) # Gráfico plot &lt;- ggplot(plot_data, aes(x, density)) + geom_line(color = &quot;blue&quot;, size = 1) + theme_minimal() + labs(title = &quot;Função densidade de probabilidade exponencial&quot;, x = &quot;Variável aleatória: vida útil das lâmpadas (h)&quot;, y = &quot;Densidade&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. # Plote para mostrar a curvatura suave plot # Para destacar a área de integração (0 a 50) x_values &lt;- seq(0, 1000, length.out = 50) # Função densidade de probabilidade exponencial: f(x) = lambda * exp (-lambda*x) # para lambda maior que zero # X ~ Exp (lambda) density_values &lt;- dexp(x_values, rate = lambda) # Pontos plot_data &lt;- data.frame(x = x_values, density = density_values) # Gráfico plot &lt;- ggplot(plot_data, aes(x, density)) + geom_line(color = &quot;blue&quot;, size = 1) + theme_minimal() + labs(title = &quot;Função densidade de probabilidade exponencial&quot;, x = &quot;Variável aleatória: vida útil das lâmpadas (h)&quot;, y = &quot;Densidade&quot;) highlight_values &lt;- c(0, 50) plot &lt;- plot + geom_ribbon(data = subset(plot_data, x &gt;= highlight_values[1] &amp; x &lt;= highlight_values[2]), aes(x = x, ymin = 0, ymax = density), fill = &quot;yellow&quot;, alpha = 0.5) plot # Valores do intervalo # 0.006230509 para [0,50] # 0.9937695 para [50 , inf]. 1 - Prob[0,50] valores &lt;- c(0, 50) #a integrar lambda &lt;- 1/8000 # horas # Integração numérica probability &lt;- integrate(function(x) dexp(x, rate = lambda), lower = valores[1], upper = valores[2])$value # Valor cat(&quot;Probabilidade de se observar valores entre &quot;, valores[1], &quot;e&quot;, valores[2], &quot;é :&quot;, probability, &quot;\\n&quot;) ## Probabilidade de se observar valores entre 0 e 50 é : 0.006230509 Exemplo: O intervalo de tempo (minutos) entre as emissões de uma fonte radioativa é uma variável aleatória contínua que pode ser modelada pela Distribuição Exponencial com parâmetro \\(\\lambda=0,20\\). Calcule a probabilidade de haver uma emissão em um intervalo de tempo inferior a 2 minutos. Definindo a variável aleatória contínua \\(T\\) como sendo o intervalo de tempo entre as emissões radioativas dessa fonte: \\(T \\sim Exp (0,20)\\) e sua função densidade de probabilidade: \\[ f(T=t)= \\begin{cases} 0,20 \\cdot \\varepsilon ^{- 0,20\\cdot t} \\text{, para } t \\ge 0 \\\\ 0 \\text{, para } x &lt; 0\\\\ \\end{cases} \\] A probabilidade de uma emissão em um intervalo de tempo inferior a 2 minutos será dada pela integral da fdp no intervalo [0;2]: \\[\\begin{align*} P( 0 &lt; T &lt; 2) &amp; = \\int_{0}^{2} \\lambda \\cdot \\varepsilon ^{- \\lambda \\cdot x} dx \\\\ P( 0 &lt; T &lt; 2) &amp; = - \\varepsilon^{-\\lambda \\cdot x} \\rvert_{0}^{2} \\\\ P( 0 &lt; T &lt; 2) &amp; = \\varepsilon^{- 0,20 \\cdot 0} - \\varepsilon^{- 0,20 \\cdot 2} \\\\ P( 0 &lt; T &lt; 2) &amp; = 1 - 0,6703 \\\\ &amp; = 0,3296 \\end{align*}\\] A probabilidade de uma emissão em um intervalo de tempo inferior a 2 min é de 0,3296, naturalmente considerável uma vez que o intervalo médio entre as emissões radioativas é de \\(\\mu = \\frac{1}{\\lambda}=\\frac{1}{0,20}= 5\\) min (esperança da variável). valores &lt;- c(0, 2) #a integrar lambda &lt;- 0.2 # lambda já foi dado # Integração numérica probability &lt;- integrate(function(x) dexp(x, rate = lambda), lower = valores[1], upper = valores[2])$value # Valor cat(&quot;Probabilidade de se observar valores entre &quot;, valores[1], &quot;e&quot;, valores[2], &quot;é :&quot;, probability, &quot;\\n&quot;) ## Probabilidade de se observar valores entre 0 e 2 é : 0.32968 Exemplo: Certo tipo de fusível elétrico tem duração de vida (horas) que segue uma Distribuição Exponencial com tempo médio de vida de 100 horas. Cada peça tem um custo de R$ 10,00 e, se durar menos de 200 horas, existe um custo adicional de R$ 8,00. Pede-se: - a probabilidade de fusível durar mais de 150 horas; e, - o custo esperado. Se a vida útil média (\\(\\mu\\)) desse fusível é de 100 horas, então o valor do parâmetro dessa distribuição será \\(\\frac{1}{100}\\) (pois \\(\\mu=\\frac{1}{\\lambda}\\)) e a variável aleatória contínua \\(T\\) será definida como sendo a vida útil do fusível: \\(T \\sim Exp (\\frac{1}{100})\\), com sua função densidade de probabilidade: \\[ f(T=t)= \\begin{cases} \\frac{1}{100} \\cdot \\varepsilon ^{- \\frac{1}{100} \\cdot t} \\text{, para } t \\ge 0 \\\\ 0 \\text{, para } x &lt; 0\\\\ \\end{cases} \\] O primeiro item pede a probabilidade de um fusível durar mais de 150 horas poderá ser dada por 1 menos o valor da integral da fdp no intervalo [0;150]: \\[\\begin{align*} P( T &gt; 150) &amp; = 1 - P(0&lt;T&lt;150) \\\\ &amp; = 1- \\int_{0}^{150} \\lambda \\cdot \\varepsilon ^{- \\lambda \\cdot x} dx \\\\ &amp; = 1 - \\varepsilon^{-\\lambda \\cdot x} \\rvert_{0}^{150} \\\\ &amp; = 1 - (\\varepsilon^{- 0,01 \\cdot 0} - \\varepsilon^{-0,01 \\cdot 150}) \\\\ &amp; = 1 - (1 - 0,22313) \\\\ &amp; = 0,22313 \\newline \\end{align*}\\] A probabilidade de um fusível ter uma vida útil maior que 150 horas é de 0,22313. valores &lt;- c(0, 150) # integrar e subtrair de 1 lambda &lt;- 1/100 # foi dada a vida útil média # Integração numérica probability &lt;- 1- integrate(function(x) dexp(x, rate = lambda), lower = valores[1], upper = valores[2])$value # Valor rotulos &lt;- c(150, &#39;infinito&#39;) # integrar e subtrair de 1 cat(&quot;Probabilidade de se observar valores entre &quot;, rotulos[1], &quot;e&quot;, rotulos[2], &quot;é :&quot;, probability, &quot;\\n&quot;) ## Probabilidade de se observar valores entre 150 e infinito é : 0.2231302 O custo unitário de um fusível é de R$ 10,00 com um custo adicional de R$ 8,00 se sua vida for inferior a 200 horas. Assim o custo esperado de um fusível será dada produto dos custos pelas respectivas probabilidades associadas: \\[ C= \\begin{cases} R\\$ 10,00 \\text{ se t &gt; 200}\\\\ R\\$ 18,00 \\text{ se t &lt; 200}\\\\ \\end{cases} \\] A probabilidade de um fusível durar mais de 200 horas poderá ser dada por 1 menos o valor da integral da fdp no intervalo [0;200]: \\[\\begin{align*} P( T &gt; 200) &amp; = 1 - P(0&lt;T&lt;200) \\\\ &amp; = 1- \\int_{0}^{200} \\lambda \\cdot \\varepsilon ^{- \\lambda \\cdot x} dx \\\\ &amp; = 1 - \\varepsilon^{-\\lambda \\cdot x} \\rvert_{0}^{200} \\\\ &amp; = 1 - (\\varepsilon^{- 0,01 \\cdot 0} - \\varepsilon^{-0,01 \\cdot 200}) \\\\ &amp; = 1 - (1 - 0,1353) \\\\ &amp; = 0,1353 \\newline \\end{align*}\\] A probabilidade de um fusível ter uma vida útil maior que 200 horas é de 0,1353. valores &lt;- c(0, 200) # integrar e subtrair de 1 lambda &lt;- 1/100 # foi dada a vida útil média # Integração numérica probability &lt;- 1- integrate(function(x) dexp(x, rate = lambda), lower = valores[1], upper = valores[2])$value # Valor rotulos &lt;- c(200, &#39;infinito&#39;) # integrar e subtrair de 1 cat(&quot;Probabilidade de se observar valores entre &quot;, rotulos[1], &quot;e&quot;, rotulos[2], &quot;é :&quot;, probability, &quot;\\n&quot;) ## Probabilidade de se observar valores entre 200 e infinito é : 0.1353353 A probabilidade de um fusível durar menos de 200 horas será dada por 1 menos o valor calculado anteriormente: \\[ P( 0 &lt; T &lt; 200) = 1 - 0,1353 = 0,8647 \\] A probabilidade de um fusível ter uma vida útil menor que 200 horas é de 0,8647. valores &lt;- c(0, 200) #a integrar lambda &lt;- 1/100 # foi dada a vida útil média # Integração numérica probability &lt;- integrate(function(x) dexp(x, rate = lambda), lower = valores[1], upper = valores[2])$value # Valor cat(&quot;Probabilidade de se observar valores entre &quot;, valores[1], &quot;e&quot;, valores[2], &quot;é :&quot;, probability, &quot;\\n&quot;) ## Probabilidade de se observar valores entre 0 e 200 é : 0.8646647 O custo esperado é de: \\(10,00 \\times 0,1353 + 18,00 \\times 0,8647 = R\\$ 16,92\\) 6.3.3 Normal A distribuição Normal (Gaussiana) é uma das mais importantes distribuições de probabilidades por possibilitar a adequada modelagem de fenômenos de diversas áreas: física, biologia, psicologia, ciências sociais e econômicas. A história da curva Gaussiana está relacionada à formulação da Teoria da Probabilidade nos séculos XVIII e XIX, que contou com contribuições de muitos matemáticos dentre os quais podemos citar Abrahan De Moivre, Pierre Simon Laplace, Adrien-Marie Legendre, Francis Galton e Johann Carl Friedrich Gauss. Esses matemáticos constataram que as variações entre repetidas medidas da mesma grandeza física apresentavam um grau surpreendente de regularidade. Com a repetição de medidas em um numero razoável observou-se que distribuição das variações poderia ser satisfatoriamente aproximada por uma curva contínua. Em 1920 Karl Pearson relembra ter usado a expressão curva normal como uma substituição de natureza diplomática para evitar uma questão internacional sobre precedência que poderia surgir no uso comum à época da denominação “Curva de Laplace-Gauss”, dois grandes matemáticos e astrônomos. Todavia, reconheceu também que a nova denominação poderia levar pessoas a incorrer no erro de supor que todas as demais distribuições seriam anormais. Uma variável aleatória contínua \\(X\\) que assuma valores \\(x\\) (\\(-\\infty &lt; x &lt; \\infty\\)) com média \\(\\mu\\) e variância \\(\\sigma^{2}\\) distribuídos segundo uma Curva Gaussiana é denotada por \\(X \\sim N(\\mu, \\sigma^{2})\\), e sua função densidade de probabilidade é dada por: \\[ f(x)=\\frac{1}{ {\\sigma . \\sqrt {2\\pi }}}. e^\\frac{{-(x-\\mu)^{2}}}{2.\\sigma^{2}} \\] A função de probabilidade cumulativa, a probabilidade de que a variável aleatória \\(X\\) apresente um valor menor ou igual a \\(x\\) é dada por: \\[ F(x) = P(X\\le x) = \\frac{1}{\\sigma \\sqrt{2}\\pi } \\underset{-\\infty }{\\overset{x}{\\int }} {e^ \\frac{-(v - \\mu)^{2}}{2\\sigma^{2}}}dv \\] Sejam as seguintes variáveis aleatórias contínuas com Distribuição Normal: \\(X \\sim N(\\mu_{X}, {\\sigma^{2}}_{X})\\), tal que \\(E(X)=\\mu_{X}\\) e \\(Var(X)= \\sigma^{2}_{X}\\); e \\(Y \\sim N(\\mu_{Y}, {\\sigma^{2}}_{Y})\\), tal que \\(E(Y)=\\mu_{Y}\\) e \\(Var(Y)= \\sigma^{2}_{Y}\\). Uma variável aleatória definida como uma soma de variáveis Normais \\(W=X \\pm Y\\) terá: E(W) = \\(\\mu_{X} \\pm \\mu_{y}\\); e, Var(W) = \\(\\sigma^{2}_{X} + \\sigma^{2}_{Y}\\). Para qualquer variável aleatória contínua com Distribuição Normal, chama-se de padronização à mudança da escala original dos dados para unidades padronizadas: scores z. Uma variável padronizada segue possuindo Distribuição Normal, sendo denotada por \\(Z \\sim N (0,1)\\), indicando que a média é \\(0\\) e o desvio-padrão é \\(1\\). Para a padronização de uma variável original \\(X\\) segue: \\[ Z = \\frac{X-\\mu}{\\sigma} \\] A função densidade de probabilidade de uma variável aleatória contínua padronizada é dada por: \\[\\begin{align*} f(z) &amp; = \\frac{1}{{\\sqrt {2\\pi } }}e^{ - \\frac{{z^2 }}{2}} \\\\ f(z) &amp; = 0,3989e^{ - 5z^2} \\end{align*}\\] E a função de probabilidade cumulativa (a probabilidade de que a variável aleatória padronizada \\(Z\\) apresente um valor menor ou igual a \\(z\\)}) é dada por: \\[\\begin{align*} F(z) &amp; = P(Z\\le z) \\\\ P(Z\\le z) &amp; = \\frac{1}{\\sqrt{2}\\pi } \\underset{-\\infty }{\\overset{z}{\\int }} e^\\frac{-u^{2}} {2} du \\end{align*}\\] A área sob a curva padronizada (probabilidade cumulativa entre dois valores \\(z\\)) é obtida em tabelas, dispensando a resolução numérica da integral acima (posto não possuir solução analítica). Essas tabelas apresentam no cruzamento de suas linhas e colunas , a área sob a curva Normal padronizada equivalente à probabilidade associada a um **determinado intervalo* como, por exemplo: Figure 6.3: Tabela Z mostrando a probabilidade ao intervalo [0 ; 1,64] (quadro superior à esquerda explica onde a área se encontra) A tabela Z possibilita: 1- encontrar a probabilidade (área) partindo de score z; e 2- encontrar o score z. Modo 1: admita que você padronizou um certo valor e obteve o score z igual a 1,64. Na coluna vertical à esquerda você deverá encontrar qual é a linha que apresenta a unidade e a primeira casa decimal desse valor: 1,6. Nas outras dez colunas verticais você deverá buscar aquela que apresenta a segunda casa decimal desse valor: 4. No cruzamento dessas duas colunas você irá fazer a leitura do número que lá dentro se encontra. Agora veja o desenho orientativo que há no canto superior à direita (cada tabela pode variar um pouco). Ele expõe graficamente uma área hachurada e na cor laranja entre o zero e um valor z. É exatamente o valor dessa área que você acabou de encontrar (a área sob a curva da fdp no intervalo [0 ; 1,64]. Modo 2: admita que você precisa determinar qual é o valor do score z para uma probabilidade (área) no intervalo [0 ; z] = 0,4495. Nessa situação, simplesmente faça o caminho reverso. Encontre que célula apresenta esse valor de 0,4495 e faça a leitura da unidade e a primeira casa decimal do valor do score z na coluna lateral à esquerda (1,6) e de sua segunda casa decimal na linha que identifica as outras dez colunas (4). A fdp da distribuição Normal apresenta uma curva simétrica centrada em sua média \\(\\mu\\). A fdp da distribuição Normal padronizada também é simétrica e centra em sua média que agora tem valor \\(0\\). A totalidade da área sob essas fdp (ou seja, o intervalo \\(-\\infty &lt; z &lt; \\infty\\)) possui área igual a \\(1\\). Cada metade, consequentemente, terá área igual a \\(0,50\\). Por esse motivo as tabelas Z mostram apenas a metade da curva da fdp e muitos exercícios irão demandar que você some a área (0,50) do restante da curva da fdp, subtraia ou faça outras operações aritméticas simples para resolvê-los. library(ggplot2) options(&quot;digits&quot;=4) prob_desejada=0.95 z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) d_0=dnorm(0, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, 0), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores score (z)&quot;, breaks = z_desejado) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(0, z_desejado), colour=&quot;red&quot;)+ geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c( z_desejado, 4), colour=&quot;black&quot;)+ labs(title= &quot;Curva da função densidade da distribuição Normal padronizada&quot;, subtitle = &quot;P(-inf; 0)=0,50 (cinza) \\nP(0 ; 1,645)=0,4495 (vermelho) \\nP(1,645 ; inf)=0,0505 (cinza) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = 0, y = 0, xend = 0, yend = d_0), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=-1, y=0.2, label=&quot;Probabilidade (área) =0,50 &quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=0.1, y=0.1, label=&quot;Probabilidade (área) =0,4495&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=2, y=0.05, label=&quot;Probabilidade (área) =0,0505&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 6.4: Curva da fdp da Distribuição Normal padronizada mostrando as áreas delimitadas pelo score z arbitrado (1,64) Exemplo: Admita que o índice pluviométrico de uma cidade siga uma distribuição normal, com média de 101,60 mm/ano e desvio padrão de 12,70 mm/ano. Quais seriam as probabilidades dessa cidade ter menos de 83,82 mm/ano e mais de 96,52 mm/ano de precipitação no próximo ano? A probabilidade de ocorrência de uma precipitação inferior a 83,82mm/ano equivale (graficamente) à área situada no intervalo [\\(-\\infty ; 83,82\\)] na curva da fdp da distribuição Normal com média 101,60mm/ano e desvio padrão de 12,70mm/ano: \\[ P(X \\le 83,82) \\equiv área[-\\infty ; 83,82] \\] A probabilidade de ocorrência de uma precipitação superior a 96,52 mm/ano equivale (graficamente) à área situada no intervalo [\\(96,52 ; +\\infty\\)] na curva da fdp distribuição Normal com média 101,60mm/ano e desvio padrão de 12,70mm/ano \\[ P(X \\ge 96,52) \\equiv área[96,52 ; +\\infty] \\] Padronizando esses valores será possível estabelecer os valores das precipitações associadas às probabilidades pedidas em termos de scores \\(z\\) que podem ser obtidas em tabelas Z. Considerando-se que a média é de 101,60mm/ano e o desvio padrão é de 12,70mm/ano, para a primeira precipitação (83,82mm/ano) teremos: \\[\\begin{align*} X_{1} &amp; = 83,82 \\\\ Z_{n} &amp; = \\frac{X_{n} - \\mu}{\\sigma}\\\\ z_{1} &amp; = -1,40 \\end{align*}\\] E a probailidade pedida equivale (graficamente) à área situada no intervalo [\\(-\\infty ; -1,40\\)] na curva da fdp distribuição Normal padronizada: \\[ P(X \\le 83,82) = P(Z \\le -1,40) \\equiv área[-\\infty ; -1,40] \\] Portanto, uma precipitação de 83,82mm/ano localiza-se a -1,40 desvios padrão à esquerda da média da curva Normal padronizada (\\(\\mu=0\\)). Em uma tabela da Distribuição Normal Padronizada temos a probabilidade associada ao intervalo \\(P(0&lt;Z&lt;z)\\) tabelada para vários valores de \\(z\\). No caso, veremos que para um valor \\(P(0&lt;z&lt;1,40)=0,4192\\) (lembre-se: a curva é simétrica por essa razão as tableas resumem-se a mostrar um dos lados). Sendo a curva simétrica, a área total (probabilidade) sob a fdp é igual a \\(1\\): 0,50 à esquerda e 0,50 à direita. Assim, a área hachurada em vermelho na Figura 6.5 é a probabilidade pedida: \\[\\begin{align*} P(X \\le 83,82) &amp; = 0,50 - 0,4192 \\\\ P(X \\le 83,82) &amp; = 0,0808 \\end{align*}\\] # Integração numérica no R fx &lt;- function(x){(1/(12.7*sqrt(2*pi))) * exp( -(1/2)*((x - 101.6)/(12.7))^2)} p1=integrate(fx, 83.82, 101.6) 1-(p1$value + 0.5) ## [1] 0.08076 # Ou usando a função no R pnorm(83.82, 101.6, 12.7) ## [1] 0.08076 library(ggplot2) options(&quot;digits&quot;=4) prob_desejada=0.0808 z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) d_0=dnorm(0, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado), colour=&quot;red&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores score (z)&quot;, breaks = z_desejado) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado, 0), colour=&quot;black&quot;)+ geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, 4), colour=&quot;black&quot;)+ labs(title= &quot;Curva da função densidade da distribuição Normal padronizada&quot;, subtitle = &quot;P(-inf; -1,40)=0,0808 (vermelho) \\nP(-1,40 ; 0 )=0,4192 (cinza) \\nP(0 ; inf)=0,50 (cinza) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = 0, y = 0, xend = 0, yend = d_0), color=&quot;blue&quot;, lty=2, lwd=0.3)+ theme_bw() Figure 6.5: Curva da fdp da Distribuição Normal padronizada mostrando as áreas delimitadas pelo score z calculado (-1,40) De modo análogo para a segunda questão 96,52 mm/ano) teremos: \\[\\begin{align*} X_{2} &amp; = 96,52 \\\\ Z_{n} &amp; = \\frac{X_{n} - \\mu}{\\sigma}\\\\ z_{2} &amp; = -0,40 \\end{align*}\\] E a probailidade pedida equivale (graficamente) à área situada no intervalo [$-0,40 ; $] na curva da fdp distribuição Normal padronizada: \\[ P(X \\ge 96,52) = P(Z \\ge -0,40) \\equiv área[-\\infty ; -1,40] \\] Portanto, uma precipitação de 96,52 mm/ano localiza-se a -0,40 desvios padrão à esquerda da média da curva Normal padronizada (\\(\\mu=0\\)). Em uma tabela da Distribuição Normal Padronizada temos a probabilidade associada ao intervalo \\(P(0&lt;Z&lt;z)\\) tabelada para vários valores de \\(z\\). No caso, veremos que para um valor \\(P(0&lt;z&lt;0,40)=0,1554\\) (lembre-se: a curva é simétrica por essa razão as tableas resumem-se a mostrar um dos lados). Sendo a curva simétrica, a área total (probabilidade) sob a fdp é igual a \\(1\\): 0,50 à esquerda e 0,50 à direita. Assim, a área hachurada em vermelho na Figura 6.6 é a probabilidade pedida: \\[ P(X \\ge 96,52) = 0,50 + 0,4192 = 0,6554 \\] # Integração numérica no R fx &lt;- function(x){(1/(12.7*sqrt(2*pi))) * exp( -(1/2)*((x - 101.6)/(12.7))^2)} p1=integrate(fx, 101.6, 96.52) 1-(p1$value + 0.5) ## [1] 0.6554 # Ou usando a função no R pnorm(96.52, 101.6, 12.7, lower.tail = FALSE) # para calcular à direita ## [1] 0.6554 library(ggplot2) options(&quot;digits&quot;=4) prob_desejada=0.3446 z_desejado=round(qnorm(prob_desejada),3) d_desejada=dnorm(z_desejado, 0, 1) d_0=dnorm(0, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores score (z)&quot;, breaks = z_desejado) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado, 0), colour=&quot;red&quot;)+ geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(0, 4), colour=&quot;red&quot;)+ labs(title= &quot;Curva da função densidade da distribuição Normal padronizada&quot;, subtitle = &quot;P(-inf; -0,40)=0,3446 (cinza) \\nP(-0,40 ; 0)=0,1554 (vermelho) \\nP(0 ; inf)=0,50 (vermelho) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = 0, y = 0, xend = 0, yend = d_0), color=&quot;blue&quot;, lty=2, lwd=0.3)+ theme_bw() Figure 6.6: Curva da fdp da Distribuição Normal padronizada mostrando as áreas delimitadas pelo score z calculado (-0,40) 6.3.4 Student “t” Se uma variável aleatória \\(T\\) contínua com \\(\\nu\\) graus de liberdade segue a , sua função densidade de probabilidade é dada por: \\[ f(t) = \\frac{-\\Gamma \\left(\\frac{\\nu +1}{2}\\right)}{\\sqrt{\\nu \\pi }\\cdot \\Gamma \\cdot \\left(\\frac{\\nu }{2}\\right)}\\cdot {\\left(1+\\frac{{t}^{2}}{v}\\right)}^{\\frac{-\\left(\\nu +1\\right)}{2}} \\] com \\(\\Gamma (n) = (n!)\\) Uma variável aleatória contínua com essa distribuição possui: \\(E(T)=\\mu=0\\); e, \\(Var(T)=\\sigma^{2}=\\frac{\\nu}{(\\nu -2)}\\), para \\(\\nu &gt; 2\\) Admitamos que a partir de uma amostra aleatória composta por \\(n\\) valores retirados de uma população Normal com variância conhecida \\(\\sigma^{2}\\) deseje-se estimar a média \\(\\mu\\). Para grandes amostras (\\(n \\ge 30\\)) a distribuição amostral de \\(\\stackrel{-}{X}\\) é aproximadamente Normal, com média \\(\\mu\\) e variância \\(\\frac{\\sigma^{2}}{n}\\). Isso torna possível estabelecer a seguinte estatística padronizada anteriormente vista: \\[ Z \\sim \\frac{\\bar X -\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) \\] Entretanto, para amostras de tamanho reduzido e variância desconhecida, a adoção do desvio padrão amostral \\(S\\) na estatística anterior conduz a uma outra distribuição. Essa nova distribuição ainda é simétrica e com média \\(\\mu=0\\); todavia não mais seria a Normal padronizada pois seu denominador \\(\\frac{S}{\\sqrt{n}}\\) é uma variável aleatória (\\(S\\) é uma variável aleatória pois depende da amostra extrída ao passo o denominador anterior era uma constante: \\(\\sigma\\)). Essa família de distribuições (cuja forma tende à de uma distribuição Normam padronizada quando \\(n \\to \\infty , t_{n} \\to N(0,1)\\) ) foi estabelecida pelo químico e estatístico inglês William Sealy Gosset. \\[ T \\sim \\frac{\\bar X -\\mu}{S/\\sqrt{n}} \\sim t_{n-1} \\] Para se trabalhar com essa distribuição é preciso saber qual sua forma específica e isso é informado por uma estatística denominada graus de liberdade: \\(\\nu\\). Toda estatística de teste que dependa de uma variável aleatória possui graus de liberdade (\\(\\nu\\)). O número de informações independentes (ou livres) da amostra dá o número de graus de liberdade da Distribuição \\(t\\) de Student. Na situação acima o propósito é estimar a média populacional \\(\\mu\\) através da média amostral \\(\\stackrel{-}{X}\\); todavia, tivemos também que estimar sua variância \\(\\sigma^{2}\\) através de \\(S^{2}\\), de tal modo que o número de graus de liberdade será \\(\\nu=n-1\\): o tamanho da amostra menos 1. A área sob a curva da fdp de uma distribuição de Student (probabilidade cumulativa entre dois valores \\(t\\)) é também obtida em tabelas. Essas tabelas apresentam no cruzamento de suas linhas e colunas , o valor “t” para várias áreas (probabilidades) associadas cmom: ao intervalo fechado: [-t ; +t] (Figura 6.8); o intervalo aberto à esquerda: [-inf ; t] (Figura 6.9); e, o intervalo aberto à direita: [t, inf] (Figura 6.10). Nas linhas horizontais lê-se os graus de liberdade \\(\\nu\\) e nas colunas as áreas (probabilidades). Figure 6.7: Tabela t mostrando duas áreas (probabilidades) para um grau de liberdade igual a 10. No intervalo fechado [-0,1289 ; 0,1289] a probabilidade é de 0,90 e para os intervalos abertos à direita: [0,1289 ; inf] e à esquerda: [+inf ; 0,1289] é de 0,95. A tabela t possibilita: 1- encontrar a probabilidade (área) partindo de um valor “t”; e 2- encontrar um valor “t” para determinada probabilidade A fdp da distribuição de Student apresenta também uma curva simétrica centrada em sua média \\(\\mu=0\\). A totalidade da área sob essa fdp (ou seja, o intervalo \\(-\\infty &lt; t &lt; \\infty\\)) possui área igual a \\(1\\). Cada metade, consequentemente, terá área igual a \\(0,50\\). Muitos exercícios irão demandar que você some a área (0,50) do restante da curva da fdp, subtraia ou faça outras operações aritméticas simples para resolvê-los. library(ggplot2) alfa=0.05 prob_desejada1=alfa/2 df=10 t_desejado1=round(qt(prob_desejada1,df ),4) d_desejada1=dt(t_desejado1,df) prob_desejada2=1-alfa/2 df=10 t_desejado2=round(qt(prob_desejada2, df),4) d_desejada2=dt(t_desejado2,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado1, t_desejado2)) + labs(title= &quot;Curva da função densidade \\nDistribuição t (df=10)&quot;, subtitle = &quot;P(-2,228 ; 2,228)=0,90 (cinza) \\nP(-inf ; -2,228)=P(2,086; inf)=0,05 (vermelho)&quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = t_desejado2, y = 0, xend = t_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=-0.1, y=0.2, label=&quot;Probabilidade (área) =0,90 \\n(gl=10)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=-3.5, y=0.1, label=&quot;Probabilidade (área) =0,05 \\n(gl=10)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=2.5, y=0.1, label=&quot;Probabilidade (área) =0,05 \\n(gl=10)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 6.8: Curva da fdp da Distribuição Studentpara 10 graus de liberdade, mostrando as áreas delimitadas pelos valores +/-t (+/-2,28) alfa=0.025 prob_desejada=alfa df=10 t_desejado=round(qt(prob_desejada,df ),4) d_desejada=dt(t_desejado,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, 4), colour=&quot;black&quot;)+ scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado)) + labs(title= &quot;Curva da função densidade \\nDistribuição t (df=10)&quot;, subtitle = &quot;P(-inf ; -2,228)=0,025 (vermelho) \\nP(-2,228 ; +inf)= 0,975 (cinza)&quot;)+ geom_segment(aes(x = t_desejado, y = 0, xend = t_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=-0.1, y=0.2, label=&quot;Probabilidade (área) =0,975 \\n(gl=10)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=-3.5, y=0.1, label=&quot;Probabilidade (área) =0,025 \\n(gl=10)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 6.9: Curva da fdp da Distribuição Student para 10 graus de liberdade, mostrando as áreas delimitadas pelo valor -t (-2,28) alfa=0.025 prob_desejada=1-alfa df=10 t_desejado=round(qt(prob_desejada,df ),4) d_desejada=dt(t_desejado,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(-4, 0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado, 4), colour=&quot;black&quot;)+ scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado)) + labs(title= &quot;Curva da função densidade \\nDistribuição t (df=10)&quot;, subtitle = &quot;P(-inf ; 2,228)=0,975 (vermelho) \\nP(2,228 ; +inf)= 0,025 (cinza)&quot;)+ geom_segment(aes(x = t_desejado, y = 0, xend = t_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=0, y=0.2, label=&quot;Probabilidade (área) =0,975 \\n(gl=10)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=2.5, y=0.1, label=&quot;Probabilidade (área) =0,025 \\n(gl=10)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 6.10: Curva da fdp da Distribuição Student para 10 graus de liberdade, mostrando as áreas delimitadas pelo valor -t (-2,28) 6.3.5 Qui-Quadrado Considerem \\(X_{1}\\),\\(X_{2}\\),…,\\(X_{\\nu}\\) como \\(\\nu\\) variáveis aleatórias contínuas independentes e normalmente distribuídas com média zero e variância 1. Definamos também uma variável aleatória resultante da soma dos quadrados das variáveis anteriormente especificadas: \\[ \\chi^{2} = X_{1}^{2} + X_{2}^{2}+...X_{\\nu}^{2} \\] A variável aleatória \\(\\chi^{2}\\) possui seguinte fdp para \\(x &gt; 0\\) (para \\(x\\le 0, f(x) = 0)\\), com \\(\\nu\\) graus de liberdade: \\[ f(x) = \\frac{1}{{2}^{\\frac{\\nu}{2}} \\cdot \\Gamma \\cdot (\\frac{\\nu}{2})} \\cdot {x}^{ {(\\frac{\\nu}{2}})^{-1} \\cdot \\epsilon ^{\\frac{-\\nu}{2}} } \\] A função de probabilidade cumulativa é dada por: \\[ P(\\chi^{2} \\le x) = \\frac{1}{{2}^{\\frac{\\nu}{2}} \\cdot \\Gamma \\cdot (\\frac{\\nu}{2})} \\underset{-\\infty }{\\overset{x}{\\int }} {u}^{ {(\\frac{\\nu}{2}})^{-1} \\cdot \\epsilon ^{\\frac{-\\nu}{2}} }du \\] Algumas propriedades da distriuição Qui-quadrado: Pelo Teorema Central do Limite esta família de distribuições tende a uma distribuição Normal quando o número de graus de liberdade tende ao infinito (\\(\\nu \\to \\infty\\) (\\(\\chi^{2} \\to N(0,1)\\))); Se uma variável é definida como a soma de duas variáveis independentes com Distribuição Qui-quadrado com \\(\\nu_{1}\\) e \\(\\nu_{2}\\) graus de liberdade, essa variável também seguirá a Distribuição Qui-quadrado com \\(\\nu_{1} + \\nu_{2}\\) graus de liberdade É assimétrica e definda para \\(x &gt; 0\\). 6.3.6 Fisher-Snedecor “F” Uma variável aleatória contínua definida como \\(X \\sim F(\\nu_{1},\\nu_{2})\\) segue a Distribuição Fisher-Snedecor com parâmetros \\(\\nu_{1}\\) e \\(\\nu_{2}\\), números inteiros positivos conhecidos como graus de liberdade do numerador e do denominador, respectivamente. A Distribuição de Fisher-Snedecor é também conhecida como a Distribuição da razão de variâncias. Uma variável aleatória \\(X\\) que segue uma Distribuição de Fisher-Snedecor com \\(\\nu_{1}\\) e \\(\\nu_{2}\\) graus de liberdade tem sua pdf dada por: \\[ f(x) = \\frac{\\Gamma((\\nu_{1}+\\nu_{2})/2)(\\nu_{1}/\\nu_{2})^{\\nu_{1}/2}x^{\\nu_{1}/2-1}} {\\Gamma(\\nu_{1}/2)\\Gamma(\\nu_{2}/2)[(\\nu_{1}/\\nu_{2})x+1]^{(\\nu_{1}+\\nu_{2})/2}} \\qquad x &gt; 0, \\] com \\(\\nu_{1} = 1,2,\\ldots\\) e \\(\\nu_{2} = 1,2, \\ldots \\,\\). "],["tabelas.html", "6.4 Tabelas", " 6.4 Tabelas Figure 6.11: Tabela de valores z da Distribuição Normal padronizada Figure 6.12: Tabela de valores t da Distribuição de Student Figure 6.13: Tabela de valores x da Distribuição Qui-quadrado Figure 6.14: Tabela de valores x da Distribuição F específicos do percentil 95 (há outras tabelas, para outros percentis) "],["planejamento_pesquisas.html", "Capítulo 7 Introdução ao planejamento de pesquisas", " Capítulo 7 Introdução ao planejamento de pesquisas O estudo de uma realidade ainda não compreendida impõe ao pesquisador a formulação de hipóteses sobre suas possíveis causas, qualquer que seja a área do conhecimento: ciências biológicas; ciências exatas; ciências agrárias; ciências humanas; ciência sociais e outras. Figure 7.1: Representação esquemática do fluxo de infomações da amostra à produção de conhecimento Uma hipótese é uma conjectura racional feita após um grande número de observações e experimentos; é uma tese que precisa ser confirmada ou verificada por meio de novas observações e experimentos. Uma teoria científica é transitória. Uma conjectura temporariamente sustentada que um dia poderá ser refutada e substituída por outra. Conclusões baseadas em raciocínios plausíveis são provisórias, ao contrário daquelas produzidas por raciocínios demonstrativos. Considere as hipóteses a seguir: Exemplo: Crianças socialmente isoladas assistem mais televisão do que crianças bem integradas a seus grupos? Exemplo: Famílias constituídas por um só dos genitores (pai ou mãe ausentes) geram mais delinquentes? Exemplo: Diferentes tipos de uso do solo urbano influenciam na taxa de ocorrência de crimes? Só após ter-se bem definido pelo pesquisador o que seria uma criança socialmente isolada e uma criança bem integrada a um grupo; assim como o que seria família, genitor ausente e até mesmo o que é um delinquente, o que é um crime e quais são os usos do solo urbano é que se pode avançar com o planejamento da pesquisa até a sua execução (entrevistas com crianças que responderiam o número de horas que passam defronte à televisão por dia ou um levantamento comparativo que permita verificar se há alguma correlação entre o comportamento social e o ambiente familiar de origem). É necessário ao pesquisador testar suas hipóteses com informações trazidas da realidade estudada mesmo que, aparentemente, pareçam verdadeiras porque, caso contrário, seu julgamento seria conduzido baseado em ideias pré-concebidas por experiências pessoais anteriores, muitas vezes tendenciosas, resultando em conclusões cientificamente nulas. "],["planejamento-de-pesquisas.html", "7.1 Planejamento de pesquisas", " 7.1 Planejamento de pesquisas Alguns consideram o artigo publicado em 1895 pelo estatístico norueguês Anders Nicolai Kiaer ( Observations et expériences concernant les dénombrements représentatifs ) como o nascimento oficial da pesquisa por amostragem, apesar de existirem registros anteriores da realização de pesquisas por Laplace, Lavoisier e outros (link). Pesquisa é uma investigação sistemática para se obter informações precisas que permitam descrever, explicar o fenômeno que se deseja estudar. Pesquisas são baseadas em raciocínio lógico e envolve métodos indutivos e dedutivos. Requerem uma análise aprofundada de todos os dados coletados para que não haja anomalias associadas a eles. Uma pesquisa cria um caminho para gerar novas perguntas: os dados existentes ajudam a criar mais oportunidades de pesquisa. Uma pesquisa tem natureza analítica: utiliza todos os dados disponíveis para que não haja ambiguidade na inferência. A precisão é um dos aspectos mais importantes da pesquisa: as informações obtidas devem ser o mais precisas e verdadeiras possível: precisão nos instrumentos utilizados, nas calibrações de instrumentos ou ferramentas, treinamento de operadores. "],["tipos-de-pesquisas.html", "7.2 Tipos de pesquisas", " 7.2 Tipos de pesquisas Quadro de tipos de pesquisas conforme sua classificação Classificação Tipos de pesquisas Finalidade básica (fundamental) aplicada (tecnológica) Abordagem qualitativa quantitativa (descritiva ou analítica) Objetivos exploratória explicativa Tempo transversal longitudinal Natureza observacional experimental Obtenção dos dados observacional experimental por amostragem 7.2.1 Quanto à finalidade na pesquisa básica os dados coletados para aprimorar o conhecimento; a principal motivação é a expansão do conhecimento; é uma pesquisa não comercial que não tem como propósito imediato a criação ou invenção de nada; e, uma pesquisa aplicada se concentra na análise e solução de problemas existentes na vida real; refere-se ao estudo que ajuda a resolver problemas práticos usando métodos científicos. 7.2.2 Quanto à forma de abordagem Os tipos de métodos de pesquisa podem ser amplamente divididos em duas categorias quantitativas e qualitativas: a pesquisa quantitativa descreve, infere e resolve problemas usando números; a ênfase é colocada na coleta de dados numéricos, no resumo desses dados e na realiazação de inferências a partir dos dados; a pesquisa qualitativa é baseada em palavras, sentimentos, opiniões, sons e outros elementos não numéricos e não quantificáveis. 7.2.3 Quanto aos objetivos uma pesquisa exploratória é conduzida para explorar um grupo de perguntas; as respostas e análises podem não oferecer uma conclusão final para o problema analisado; tem como objetivo lidar com novas problemáticas que não foram exploradas antes; uma pesquisa explicativa é conduzida para entender o impacto de certas alterações em procedimentos padrão já estabelecidos; a realização de experimentos é a forma mais popular de pesquisa casual 7.2.4 Quanto ao desenvolvimento no tempo em uma pesquisa transversal a análise está fixada em um momento específico no tempo; uma pesquisa longitudinal desenrola-se em um período de tempo determinado 7.2.5 Quanto à natureza em uma pesquisa observacional o pesquisador atual de modo passivo; uma pesquisa experimental o pesquisador é ativo ao promover processos de modo deliberado; em uma pesquisa amostral o pesquisador define uma população que apresenta a característica de inetresse do estudo. 7.2.6 Quanto à forma de obtenção dos dados nos levantamento de dados em uma pesquisa observaciona o pesquisador atua meramente como expectador de fenômenos ou fatos, sem, no entanto, realizar qualquer intervenção que possa interferir no curso natural e/ou no desfecho dos mesmos, embora possa, neste meio tempo, realizar medições, análises e outros procedimentos para coleta de dados; em pesquisas experimentais o delineamento do experimento estabelece o modo como as variáveis em estudo serão aplicadas ao objeto com o propósito de se obter uma informação (resposta) sobre sua influência para validação ou não de uma hipótese previamente estabelecida; levantamentos amostrais são aqueles nos quais os dados são extraídos de um subconjunto tecnicamente extraído de uma população bem definida por meio de procedimentos controlados pelo pesquisador e que podem ser subdivididos em probabilísticos (casuais ou aleatórios) e não probabilísticos (intencionalmente dirigidos). "],["principais-etapas-de-uma-pesquisa.html", "7.3 Principais etapas de uma pesquisa:", " 7.3 Principais etapas de uma pesquisa: Definição precisa do objetivo; Planejamento; Execução; Analise dos dados obtidos; Resultados; e, Conclusões. 7.3.1 Objetivo Ao se iniciar qualquer pesquisa deve-se ter bem muito bem definido o problema a ser pesquisado, reduzido a uma hipótese testável. Os objetivos de uma pesquisa devem ser elaborados de forma bastante clara (já que as demais etapas da pesquisa tomam como base esses objetivos) e, invariavelmente, envolve uma extensa revisão da literatura existente sobre o assunto. Exemplo: (objetivo geral) estabelecer o perfil dos estudantes universitários de Londrina para se (objetivos específicos) conhecer a renda média familiar e cidade de origem. Hipótese: a renda média familiar dos estudantes com origem diversa de Londrina é menor que do que os da própria cidade. Uma vez que o objetivo geral está estabelecido e as hipóteses a serem testadas foram formuladas deve-se definir a população alvo cujos elementos contém a informação desejada considerando as definições estabelecidas para o problema. todas as universidades de Londrina (ou apenas as universidades públicas ou particulares); todos os cursos (ou algum em particular) … "],["população.html", "7.4 População", " 7.4 População Denomina-se por população ao universo de todos os elementos que apresentam a característica (informação) sob estudo (o termo aqui é utilizado em sentido estritamente técnico, nada relacionado ao número de habitantes de um determinado local). os pesos dos estudantes de uma determinada escola (população: todos os alunos); os salários pagos por uma empresa (população: todos os funcionários legalmente existentes); a proporção de indivíduos favoráveis a determinado projeto em uma cidade (população: todos os habitantes dessa cidade); a durabilidade das peças sob produção em uma certa fábrica (população: todas as peças produzidas por essa fábrica); o número de horas passadas defronte à televisão por crianças até 10 anos de idade no Brasil (população: todas as crianças do Brasil com até 10 anos). "],["censo.html", "7.5 Censo", " 7.5 Censo Denomina-se por censo à investigação de todos os elementos da população defnida, o que resulta em apuração exata da informação requerida na pesauisa. Todavia, muitos objetos de pesquisa impõem um grau de dificuldade e custo financeiro muito elevados para a execução de um censo o que acaba por tornarem não muito frequentes e, usualmente são realizados apenas pelo estado para dar suporte ao planejamento nacional ou local. "],["amostra.html", "7.6 Amostra", " 7.6 Amostra A coleta de dados em toda a população é inviável (ou até mesmo impossível) por diversas razões como, por exemplo: tempo e/ou recursos financeiros limitados; grande dispersão geográfica da população impondo complicações de ordem logística; ensaios destrutivos (corpos de prova) para geração de informações; inexistência a priori de dados, demandando a realização de experimentos para a sua geração. Denomina-se por amostra a qualquer subconjunto da população, extraído mediante procedimentos tecnicamente prescritos. Se a característica em estudo em uma população fosse homogênea em todos os seus elementos, qualquer tamanho de amostra seria suficiente (na realidade, bastaria um elemento dessa população para estudar a característica em toda ela). Considerando que existe variabilidade da característica nos elementos da população o pesquisador deve usar procedimentos estatísticos para a realização da amostragem e assegurar que tal variabilidade se reflita igualmente na amostra. Quando a população é grande o estudo de uma fração (amostra) mostra-se mais vantajoso pelas seguintes razões: redução de custos; redução de prazos: problemas relacionados à data de referência e a imprecisões introduzidas ao se fixar uma data pretérita (dificuldade em se recordar); e, maior precisão nas informações: menos entrevistadores (mas com alto nível de treinamento) e procedimentos de acompanhamento mais rigoros. Todavia há situações nas quais a extração de uma amostra não recomendada como: população pequena a característica de interessa é de fácil mensuração na população; necessidade de elevada precisão na estimação. "],["planejamento-do-levantamento-amostral.html", "7.7 Planejamento do levantamento amostral", " 7.7 Planejamento do levantamento amostral O planejamento do levantamento amostral deve considerar: -população objeto: identificar a população total de interesse sobre a qual desejamos obter informações; - característica populacional: delimitar o aspecto da população que interessa ao estudo; - unidade amostral: definida de acordo com o interesse do estudo é onde a informação de interesse está; pode ser uma peça, um indivíduo, uma família, uma fazenda, um corpo de prova, etc; - erro amostral: diferença entre um resultado obtido pela análise da informação trazida por uma amostral específica e o verdadeiro valor da informação na população; - tamanho da amostra: decorrência do item anterior e também das probabilidades de cometimento de erros do tipo I e II estabelecidas a priori (testes de hipóteses) "],["elaboração-dos-questionários.html", "7.8 Elaboração dos questionários", " 7.8 Elaboração dos questionários Um questionário deve ser previamente elaborado de modo a manter o foco na obtenção de dados necessários à pesquisa: facilitação da comunicação: a linguagem deve ser a mesma adotada pelo público-alvo; e a redação precisa ortograficamente; perguntas ambíguas ou não relacionadas à hipótese a ser testada devem ser evitadas, bem como o uso de termos ou simples palavras que possam induzir o respondente a uma opção; respostas possíveis: oferecer todas as possíveis alternativas de resposta para que o respondente possa encontrar sua melhor opção e não desistir da pesquisa; 7.8.1 Tipos de perguntas: pergunta desqualificatória: funciona como um filtro para evitar que respondentes que não integrem o público-alvo respondam à pesquisa; pergunta de resposta única: modelo de pergunta mais comum; pergunta de seleção múltipla: o respondente pode selecionar todas as opções que desejar dentre as alternativas oferecidas; pergunta em escala: formato de pergunta onde o respondente escolhe em uma escala de pontos pré-determinada (0 a 5; 0 a 10; 1 a 5, entre outros) e permite uma segunda análise a perguntas com apenas duas opções (concordo totalmente ou discordo totalmente, por exemplo). Algumas vantagens de pesquisas virtuais: impessoalidade: a ausência do entrevistador induz o respondente a uma reposta sincera; conveniência: o respondente pode participar da pesquisa em horário mais flexível; abrangência: permite alcançar mais facilmente um maior número de pessoas; menor custo envolvido; e, facilidade de tabulação: as respostas apresentadas pelo respondente podem ser automaticamente tabuladas e apresentadas na forma de gráficos. 7.8.2 Execução do levantamento amostral Encaminhamento dos questionários (ou disponibilização em meios virtuais); realização das entrevistas, do experimento ou ainda da observação. 7.8.3 Análise exploratória dos dados Obtenção de sínteses numéricas, apresentação na forma de tabelas e gráficos de variados formatos das respostas obtidas nos questionários. 7.8.4 Resultados e conclusões Apresentação dos resultados coerentes com os objetivos estipulados e a conclusão acerca da hipótese inicialmente proposta (rejeição u não rejeição da hipótes nula contraposta àquela formulada). "],["técnicas-de-amostragem.html", "7.9 Técnicas de amostragem", " 7.9 Técnicas de amostragem O modo de se obter uma amostra é tão importante, e existem tantos modos de fazê-lo, que esses procedimentos constituem especialidades dentro da Estatística.   Todavia os que são mais frequentemente empregados estão representados na Figura \\(\\ref{fig35}\\): Figure 7.2: Principais procedimentos para se extrair uma amostra "],["amostragem-probabilística.html", "7.10 Amostragem probabilística", " 7.10 Amostragem probabilística Uma amostragem de natureza probabilística é aquela que reúne todas as técnicas pelas quais se deixa completamente ao acaso a escolha dos elementos da população a serem incluídos na amostra. A aleatorização visa assegurar que a informação extraída da amostra possa ser generalizada na população de origem. A cada extração a probabilidade de um elemento ser incluído é igual para todos (embora ela e altere em razão de ser uma extração sem reposição). 7.10.1 Amostragem aleatória simples (AAS) Consiste na seleção de \\(n\\) elementos amostrais de tal modo que cada um deles tenha a mesma probabilidade de pertencer à amostra que os demais. Figure 7.3: Amostra aleatória simples AAS Duas situações distintas: com reposição do elemento amostral escolhido: o mesmo elemento da população pode ser amostrado mais de uma vez (a probabilidade de seleção não se altera); ou, sem reposição: cada elemento da população é amostrado uma única vez (a probabilidade de seleção se altera) Amostragem aleatória simples sem reposição. Admita uma população (\\(N=5\\)) composta pelos elementos: {a, b, c, d, e} (podem ser as rendas anuais de cinco pessoas, os pesos de cinco vacas ou cinco modelos diferentes de aviões) da qual se deseje extrair uma amostra de tamanho \\(n=3\\). Haverá 10 amostras possíveis de serem extraídas com tamanho 3 (\\(n=3\\)): {abc, abd, abe, acd, ace, ade, bcd, bce, bde, cde} pois: \\[ C_{(N,n)} = \\frac{ N! }{ n! \\times ( N-n)!}=10 \\] Amostragem aleatória simples com reposição. Considere agora a mesma população anterior (\\(N=5\\)) e o mesmo tamanho da amostra (\\(n=3\\)). Se a amostragem for feita com reposição teremos então \\(N^{n}=125\\) amostras possíveis de serem extraídas: {aaa, aab, aac, aad, aae, aba, abb, abc, abd, abe, ……} # Dados conjunto=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # As 10 combinações possíveis tomando-se 3 elementos: library(combinat) ## ## Attaching package: &#39;combinat&#39; ## The following object is masked from &#39;package:utils&#39;: ## ## combn #combn(conjunto, 3) (remova o # para executar) # As 125 permutações possíveis tomando-se 3 elementos: # permn(conjunto) (remova o # para executar) # Extração de uma amostra (sem reposição) composta por 3 elementos do conjunto: amostra_sr=sample(conjunto, 3, replace=FALSE) amostra_sr ## [1] &quot;a&quot; &quot;d&quot; &quot;b&quot; # Extração de uma amostra (com reposição) composta por 3 elementos do conjunto: amostra_cr=sample(conjunto, 3, replace=TRUE) amostra_cr ## [1] &quot;b&quot; &quot;e&quot; &quot;b&quot; Do ponto de vista da quantidade de informação contida na amostra, a amostragem sem reposição é mais adequada. Todavia a amostragem com reposição conduz a um tratamento teórico mais simples, pois ele implica que tenhamos independência entre as unidades selecionadas (não há alteração na probabilidade de seleção). Para populações muito grandes a reposição ou não é irrelevante. Uma vez determinadas as possíveis amostras, segue-se o problema de como elas serão efetivamente extraídas na prática numa amostragem aleatória simples. Numa situação simples como a que acabamos de conceber poderíamos escrever cada uma das 10 (ou 125) possíveis amostras em um pedaço de papel e colocá-los em uma urna para serem sorteados. Ou então enumerar os elementos da lista de possibilidades atribuindo um número a cada um e, em seguida, usar uma tabela de números aleatórios (ou um programa computacional para sua geração) para a escolha dos elementos que integrarão a amostra. Uma AAS raramente é realizada na prática pois é necessário dispor de uma listagem bem definida a priori. Assim, sob circunstâncias reais, um planejamento amostral pode ser definido de modo a assegurar que uma amostra mais informativa, mais barata e rápida possa ser extraída, principalmente quando a amostragem aleatória simples mostrar-se impraticável. Em estudos de larga escala muitas vezes requerem uma abordagem mista. A amostragem mista tem vantagens a nível prático, quando se conhecem algumas informações da população; assim sendo define-se uma característica dos elementos a incluir na amostra, deixando-se os restantes fatores ao acaso. Neste tipo de amostragem salientam-se os seguintes métodos: 1- sistemática; 2- estratificada; e, 3- por conglomerado. 7.10.2 Amostragem aleatória sistemática Figure 7.4: Amostra sistemática Quando os elementos da população estão dispostos sob alguma maneira organizada e aleatória (linha de produção, listagens, … ) a extração de elementos pode ser realizada pela estipulação de um ponto de partida aleatório (o primeiro elemento a ser tomado como integrante da amostra) e de um passo (intervalo), de modo que a seleção dos demais elementos será feita a cada \\(k\\) elementos da listagem. Roteiro: se \\(N\\) é o tamanho da população a ser amostrada; e \\(n\\) o tamanho da amostra que se deseja; calcula-se o passo (intervalo) a ser adotado para a extração dos demais elementos amostrais. O primeiro elemento a ser coletado será aleatoriamente escolhido dentre os \\(k\\) primeiros. \\[ S=\\frac{N}{n} \\] Sorteia-se o ponto de partida (um dos \\(S\\) números do primeiro intervalo) e depois, a cada \\(S\\) elementos da população, retira-se um para fazer parte da amostra, até completar o valor de\\(n\\). Algumas situações possíveis de se encontrar: se \\(S\\) for fracionário pode-se aumentar \\(n\\) até tornar \\(S\\) um inteiro; reduzir \\(N\\) em 1 unidade; se \\(N\\) for um número primo, excluem-se por sorteio alguns elementos da população para tornar \\(S\\) inteiro. Exemplo: considerem uma população composta por pelos seguintes elementos P={1, 2, 3, 4, 5, 6, 7, 8, 9, 10} (N=10) da qual desejamos extrair uma amostra de tamanho 3 (n=3).   O passo \\(S\\) (o intervalo de extração de cada elemento) será igual a \\(S=\\frac{N}{n}=\\frac{10}{3}=3,33\\) (fracionário). Aumentando-se para \\(n=4\\) resultará também em um \\(S\\) fracionário (2,5). Com \\(n\\)=5, \\(S=2\\). O primeiro elemento a integrar a amostra será será aleatoriamente escolhido dentre os 5 (\\(S\\)) primeiros. Assim, as duas possíveis amostras serão: \\[\\begin{align*} A1 &amp; = {1, 3, 5, 7, 9}; e, \\\\ A2 &amp; = {2, 4, 6, 8, 10}. \\\\ \\end{align*}\\] Avaliar, alternativamente, excluir aleatoriamente 1 elemento da população (\\(N=9\\)). Mantendo-se \\(n=3\\) teremos \\(S=3\\). \\[\\begin{align*} A1 &amp; = {1, 4, 7}; \\\\ A2 &amp; = {2, 5, 8}; e, \\\\ A3 &amp; = {3, 7, 9}. \\\\ \\end{align*}\\] Exemplo: uma operadora telefônica pretende saber a opinião de seus assinantes comerciais sobre seus serviços na cidade de Florianópolis. Supondo que há 25.037 assinantes comerciais e a amostra precisa ter no mínimo 800 elementos, mostre como seria organizada uma amostragem sistemática para selecionar os respondentes sabendo que a operadora dispõe de uma lista ordenada alfabeticamente com todos os seus assinantes. Calculando o passo (\\(S\\)):   \\[\\begin{align*} S &amp; = \\frac{N}{n} \\\\ &amp; = \\frac{25037}{800} \\\\ &amp; = 31,29 \\end{align*}\\] Aumentar \\(n\\) não irá resolver o problema (\\(N=25037\\) é um número primo). Arredondar \\(S\\) para cima irá extrapolar o tamanho da população (\\(32 \\times 800=25600 &gt;25037\\)). Podemos arredondar \\(S\\) para baixo (\\(31 \\times 800=24800\\)) para baixo e excluir aleatoriamente 237 elementos da população (é uma população relativamente grande e isso não acarretará problema algum).   Assim nossa amostra será composta por 800 elementos (\\(n\\)) de uma população de (reduzida a) \\(24800\\) elementos. Sorteamos aleatoriamente o primeiro elemento dentre os 31 primeiros da listagem. Os demais, a cada 31 elementos. Na amostragem sistemática deve-se avaliar o risco de periodicidades sistemáticas: se lista de elementos estiver organizada com base em alguma informação da população (escolaridade, renda, …) que possa induzir a algum tipo de viés; se em um processo produtivo for sabidamente reconhecido que falhas podem se tornar mais frequentes a cada certo número de unidades produzidas (máquinas descalibradas). 7.10.3 Amostragem aleatória estratificada Figure 7.5: Amostra estratifiada Quando se pode identificar na população a presença de grupos distintos (estratos) a amostragem estratificada se dá pela realização de amostragens aleatórias simples dentre os elementos de cada um desses grupos.   Um estrato é uma subdivisão da população onde se observa a existência de uma razoável homogeneidade interna da informação desejada. Desse modo, é esencial para que a amostra final tenha qualidade, que entre os estratos estabelecidos exista heterogeneidade e assim, cada indivíduo pertença a apenas um estrato. Há dois modos possíveis de se realizar uma amostragem estratificada: não proporcional; e, proporcional. Em uma amostragem estratificada não proporcional o total de elementos extraídos de cada estrato é igual à razão do tamanho da amostra pelo número de estratos (de cada estrato serão escolhidos aleatoriamente um mesmo número de elementos). Esse modo de extração de elementos implica considerar igual representatividade de cada estrato na população, independentemente de quantos elementos ele abrigue (estratos menores teriam um mesmo peso que estrato maiores). Já na amostragem estratificada proporcional a amostra extraída de cada um dos estratos segue algum critério de ponderação do peso ou variabilidade de cada estrato da população. Na alocação proporcional ao tamanho dos estratos a proporção relativa de cada uma das \\(k\\) amostras extraídas (\\(n_{k}\\)) em relação ao tamanho de cada um dos \\(k\\) extratos (\\(N_{k}\\)) é a mesma (garantindo que estratos maiores tenham mais elementos dentro da amostra final e que estratos menores tenham menos presença nela): \\[ \\frac{n_{1}}{N_{1}} = \\frac{n_{2}}{N_{2}} = \\dots = \\frac{n_{k}}{N_{k}} \\] Onde: \\(N\\) é o tamanho da população; \\(n\\) o tamanho da amostra que se deseja extrair da população; \\(N_{i}\\) é o tamanho do \\(i-ésim\\)o estrato da população, tal que \\(N=N_{1}+N_{2}+\\dots+N_{k}\\); \\(n_{i}\\) o tamanho da \\(i-ésima\\) amostra a ser extraída do \\(i-ésimo\\) estrato, tal que \\(n = n_{1} + n_{2} + \\dots + n_{k}\\). O tamanho da \\(i-ésima\\) amostra a ser extraída de um \\(i-ésimo\\) estrato será determinada em razão do tamanho da amostra que se deseja extrair (\\(n\\)), o tamanho da população (\\(N\\)) e o tamanho do \\(i-ésimo\\) estrato (\\(N_{i}\\)) tal que: \\[ n_{i} = \\frac{N_{i}}{N} \\cdot n \\] para i=1,2,…,k estratos. Exemplo: considerem uma comunidade universitária composta 8000 indivíduos (N=8000) sendo 800 professores (\\(N_{1}=800\\)), 1200 funcionários (\\(N_{2}=1200\\)) e 6000 estudantes (\\(N_{3}=6000\\)), da qual se estipulou extrair uma amostra de tamanho igual a 900 elementos (\\(n=900\\)) para fins de uma pesquisa sobre o estilo de liderança preferido, que se considera ser diferente para cada grupo componente da comunidade acadêmica. Numa amostragem estratificada não proporcional os elementos são extraídos em igual quantidade de cada um dos estratos: 300 professores; 300 funcionários; e, 300 alunos. Numa amostragem estratificada uniforme todas os elementos são extraídos em quantidade de modo independente do peso proporcional dos estratos na população. Esse tipo de amostragem apresenta resultados menos precisos mas, em contrapartida, estudar características de cada camada de forma mais eficiente. Numa amostragem estratificada proporcional os elementos são extraídos de cada um dos estratos considerando-se seus diferentes tamanhos (suas proporções em relação à população total): o estrato dos professores possui \\(N_{p}=800\\) elementos; o estrato dos funcionários possui \\(N_{f}=1200\\) elementos; e, o estrato dos estudantes possui \\(N_{e}=6000\\) elementos. Para uma amostra com um total de \\(n=900\\) elementos seguem-se as quantidades a serem extraídas aleatoriamente de cada u dos três estratos: \\(n_{p}=\\frac{N_{p}}{N}.n=\\frac{800}{8000}.900=90\\) professores; \\(n_{f}=\\frac{N_{f}}{N}.n=\\frac{1200}{8000}.900=135\\) funcionários; \\(n_{e}=\\frac{N_{e}}{N}.n=\\frac{6000}{8000}.900=675\\) alunos; Partindo-se desses tamanhos amostrais determinados (90 professores, 135 funcionários e 675 alunos) pode-se recorrer à extração sistemática usando-se a listagem dessas três categorias: \\(S_{p}=\\frac{N_{p}}{n_{p}}\\) em que \\(S_{p}\\) é o passo a ser seguido na extração de \\(n_{p}\\) professores (90) da ``população’’ de \\(N_{p}\\) professores (800); \\(S_{f}=\\frac{N_{f}}{n_{f}}\\) em que \\(S_{f}\\) é o passo a ser seguido na extração de \\(n_{f}\\) funcionários (135) da ``população’’ de \\(N_{f}\\) funcionários (1200); e, \\(S_{e}=\\frac{N_{e}}{n_{e}}\\) em que \\(S_{e}\\) é o passo a ser seguido na extração de \\(n_{e}\\) alunos (675) da ``população’’ de \\(N_{e}\\) alunos (6000). Muitos ajustes devem ser feitos pois, de modo frequente, os resultados dos passos obtids na prática resultam em números fracionários. Todavia, devemos ter sempre procurar não reduzir o tamanho amostral e ter em mente que o tamanho da população não pode ser aumentado. Para os professores: \\(S_{p}=\\frac{800}{90}=8,88\\). Se tomamos \\(S_{p}=9\\) (um professor a cada nove da lista) veremos que para extrair 90 professores a população teria de ser de 810 (a população é de 800). Uma das opções seria usar \\(S_{p}=8\\) e se extrair 100 professores (uma amostra um pouco maior). Outra possibilidade seria ainda usar \\(S_{p}=8\\) remover aleatoriamente 80 professores da população e então tomar 90 professores (pois com \\(S_{p}=8\\), \\(8 . 90=720\\)). Para os funcionários: \\(S_{f}=\\frac{1200}{135}=8,88\\) (o mesmo porque essas amostras foram estabelecidas de modo proporcional). Se tomamos \\(S_{f}=9\\) (um funcionário a cada nove da lista) veremos que para extrair 135 funcionários a população teria de ser de 1215 (a população é de 1200). Uma das opções seria usar \\(S_{f}=8\\) e se extrair 150 funcionários (uma amostra um pouco maior). Outra possibilidade seria ainda usar \\(S_{f}=8\\) remover aleatoriamente 120 funcionários da população e então tomar 135 funcionários (pois com \\(S_{f}=8\\), \\(8 . 135=1080\\)). Do mesmo modo para os alunos: \\(S_{e}=\\frac{6000}{675}=8,88\\) (o mesmo porque essas amostras foram estabelecidas de modo proporcional). Se tomamos \\(S_{e}=9\\) (um aluno a cada nove da lista) veremos que para extrair 675 alunos a população teria de ser de 6075 (a população é de 6000). Uma das opções seria usar \\(S_{e}=8\\) e se extrair 750 alunos (uma amostra um pouco maior). Outra possibilidade seria ainda usar \\(S_{e}=8\\) remover aleatoriamente 600 funcionários da população e então tomar 600 alunos (pois com \\(S_{e}=8\\), \\(8 . 135=1080\\)). Ao final poderíamos extrair então 100 professores, 150 funcionários e 750 alunos; ou, pela segnda possibilidade, extrair 90 professores, 135 funcionários e 600 alunos (eliminando-se aleatoriamente elementos das populações antes de se sistematizar a extração, como antes explicado). A proporção de elementos extraídos de cada um dos estratos é constante entre os extratos, asseguando uma extração proporcional: \\[ \\frac{100}{800} = \\frac{150}{1200} =\\frac{750}{6000}=0,125 \\\\ \\frac{90}{720} = \\frac{135}{1080} =\\frac{675}{5400}=0,125 \\] Pode-se otimizar uma amostragem estratificada proporcional consideran-de também sua variabilidade interna. O tamanho de cada uma das amostras (\\(n_{1},n_{2},\\dots,n_{k}\\)) dos diferentes estratos são proporcionais aos tamanhos dos estratos (\\(N_{1},N_{2},\\dots, N_{k}\\)) e também segundo algum critério adicional (otimização), como a variabilidade interna de cada estrato (\\(\\sigma_{1},\\sigma_{2},\\dots,\\sigma_{k}\\)) de modo a se manter iguais as razões: \\[ \\frac{n_{1}}{N_{1} \\cdot \\sigma_{1}} = \\frac{n_{2}}{N_{2} \\cdot \\sigma_{2}} = \\dots = \\frac{n_{k}}{N_{k} \\cdot \\sigma_{k}} \\] Onde: \\(N\\) é o tamanho da população; \\(n\\) o tamanho da amostra que se deseja extrair da população; \\(N_{i}\\) é o tamanho do \\(i-ésim\\)o estrato da população, tal que \\(N=N_{1}+N_{2}+\\dots+N_{k}\\); \\(n_{i}\\) o tamanho da \\(i-ésima\\) amostra a ser extraída do \\(i-ésimo\\) estrato, tal que \\(n = n_{1} + n_{2} + \\dots + n_{k}\\);e, \\(\\sigma_{i}\\) é o desvio padrão do \\(i-ésimo\\) estrato. O tamanho da \\(i-ésima\\) amostra a ser extraída de um \\(i-ésimo\\) estrato será determinada em razão do tamanho da amostra que se deseja extrair (\\(n\\)), o tamanho da população (\\(N\\)), do tamanho e variabilidade do \\(i-ésimo\\) estrato (\\(N_{i}\\) e \\(\\sigma_{i}\\)) tal que: \\[ n_{i} =\\frac{ n \\cdot N_{i} \\cdot \\sigma_{i} }{ N_{1} \\cdot \\sigma_{1} + N_{1} \\cdot \\sigma_{1} + \\dots+ N_{k} \\cdot \\sigma_{k}} \\] para i=1,2,…, k estratos. Exemplo: considere estudar a opinião de estudantes de uma universidade com relação à legalização do aborto. A equipe possui dados descritivos relacionados ao sexo, orientação religiosa e rendimento médio familiar de toda a comunidade acadêmica. Na revisão bibliográfica identifica-se que algumas das variáveis que habitualmente implicam em opiniões diferentes (escolaridade e idade) já não mais precisam ser consideradas; todavia, outras ainda devem ser consideradas. Assim, um plano de estratificação de vários niveis pode ser estabelecido partindo-se da premissa de homogeneidade de opinião interna em cada um deles: sexo, orientação religiosa e rendimento familiar. Considerando uma amostra de \\(n=1.000\\) estudantes e as seguintes medidas descritivas disponibilizadas pela universidade e relacionadas à sua população de estudantes: sexo: 35% masculino e 65% feminino; orientação religiosa: 60% católica; 20% evangélica; 10% sem; 5% espírita e 5% outras; e, rendimento médio mensal familiar: 35% até R$ 4.000,00, 65% acima de R$ 4.000,00. podemos estabelecer vária camadas estratificadas proporcionalmente, tal como a ilustrado na Figura 7.6. Figure 7.6: Plano de estratificação proporcional 7.10.4 Amostragem aleatória por conglomerados Figure 7.7: Amostragem por conglomerados Muitas vezes a dispersão espacialde uma população a ser investigada torna impeditiva uma amostragem aleatória simples.   Um modo de contornar essa dificuldade é dividir a área total onde se assenta a população de interesse em várias áreas geográficas menores e sem sobreposição, tais como cidades, regionais de cidades, bairros, quarteirões de um bairro, …. Essa subdivisão pode também ser realizada valendo-se de critérios organizacionais como, por exempo, universidades, escolas, grau escolar, departamentos de uma empresa, …. As subpopulações que se localizam nessas áreas menores passam a ser denominadas de conglomerados e são como que representações em escala reduzida da população total. A heterogeneidade presente na população original passa a estar representada dentro de um conglomerado. Ou seja, é essencial para a qualidade final da amostra extraída desse modo, que os elementos dentro de cada conglomerado sejam tão diversos quanto a diversidade que se observa nos elementos da população total (a ideia de representação em escala reduzida). Em uma amostragem de apenas 1 estágio, após serem aletariamente sorteados um certo número de conglomerados, todos os elementos internos desses conglomerados são estudados. Todavia, considerando que os elementos de um conglomerado natural dentro de uma população são habitualmente mais homogêneos do que os elementos da população total (os moradores de um bairro são mais semelhantes entre si do que todos os moradores do município), pode não ser necessário um grande número de elementos para se representar adequadamente um conglomerado natural. Uma diretriz científica num processo de amostragem por conglomerados é maximizar o número de conglomerados e diminuir o número de elementos aleatoriamente escolhidos dentro de cada um deles. Recomenda-se observar as diferenças de tamanho existentes entre cada conglomerado, de modo a equilibrar a probabilidade. A probabilidade de seleção de um elemento num desenho de amostragem com probabilidade proporcional ao tamanho: na primeira etapa é dada a cada conglomerado uma oportunidade de seleção proporcional ao seu tamanho; e, na segunda etapa um mesmo número de elementos é escolhido dentro de cada conglomerado selecionado. Esses procedimentos igualam as probabilidades últimas de seleção de todos os elementos da população pois: conglomerados com mais elementos têm maior probabilidade de serem selecionados; e, elementos em conglomerados maiores têm menor chance de seleção do que elementos em conglomerados menores. Exemplo: a população universitária de Londrina (estimada em 25.000 estudantes) pode ser entendida como distribuída em vários conglomerados organizacionais como, por exemplo: UEL; UNIFIL; PUC; INESUL; UTFPr; Arthur Thomas; CESUMAR; Pitágoras; Positivo; …. Se desejamos realizar uma pesquisa entre os estudantes universitários de Londrina (na qual sabe-se que não fará diferença se a instituição é pública ou privada) podemos sortear aleatoriamente alguns desses conglomerados.   Entretanto, lembrando que todos os elementos de um conglomerado devem ser entrevistados, pode ser que o número de estudantes em cada conglomerado escolhido ainda seja por demais elevado.   Nesse caso, um segundo estágio (como, por exemplo, utilizar a subdivisão administrativa que as universidades habitualmente adotam ao se subdividir em diversos centrso de estudos como conglomerados dentro dela) pode ser proposto. Assim como na estratificação, a proposição de conglomerados deve sempre consider as variáveis condicionantes relacionadas com o objeto de estudo para que as informações de todas as unidade amostrais finais a serem entrevistadas possa ser usada seguramente para se inferir sobre a informação na população sob estudo. Exemplo: a Pesquisa Nacional por Amostra de Domicílios (PNAD) do IBGE coleta informações demográficas e socioeconômicas sobre a população brasileira. Sinteticamente, utiliza amostragem por conglomerados em três estágios: primeiro estágio: amostras de municípios (conglomerados) para cada uma das regiões geográficas do Brasil (Norte, Nordeste, Centro-Oeste, Sudeste e Sul); segundo estágio: setores censitários sorteados (subdivisão estabelecida pelo IBGE dentro de um município) em cada município (conglomerado sorteado); terceiro estágio: domicílios sorteados aleatoriamente em cada setor censitário. Figure 7.8: Ilustração comparativa dos principais modos de extração de amostras Exemplo: considere estudar a opinião de estudantes universitários de toda uma cidade com relação à legalização do aborto. A equipe possui dados descritivos relacionados ao sexo, orientação religiosa e rendimento médio familiar de toda a comunidade acadêmica. Na revisão bibliográfica identifica-se que algumas das variáveis que habitualmente implicam em opiniões diferentes (escolaridade e idade) já não mais precisam ser consideradas; todavia, outras ainda devem ser consideradas. Assim, um plano de estratificação de vários niveis pode ser estabelecido partindo-se da premissa de homogeneidade de opinião interna em cada um deles: sexo, orientação religiosa e rendimento familiar. Nesse caso, podemos considerar cada universidade como um conglomerado. Numa primeira etapa promovemos um sorteio e, na sequência, uma estratitificação da amostra total em termos da população estudantil de cada conglomearado. A partir desse ponto, em cada universidade, estratificações suplementares são feitas para se considerar proporcionalmente as diferentes opiniões (sexo, orientação religiosa, renda). Ao final, após vários estágios, uma amostra não probabilística pode ser extraída de cada grupo individualizado anteriormente, tal como a ilustrado na Figura 7.9. Figure 7.9: Planejamento da extração da amostra "],["amostragem-não-probabilística.html", "7.11 Amostragem não probabilística", " 7.11 Amostragem não probabilística Não obstante os métodos de amostragem probabilísticos serem adequados à generalização da informação colhida, há diferentes situações para as quais podemos nos decidir por métodos probabilísticos como, por exemplo, para tornar a pesquisa menos custosa financeiramente ou ainda porque talvez não seja necessário ter um elevado rigor e precisão nas estimativas que se deseja obter. Amostragens não probabilísticas são aquelas em que a amostra é extraída de modo dirigido ( intencional, não aleatório) pelo pesquisador em decorrência da natureza de seu estudo, não sendo consideradas a probabilidade de seleção de seus elementos. 7.11.1 Amostragem por conveniência Esta técnica é muito comum e consiste em se selecionar uma amostra da população imediatamente acessível (prontamente disponível). Considerem, por exemplo, pesquisar a opinião de estudantes universitários em Londrina sobre determinado assunto.   Poderíamos considerar cada universidade localizada em Londrina como um conglomerado e, dentro delas, realizar uma amostragem aleatória de todos os seus estudantes (ou parte, se realizarmos o delineamento em mais de um estágio).   Por conveniência podemos simplesmente decidir ir a um encontro de estudantes universitários que se realiza na cidade e perguntar a alguns deles que se declarem estudar em Londrina qual sua opinião sobre aquele assunto. As limitações desse tipo de amostragem são óbvias posto poder haver no grupo de entrevistados diferentes segmentos sociais, econômicos, políticos, filosóficos, religiosos dentre muitos outros fatores de diferenciação, que podem ser fundamentais face às opiniões que se deseja colher sobre o assunto inquerido, resultando em graves distorções. Esse tipo de amostragem, embora não aleatória, é bastante utilizada na área de marketing na qual geralmente as amostras são obtidas em locais com aglomerações, como teatros, cinemas, mercados, …. Neste caso, é importante o senso crítico do pesquisador para evitar vieses, por exemplo, não selecionar sempre pessoas de mesmo sexo, de mesma faixa etária, …. 7.11.2 Amostragem por cotas A amostragem por cotas assemelha-se com a amostragem estratificada proporcional; mas, ao contrário da amostragem estratificada, a seleção final (no estrato) não precisa ser aleatória. A população é vista de forma segregada (estratificada), dividida em diversos subgrupos como sexo, idade, raça, local de residência, ocupação, …. Para compensar a falta de aleatoriedade na seleção, costuma-se dividir a população num grande número de subgrupos e seleciona-se (não aleatoriamente) uma quantidade de elementos em cada subgrupo, proporcional ao seu tamanho. Numa pesquisa socioeconômica, a população pode ser dividida por localidade, por nível de instrução, por faixas de renda, … "],["dimensionamento-de-amostras.html", "7.12 Dimensionamento de amostras", " 7.12 Dimensionamento de amostras 7.12.1 Erros Há de distinguir dois tipos de erros associados a levamentamentos amostrais: erros amostrais, as diferenças entre o resultado obtido em uma amostra específica (uma estatística) e seu verdadeiro valor na população (o parãmetro); erros não amostrais (experimentais), decorrentes de dados amostrais coletados incorretamente, inconsistentemente, fruto de erros nas transcrições, delineamentos fracamente estabelecidos (resultando em amostras tendenciosas), leituras instrumentais imprecisas (resultantes da perda da calibração dos instrumentos ou operação por técnicos com diferentes habilidades). Os erros amostrais ocorrem porque as amostras são aleatórias: se de um grupo de 100 números extrairmos uma amostra aleatória de 10 deles a média amostral calculada teria um valor diferente a cada diferente amostra extraída (essa flutuação é assunto da teoria da distribuição das médias e proporções amostrais). Já os erros não amostrais devem ser minimizados ou melhor não existir. A determinação do tamanho de uma amostra (\\(n_{0}\\)) é função do erro amostral tolerável e do nível de significância \\(\\alpha\\) estabelecido a priori pelo pesquisador que se relaciona ao nível de confiança pretendido por \\((1-\\alpha)\\). Assim, se o nível de significância máximo admissível para o assunto pesquisado é \\(\\alpha=0,05\\), o nível de confiança será \\((1-\\alpha)=0,95\\) (uma vez que: \\(\\alpha + (1-\\alpha)=1\\)). Valores críticos de zc correspondentes a alguns níveis de significância (confiança) Níveis de confiança (1 − α): 0,80 0,90 0,95 0,99 0,999 Níveis de significância α: 0,20 0,10 0,05 0,01 0,001 zc 1,28 1,64 1,96 2,57 3,29 Todavia, como mais adiante se verá, há situações nas quais o valor crítico referente ao nível de confiança estabelecido e que será empregado no dimensionamento da amostra será obtido de uma outra distribuição (t de Student). 7.12.2 Determinação do tamanho de uma amostra para estimação da média populacional Determinação do tamanho \\(n_{0}\\) de uma amostra para estimação da média considerando-se uma população infinita (\\(N \\ge 20.n_{0}\\)) e seguindo uma distribuição Normal: \\[ n_{0} = \\frac{z_{c}^2 \\cdot \\sigma^2}{\\varepsilon^2} \\] em que: \\(n_{0}\\): é o tamanho amostral; \\(z_{c}\\): valor crítico tabelado da distribuição Normal usado para o nível de significância desejado (por exemplo, para \\(\\alpha\\)=5%, \\(z_{c}=1,96\\)); \\(\\sigma\\) desvio padrão populacional obtido em estudos prévios; e, \\(\\varepsilon\\): é o erro amostral, a máxima diferença entre \\(\\mu\\) e \\(\\stackrel{-}{x}\\) que se espera observar sob um nível de confiança de (\\(1-\\alpha\\)) . Exemplo: Qual o tamanho de amostra necessária para se estimar o peso médio de cervos em uma dada população sob estudo, admitida infinita. Sabe-se de estudos anteriores que o desvio padrão \\(\\sigma\\) do peso para animais dessa idade é de 30 kg. Utilize um erro \\(\\varepsilon\\) de 10 kg na estimação e um nível confiança \\((1-\\alpha)\\) de 95%.   \\[\\begin{align*} n_{0} &amp; = \\frac{Z^{2} \\cdot \\sigma^{2}}{\\varepsilon^{2}} \\\\ n_{0} &amp; = \\frac{1,96^{2} \\cdot 30^{2}}{10^{2}} \\\\ n_{0} &amp; \\sim 35 \\end{align*}\\] Se a população não pode ser considerada infinita, ou seja, se \\(N &lt; 20.n_{0}\\), então aplica-se uma correção sobre o valor inicialmente calculado para a (\\(n_{0}\\)) obtendo-se um novo tamanho (\\(n\\)): \\[ n=\\frac{N \\cdot n_{0}}{N + n_{0}} \\] No exemplo anterior, caso a população sob estudo fosse composta por apenas 200 animais (\\(N &lt; 20.n_{0}\\)) o tamanho da amostra seria: \\[\\begin{align*} n &amp; = \\frac{N \\cdot n_{0}}{N + n_{0}} \\\\ n &amp; = \\frac{200 \\cdot 35}{200 + 35 } \\\\ n &amp; \\sim 30 \\end{align*}\\] O conhecimento prévio do desvio padrão populacional (\\(\\sigma\\)) para utilizar as expressões acima é quase que uma exceção. Na maioria dos estudos ele é desconhecido e a única informação disponível acerca da variabilidade é o desvio padrão amostral \\(S\\). Nesse cenário, a variável Norma padronizada \\(Z\\) é substituída por uma outra, que segue a distribuição “t” de Student e, para se obter seu valor crítico \\(t_{c}\\) para um determinado nível de confiança desejado necessitamos ter uma informação adicional: os graus de liberdade (gl ou df), que são iguais ao tamanho da amostra menos 1 (\\(gl=n_{0}-1\\)). Observa-se que para \\(n \\to \\infty\\), os valores críticos de \\(z_{c}=t_{c}\\) para um mesmo nível de significância. Ocorre porém que, não tendo ainda sido retirada a amostra, não dispomos do valor de \\(s\\). Se não conhecemos nem ao menos um limite superior para \\(\\sigma\\), a única solução será colher uma amostra piloto de \\(n_0\\) elementos para, com base nela obtermos uma estimativa de \\(s\\) e estimarmos o tamanho amostral pela expressão: \\[ n = \\frac{t_{c}^2 \\cdot s^2}{\\varepsilon^2} \\] com \\(s\\) calculado sobre a amostra piloto de \\(n_{0}\\) elementos e com \\(t_{c}\\) obtido em uma tabela de valores da distribuição ``t’’. Essas tabelas de valores consideram nas suas colunas variados níveis de sginificância \\(\\alpha\\) e, nas suas linhas uma informação chamada de graus de liberdade (gl) Graus de liberdade não mais são, no contexto estudado, que o tamanho da amostra piloto \\(n_{0}\\) menos 1 (\\(gl=n_{0}-1\\). Assim, se a amostra piloto for de 5 elementos, gl=4 e será nessa linha, junto à coluna do nível de significância desejado, que o valor “t” será encontrado. Figure 7.10: Tabela t de Stdent: cada linha refere-se a um gl e cada coluna a um nível de significância (no cruzamento tem-se o valor crítico de t sob essas condições) Observe que à medida que o tamanho da amostra cresce, o valor crítico \\(t_{c}\\) se aproxima do valor crítico \\(_{c}\\) para um mesmo nível de significância. Por exemplo, para um \\(\\alpha=5\\%\\) uma amostra de 121 (df=121-1=120) elementos possui um valor crítico \\(t_{c}=1,96\\) (na distribuição de Student) e um valor crítico \\(z_{c}=1,96\\) (distribuição Normal padrão). Se \\(n \\le n_{0}\\), a amostra piloto já terá sido suficiente para ser usada na análise. Caso contrário, deve-se retirar mais elementos da população, recalcular o tamanho da amostra \\(n\\) até se observe essa desigualdade. 7.12.2.1 Margem de erro em uma estimativa amostral da média Reescrevendo-se a expressão para a determinação do tamanho amostral podemos exprimir o erro \\(\\varepsilon\\) associado à estimativa obtida de uma amostra de tamanho \\(n\\): \\(\\hat{p}\\) da média populacional \\[ \\varepsilon = z_{c}\\cdot \\sqrt{\\frac{\\sigma^{2}}{n}} \\] em que \\(\\varepsilon\\) é uma quantidade para mais e para menos da estimativa obtida de uma amostra de tamanho \\(n\\) em relação a \\(\\mu\\) sob o nível de confiança \\(1-\\alpha\\) que determina \\(z_{c}\\). A expressão anterior considera que a variâcia popuacional \\(\\sigma^{2}\\) é conhecida. Caso não se tenha informação alguma sobre seu valor, seguem-se as mesmas considerações relacionadas ao tamanho \\(n\\) da amostra: se \\(n \\ge 30\\), adotar a variância amostral \\(S^{2}\\) como aproximação de \\(\\sigma^{2}\\); se \\(n &lt; 30\\), adotar a variância amostral \\(S^{2}\\) como aproximação de \\(\\sigma^{2}\\) usando-se o valor crítico \\(t_{c}\\) da distribuição de Student (com gl/df iguais ao tamanho da amostra menos 1) 7.12.3 Determinação do tamanho de uma amostra para estimação da proporção populacional A determinação do tamanho de uma amostra para estimação da proporção populacional considerando-se uma população infinita (\\(N \\ge 20. n_{0}\\)): \\[ n_{0} = \\frac{z_{c}^{2} \\cdot \\pi \\cdot (1- \\pi) }{\\epsilon^{2}} \\] em que: \\(n_{0}\\) é o tamanho da amostra; \\(z_{c}\\) é valor crítico tabelado da distribuição Normal para o nível de significância desejado (por exemplo, para \\(\\alpha\\)=5%, \\(z_{c}\\)=1,96); \\(\\pi\\) é a proporção populacional; \\(\\varepsilon\\): é o erro amostral, a máxima diferença entre \\(\\pi\\) e \\(p\\) que se espera observar sob um nível de confiança de (\\(1-\\alpha\\)) . Quando não se dispõe de nenhuma informação a priori sobre a proporção populacional (\\(\\pi\\)) a adoção do máximo valor possível ao produto: \\(\\pi . (1- \\pi )=\\frac{1}{4}\\) assegura que o o tamanho de amostra obtido será suficiente para a estimação qualquer que seja a proporção populacional \\(\\pi\\). Isso equivale a considerar: \\[ n_{0} = \\frac{z_{c}^{2}}{\\epsilon^{2}} \\cdot \\frac{1}{4} \\] De modo análogo, se a população não pode ser considerada infinita (\\(N &lt; 20n_{0}\\)) aplica-se uma correção sobre o valor calculado do tamanho da amostra (\\(n_{0}\\)) chegando-se a um novo tamanho (\\(n\\)): \\[ n=\\frac{N \\cdot n_{0}}{N + n_{0}} \\] Exemplo:Qual o tamanho de amostra (\\(n_{0}\\)) suficiente para estimarmos a proporção da área com solo contaminado que necessita de certo tratamento de descontaminação, com precisão (\\(\\varepsilon\\)) de 0,02 e um nível de confiança (\\(1-\\alpha\\)) de 95%, sabendo que essa proporção seguramente não é superior a 0,2? \\[\\begin{align*} n_{0} &amp; = \\frac{z_{c}^{2} \\cdot \\pi \\cdot (1-\\pi) }{\\varepsilon^{2}} \\\\ n_{0} &amp; = \\frac{1,96^{2} \\cdot 0,20 \\cdot 0,80 }{0,02^{2}}\\\\ n_{0} &amp; \\sim 1.537 \\end{align*}\\] Considerando-se uma estimativa conservadora para \\(\\pi.(1-\\pi)\\) pelo máximo valor possível desse produto (\\(\\frac{1}{4}\\)) teremos: \\[\\begin{align*} n_{0} &amp; = \\frac{z_{c}^{2}}{\\varepsilon^{2}} \\cdot \\frac{1}{4} \\\\ n_{0} &amp; = \\frac{1,96^{2}}{0,02^{2}} \\cdot \\frac{1}{4} \\\\ n_{0} &amp; = 2.401 \\end{align*}\\] 7.12.3.1 Margem de erro em uma estimativa amostral da proporção Reescrevendo-se a expressão para a determinação do tamanho amostral para a situação na qual não temos nenhuma informação sobre a proporção populacional (\\(\\pi\\)), podemos exprimir o erro \\(\\varepsilon\\) associado à estimativa da proporção (\\(p\\)) obtida de uma amostra de tamanho \\(n\\) da proporção populacional (\\(\\pi\\)) sob o nível de confiança (\\(1-\\alpha\\)) pelo critério mais conservador (\\(\\pi.(1-\\pi)=\\frac{1}{4}\\)) \\[\\begin{align*} \\varepsilon &amp; = z_{c}\\cdot \\sqrt{\\frac{\\pi\\cdot \\left(1-\\pi\\right)}{n}} \\\\ \\varepsilon &amp; = z_{c}\\cdot \\sqrt{\\frac{\\frac{1}{4}}{n}} \\end{align*}\\] em que \\(\\varepsilon\\) é uma quantidade para mais e para menos da estimativa \\(p\\) obtida de uma amostra de tamanho \\(n\\) em relação a \\(\\pi\\) sob o nível de confiança \\(1-\\alpha\\) que determina \\(z_{c}\\). Exemplo: Uma pesquisa recente mostra o apoio dos eleitores a uma posição de liberação das restrições sobre a pesquisa de células estaminais embrionárias e permitir o uso médico do princípio ativo da . A pesquisa realizada para o descobriram que 50% dos prováveis eleitores de Michigan apoiam a proposta de células-tronco, 32% são contra e 18% indecisos. A pesquisa telefônica ouviu 602 prováveis eleitores de Michigan. Qual a margem de erro a um nível de significância de 95% para os eleitores a favor da liberação das pesquisas? (link: Elgin C. College) \\[\\begin{align*} \\varepsilon &amp; = {z}_{(\\frac{\\alpha }{2})}\\cdot \\sqrt{\\frac{\\hat{p}\\cdot \\left(1-\\hat{p}\\right)}{n}}\\\\ &amp; = 1,96 \\cdot \\sqrt{\\frac{0,50 \\cdot \\left(1- 0,50 \\right)}{602}}\\\\ &amp; = 0,04 \\end{align*}\\] A margem de erro é de 4 pontos percentuais para cima ou para baixo (46%; 54%) na proporção de eleitores em relação à proporção populacional \\(\\pi\\) a favor da liberação das pesquisas, sob um um nível de confiança de 95% Independent Samples T-Test 95% CI for Cohen cline6-7 t df p Cohen Lower Upper 1-7 engagement 2.365 38 0.023 0.748 0.101 1.385 "],["introdução-às-estatísticas-epidemiológicas.html", "Capítulo 8 Introdução às estatísticas epidemiológicas ", " Capítulo 8 Introdução às estatísticas epidemiológicas "],["tipos-de-estudos-epidemiológicos.html", "8.1 Tipos de estudos epidemiológicos", " 8.1 Tipos de estudos epidemiológicos Figure 8.1: Estudos epidemiológicos Quanto ao objetivo Exploratórias (descritivas) Explicativas (analíticas) Quanto ao tempo Transversais Longitudinais Quanto à natureza Observacional Experimental Estudos descritivos têm como foco principal a caracterização de uma população ou fenômeno em um momento específico, sem tentar estabelecer relações de causa e efeito. Esses estudos fornecem uma visão geral de uma situação ou contexto, frequentemente sendo usados para medir a prevalência de doenças ou comportamentos. Um estudo descritivo que relata a prevalência de diabetes em uma determinada região, baseado em dados coletados em um hospital. Estudos analíticos observacionais buscam investigar possíveis associações entre variáveis, sem que o pesquisador manipule as condições em estudo. Estes estudos podem ser transversais, quando os dados são coletados em um único momento, ou longitudinais, quando há um acompanhamento ao longo do tempo. Um estudo observacional que investiga a relação entre o consumo de alimentos ricos em gordura e doenças cardíacas, acompanhando os participantes ao longo do tempo, sem alterar seus comportamentos. Estudos analíticos experimentais envolvem a manipulação ativa de variáveis pelo pesquisador, em um ambiente controlado, para avaliar seus efeitos sobre outras variáveis. Esses estudos, geralmente longitudinais, são considerados a melhor abordagem para investigar causalidade, pois permitem o controle de fatores de confusão e a comparação direta entre grupos. U Um ensaio clínico em que um grupo de pacientes recebe um novo medicamento e outro grupo recebe um placebo para avaliar a eficácia do medicamento. "],["estudos-transversais.html", "8.2 Estudos transversais", " 8.2 Estudos transversais Um estudo transversal é um tipo de investigação observacional em que os dados são coletados em um único ponto no tempo ou durante um período curto. Esse tipo de estudo fornece uma “fotografia” instantânea das condições, comportamentos, ou características de uma população ou grupo em um determinado momento. Embora útil para fornecer uma visão geral da saúde da população, esse tipo de estudo não permite inferir relações causais entre fatores de risco e desfechos, sendo mais adequado para levantar hipóteses. Um estudo transversal pode ser usado para determinar a prevalência de hipertensão em uma população: um grupo de pessoas é examinado em um momento específico e o número de pessoas com hipertensão é registrado. Esse estudo dá uma visão geral da saúde da população naquele momento, mas não pode determinar se a hipertensão é causada por fatores observados no estudo. "],["estudos-longitudinais.html", "8.3 Estudos longitudinais", " 8.3 Estudos longitudinais Um estudo longitudinal é um tipo de investigação observacional em que os dados são coletados ao longo de um período prolongado, envolvendo múltiplas observações em diferentes momentos no tempo. Esse tipo de estudo permite acompanhar mudanças, tendências e a evolução de condições, comportamentos ou características ao longo do tempo, oferecendo uma perspectiva dinâmica dos fenômenos estudados. Diferentemente dos estudos transversais, os estudos longitudinais são mais adequados para identificar relações temporais entre fatores de risco e desfechos, permitindo melhor compreensão sobre possíveis relações causais. Um estudo longitudinal sobre hipertensão acompanharia o mesmo grupo de pessoas por vários anos, medindo regularmente sua pressão arterial e sua exposição a fatores de risco, permitindo assim observar como a pressão arterial se desenvolve ao longo do tempo e identificar quais fatores podem estar associados ao seu aparecimento ou progressão. 8.3.1 Estudos de casos e controles Figure 8.2: Estudo de casos e controles (retrospectivo) Um estudo de caso-controle, geralmente retrospectivo, começa com a identificação de um grupo de casos (indivíduos com uma doença) e um grupo de controles (indivíduos sem a doença). Por meio da anamnese o profissional de saúde ajuda o paciente a se recordar (pesquisa retrospectiva) de situações no passado que possam, de algum modo, configurar a exposição ao fator de risco pesquisado. As prevalências de exposição a um determinado fator são então medidas nos dois grupos e comparadas. Se a prevalência da exposição for maior nos casos do que nos controles, esta exposição pode então ser um fator de risco para a doença. Se a prevalência for menor entre os casos, então esta exposição pode ser um fator de proteção para a doença. Este método é particularmente útil para investigar doenças raras ou condições com longos períodos de incubação. Algumas vantagens: são estudos relativamente baratos podem investigar vários possíveis fatores de risco e são úteis para doenças raras Algumas desvantagens: são muito vulneráveis a vícios de seleção e observação não são adequados para investigar exposições raras não podem obter estimativas da incidência de doença 8.3.2 Estudos de coorte Figure 8.3: Estudos de coorte (prospectivos) Estudos de coorte são prospectivos. Assim, um grupo de indivíduos expostos e outro grupo de não expostos a uma causa potencial de doença são acompanhados ao longo do tempo. A incidência da doença é então comparada entre os dois grupos. Para conduzir este tipo de estudo, é fundamental que uma hipótese clara seja formulada previamente ao início da investigação. Considerando que esses estudos tendem a ser muito caros, eles geralmente são implementados somente depois que a hipótese foi explorada com outros desenhos de estudo mais econômicos. Algumas vantagens: a exposição é medida antes do início da doença; as exposições raras podem ser estudadas selecionando grupos de indivíduos apropriados; a incidência da doença pode ser medida nos grupos de expostos e não expostos. Algumas desvantagens: o estudo pode ser extenso e caro, especialmente se o período necessário para observar o efeito for prolongado; mudanças na condição de exposição e nos critérios diagnósticos podem ocorrer durante o período do estudo e isto pode afetar a classificação dos indivíduos em expostos e não expostos e em doentes e não doentes; a perda de indivíduos durante o seguimento pode introduzir sérios vieses no estudo. 8.3.3 Estudos clínicos aleatorizados Figure 8.4: Estudos clínicos aleatorizados Em estudos epidemiológicos experimentais, também conhecidos como estudos de intervenção ou ensaios clínicos, os indivíduos participantes são alocados a diferentes grupos de acordo com a presença ou não de exposição. No entanto, nesses estudos é o pesquisador quem define quais os indivíduos que receberão a exposição e esta exposição é uma medida preventiva ou terapêutica. Este tipo de estudo tem como principal vantagem a possibilidade de garantir a validade dos resultados. Os estudos experimentais são classificados em dois grandes grupos: as intervenções terapêuticas e as intervenções preventivas. As intervenções terapêuticas incluem pacientes que apresentam uma condição de saúde específica e o objetivo é avaliar a capacidade de determinada intervenção produzir a recuperação, reduzir sintomas, prevenir recrudescimento ou diminuir o risco de uma evolução desfavorável. Para este tipo de estudo, a unidade de amostragem e análise é o indivíduo. As intervenções preventivas envolvem pessoas sadias e o objetivo é avaliar a capacidade de uma intervenção em prevenir a ocorrência de um evento indesejado. As unidades de amostragem nesses casos podem ser tanto os indivíduos como comunidades. Considerando que os participantes são deliberadamente selecionados pelo pesquisador para receber ou não uma intervenção, os estudos epidemiológicos experimentais envolvem questões éticas importantes e estão sujeitos a regulação legal. "],["terminologia.html", "8.4 Terminologia", " 8.4 Terminologia Epidemiologia A epidemiologia é uma ciência médica que se concentra na distribuição e nos determinantes (fatores de risco) da frequência das doenças na população (desfechos) , examinando seus padrões em busca de determinar por que alguns grupos ou certos indivíduos desenvolvem uma doença ao passo que outros não. Estudos epidemiológicos Estudos epidemiológicos são experimentos científicos realizados com o propósito mais comum de se desejar saber se determinadas características pessoais, hábitos ou aspectos do ambiente onde uma pessoa vive estão associados com certa doença, manifestações de uma doença ou outro evento de interesse do pesquisador. Desfecho (“sucesso”) Desfecho é o termo usado para designar a ocorrência do evento de interesse em uma pesquisa. O desfecho pode ser o surgimento de uma doença, de um determinado sintoma, o óbito ou qualquer outro evento relacionado ao processo de saúde-doença. Uma dificuldade inerente está em quantificar a intensidade do desfecho. Fator de risco (fator sob estudo) Fator de risco é a denominação usada em Epidemiologia para designar uma variável que se supõe estar associada ao desfecho. Refere-se portanto a um aspecto de hábitos pessoais ou a uma exposição ambiental, que pode estar associada a uma maior probabilidade de ocorrência de uma doença. Uma dificuldade inerente reside em como quantificar a exposição. Risco Por risco entende-se a “a probabilidade de um membro de uma população definida desenvolver uma dada doença (ou condição) em um período de tempo”. Perceba que nesta definição é possível observar três elementos: base populacional, doença (ou condição) e tempo. População em risco Um fator importante no cálculo das medidas da frequência de uma doença é a estimativa correta do número de pessoas em estudo. Idealmente, esses números devem incluir apenas pessoas potencialmente suscetíveis às doenças (ou condições) em estudo. Por exemplo: homens não devem ser incluídos no cálculo da frequência de câncer do colo do útero e, vice-e-versa para câncer de próstata. Uma vez que os fatores de risco geralmente podem ser modificados, intervir para alterá-los em uma direção favorável pode reduzir probabilidade de ocorrência da doença. O resultado dessas intervenções pode ser estatisticamente verificado em variados tipos de ensaios ou medidas repetidas usando-se os mesmos métodos e definições. Figure 8.5: Adaptação: Basic Epidemiology: R. Bonita, R. Beaglehole, T Kjelltröm, 2006 (p. 17) Confundimento A palavra “confundir” vem do latim confundere e significa misturar (fundir junto). O confundimento é outra importante questão em estudos epidemiológicos. Em um estudo da associação entre a exposição a uma causa (fator de risco) e a ocorrência de uma doença, o confundimento pode ocorrer quando existe outra exposição na população e está associada tanto à doença quanto ao fator de risco em estudo. O confundimento pode ter uma influência muito importante, podendo até alterar a direção aparente de uma associação. Uma variável que aparece como fator de proteção pode, após o controle de confundimento, ser considerada um fator de risco. Ou então o confundimento pode criar a aparência de uma relação causa-efeito que, na verdade, não existe. O confundimento ocorre quando os efeitos de duas exposições (fatores de risco) e a análise conclui que o efeito é devido a um fator e não a outro. O confundimento surge porque a distribuição não aleatória de fatores de risco na fonte também ocorre na população de estudo, fornecendo estimativas enganosas de efeito. Nesse sentido, pode parecer um viés, mas na verdade não resulta de um erro sistemático no projeto de pesquisa. Um exemplo de confundimento pode ser a explicação para a relação demonstrada entre beber café e o risco de doenças cardíaca coronariana, pois sabe-se que o consumo de café está associado com o uso de tabaco: as pessoas que bebem café são mais propensos a fumar do que as pessoas que não bebem café. Também é sabido que o tabagismo é uma causa de doença cardíaca coronariana. É, portanto, possível que a relação entre o consumo de café e doenças cardíacas doença meramente reflete a associação causal conhecida do uso de tabaco e doenças cardíacas. Nesta situação, fumar causa confundimento na aparente relação entre o consumo de café e doença cardíaca coronariana porque o tabagismo está correlacionado com beber café e é um fator de risco mesmo para quem não bebe café. Para se contornar esse tipo de problema deve-se, na etapa de delineamento do experimento, estabelecer os fatores envolvidos e, na realização da pesquisa observar a: casualização: as amostras devem ser de tal modo constituídas que variáveis e confundimento nelas existam, potencialmente, em igual proporção (como, por exemplo, fumantes e não fumantes; restrição: se estamos estudando a relação do café com doenças coronarianas, admitir apenas não fumantes. Vícios de seleção e de observação Vícios de seleção ocorrem quando os casos e controles são escolhidos de maneira que não representem corretamente a população. Vícios de observação ocorrem quando há erros na forma como a exposição ou os desfechos são medidos. "],["medidas-de-risco-morte-associação-e-correlação.html", "8.5 Medidas de risco, morte, associação e correlação", " 8.5 Medidas de risco, morte, associação e correlação Incidência (I); Prevalência (P); Incidência cumulativa (risco - IC); Fatalidade dos casos (FC); Taxa de mortalidade (TM); Diferença de risco (risco atribuível - RA); Razão de risco (risco relativo - RR); Risco atribuível proporcional (fração etiológica - FE); Odds ratio (razão de chances - OR); e, Correlação linear de Pearson. A morbidade é um dos importantes indicadores de saúde. É um termo genérico usado para designar o conjunto de casos de uma dada doença ou a soma de agravos à saúde que atingem um grupo de indivíduos.   Medir morbidade nem sempre é uma tarefa fácil, pois são muitas as limitações que contribuem para essa dificuldade, como a subnotificação.   Para fazer essas mensurações, utilizam-se principalmente as medidas de incidência e prevalência. 8.5.1 Incidência Incidência representa a proporção de número de novos casos de uma determinada doença em um intervalo de tempo em uma população exposta ao risco. É, por conseguinte, uma medida dinâmica pois pode sofrer alteração em razão do tempo no qual o estudo foi realizado.   Para um indivíduo pertencente à população exposta, indica a probabilidade de desenvolver a doença (risco).   Observe como calcular a incidência:   \\[ I=\\frac{\\text{Número de novos casos de uma doença durante um determinado período de tempo}}{\\text{Tamanho da população exposta ao risco nesse determinado período de tempo}} \\\\ \\text{apresentação usualmente na forma: }I(\\times 10^{n}) \\]   Exemplo: para se determinar a incidência de meningite no Maranhão no ano de 2014, será necessário saber o número de casos de meningite que ocorreram naquele período de tempo entre os residentes do Maranhão e o número de habitantes do estado no mesmo período de tempo (todos os possíveis expostos à doença):   \\[ I=\\frac{\\text{177 novos casos notificados de meningite no Maranhão em 2014}}{\\text{2.648.532 (população do Maranhão em 2014)}} (\\times 10^{5}) \\\\ I = \\frac{6,68}{100.000 } \\]   Os dados sobre prevalência e incidência tornam-se muito mais úteis se convertidos em taxas!   Como você pode notar, os casos novos, ou incidentes, são aqueles que não existiam no início do período de observação (tempo analisado), mas que vieram a ocorrer no decorrer desse período.   As taxas de incidência tendem a variar conforme o número de episódios da doença analisada, o número de pessoas que tiveram um episódio de uma doença, tempo para diagnosticá-la e a duração da investigação. 8.5.2 Prevalência Prevalência representa a proporção de indivíduos de uma população que é acometida por uma determinada doença (ou agravo) em um determinado momento. É considerada uma medida estática. Ela engloba tanto os casos casos preexistentes, quanto os novos que ocorreram no período. Indica a probabilidade de ter a doença. Observe como calcular a prevalência: \\[ P=\\frac{\\text{Número de casos existentes de doença em um determinado momento no tempo}}{\\text{Tamanho da população em risco nesse mesmo momento no tempo}}\\\\ \\text{apresentação usualmente na forma: }P(\\times 10^{n}) \\] Exemplo: se em uma determinada comunidade mensurou-se 89 casos de indivíduos portadores de hipertensão em um determinado momento. Sabendo-se que a população (todos estão potencialmente expostos) dessa comunidade é de 3.500 a prevalência será: \\[ P=\\frac{\\text{89 casos de hipertensão na comunidade no dia 01/01/2014}}{\\text{3.500 indivíduos como população em risco na comunidade em 01/01/2014}} (\\times 10^{2})\\\\ P= \\frac{2,54}{100 } \\] Os dados sobre prevalência e incidência tornam-se muito mais úteis se convertidos em taxas! 8.5.2.1 Relação entre prevalência e incidência A prevalência depende tanto da incidência quanto da duração da doença. Se os casos de incidentes não forem resolvidos e continuarem ao longo do tempo eles se tornarão casos prevalentes. Nesse sentido: \\[ P=\\text{Incidência} \\times \\text{Duração média da doença} \\] 8.5.2.2 Quadro comparativo entre medidas de incidência e de prevalência Quadro comparativo entre medidas de incidência e de prevalência Incidência Prevalência Numerador Número de novos casos de doença durante um determinado período de tempo Número de casos existentes de doença em um determinado momento no tempo Denominador Tamanho da população em risco Tamanho da população em risco Foco Se o evento é um caso novo Tempo de início da doença Presença ou ausência de uma doença O período de tempo é arbitrário Um “instantâneo” no tempo Uso Expressa o risco de adoecer A principal medida de doenças ou condições agudas, mas também usado para doenças crônicas Mais útil para estudos de causalidade Estima a probabilidade da população estar doente no período de tempo estudado Útil no estudo da carga de doenças crônicas e implicações para os serviços de saúde 8.5.3 Incidência cumulativa - IC (Risco)   Incidência Cumulativa (ou risco) é uma medida da ocorrência de uma doença.   Ao contrário da Incidência, no denominador temos agora o número de pessoas na população exposta sem a doença no começo do período do estudo:   \\[ IC=\\frac{\\text{Núm. de novos casos de uma doença durante um determ. período de tempo}}{\\text{Tam. da pop. em risco (exposta) livre (sem) da doença no começo de um determ. período de tempo}}\\\\ \\text{apresentação usualmente na forma: }IC(\\times 10^{n}) \\] 8.5.3.1 Quadro comparativo entre medidas de risco e prevalência   Quadro comparativo entre medidas de risco e prevalência Característica Risco Prevalência O que é medido Probabilidade da doença Percentagem da população com a doença Unidade adimensional adimensional Momento do diagnóstico da doença: Casos novos (recém diagnosticados) Existentes Sinônimos Incidência cumulativa - 8.5.4 Fatalidade dos Casos (FC)   Fatalidade dos casos é uma medida da severidade da doença, definida como a proporção de casos com desfecho em óbito pelo total de acometidos (portadores da condição) em um determinado período de tempo. \\[ FC(\\%)=\\frac{\\text{Número de mortes de casos diagnosticados da doença durante um determinado período de tempo}}{\\text{Número de casos diagnosticados nesse período de tempo}} \\\\ \\text{apresentação usualmente na forma: }FC(\\%)(\\times 100) \\] 8.5.5 Taxas de mortalidade (TM)   A principal desvantagem da Taxa bruta de mortalidade é que ela não leva em conta o fato de que a chance de morrer varia de acordo com idade, sexo, etnia e incontáveis outros fatores (sociais, econômicos, ).   Geralmente não é apropriado usá-la para comparar diferentes períodos de tempo ou áreas geográficas. Por exemplo, padrões de morte em núcleos urbanos recentemente constituídos e formados predominantemente por famílias jovens provavelmente serão muito diferentes das estâncias balneares escolhidas frequentemente por aposentados.   A Taxa bruta de mortalidade para todas as mortes ou uma causa específica de morte é calculado da seguinte forma:   \\[ TM(\\%)=\\frac{\\text{Número de mortes durante um determinado período de tempo}}{\\text{Número de pessoas sob risco de morte nesse período de tempo}}\\\\ \\text{apresentação usualmente na forma: }TM (\\times 10^{n}) \\] 8.5.6 Sobrevida Uma vez que a TM representa a proporção de pessoas afetadas por uma doença e que faleceram em decorrência dela, a sobrevida S pode ser considerada como seu complemento:   \\[ S=1-TM \\] 8.5.7 Taxas mais específicas taxa de mortalidade infantil; taxa de mortalidade maternal; taxa de mortalidade entre adultos; ou, taxas de mortalidade ajustadas por faixa etária.   Quantificar a ocorrência de doenças ou alterações nos estados de saúde é o primeiro passo de um estudo epidemiológico. "],["medidas-de-associação-em-estudos-de-coorte.html", "8.6 Medidas de associação em estudos de coorte", " 8.6 Medidas de associação em estudos de coorte Uma tabela é uma forma de representação retangular que permite mostrar clara e resumidamente os dados correspondentes a uma ou mais variáveis, visualizar o comportamento dos dados e facilitar o entendimento das informações. Uma tabela de dupla entrada permite extrair facilmente as proporções individuais, marginais e associadas relativas a duas variáveis (tabelas com mais variáveis são possíveis de serem construídas). Especificamente para estudos epidemiológicos, admita que as variáveis envolvidas se refiram a contagens relacionadas à ocorrência de uma doença em dois grupos de pessoas sob diferentes exposições. O grupo não exposto ao fator de risco é frequentemente usado como referência. o grupo de pessoas expostas a um determinado fator de risco; o grupo de pessoas não expostas. Casos classificados em relação ao desfecho a partir da exposição ao fator de risco Fator de risco Desfecho observado (doença) Total Presente Ausente Exposto (a) (b) (e) Não exposto (c) (d) (f) Total (a + c) (b + d) (e + f) Exemplo: Incidência de baixo peso ao nascer em recém-nascidos de Pelotas (RS) segundo o hábito tabágico da mãe durante a gravidez (1982) Incidência de baixo peso ao nascer em recém-nascidos de Pelotas, RS, segundo o hábito tabágico da mãe durante a gravidez (1982) Classificação da mãe Baixo peso ao nascer Total Sim Não Fumante 275 (a) 2.144 (b) 2.419 (e) Não fumante 311 (c) 4.496 (d) 4.807 (f) Total 586 6.640 7.226 8.6.1 Incidência observada de nascimentos com baixo peso entre mães expostas ao risco (não fumantes): \\(I_{e}\\) \\[ I_{e}=\\frac{(a)}{(e)} \\times 100 = \\frac{275}{2.419} \\times 100 = 11,37 \\% \\] Interpretação: 11,37% das crianças analisadas e que têm mães tabagistas nasceram com baixo peso. 8.6.2 Incidência observada de nascimentos com baixo peso entre mães não expostas ao risco (não fumantes): \\(I_{0}\\) \\[ I_{0}: \\frac{(c)}{(f)} \\times 100 = \\frac{311}{4.807} \\times 100 = 6,47 \\% \\] Interpretação: 6.47% das crianças analisadas e que têm mães não tabagistas nasceram com baixo peso. 8.6.3 Prevalência de nascimentos com baixo peso na população estudada \\[ \\frac{(a) + (c)}{(e) + (f)} \\times 100 = \\frac{586}{7.226} \\times 100 = 8,11\\% \\] Interpretação: 8,11% das crianças avaliadas nasceram com baixo peso. 8.6.4 Diferença de risco (Risco atribuível - RA) A diferença de risco (também chamada de excesso de risco ou risco atribuível) é a diferença nas taxas de ocorrência entre os grupos expostos e não expostos da população. Essa medida quantifica o excesso absoluto de risco associado a uma dada exposição. É uma medida útil do problema de saúde pública causado pela exposição ao fator de risco. Analisando-se as incidências na Tabela vemos que a diferença de risco de nascimento de bebês com baixo peso entre mães fumantes e não fumantes é: \\[\\begin{align*} RA &amp; =\\frac{(a)}{(e)} - \\frac{(c)}{(f)} \\\\ &amp; = \\frac{275}{2.419} - \\frac{311}{4.807} \\\\ &amp; = 0,11368334 - 0,064697316 \\\\ &amp; = 4,9 \\% \\end{align*}\\] Interpretação: 4,9% do risco de nascer com baixo peso pode ser atribuído ao fato de terem mães tabagistas. 8.6.5 Razão de risco (Risco relativo - RR) A razão de risco (também chamada de risco relativo) é o quociente entre as taxas de ocorrência entre os grupos expostos e não expostos da população. Pode ser interpretado como a probabilidade de um indivíduo exposto apresentar o desfecho relativa à de um indivíduo não exposto também apresentar. razão de risco maior que 1: fator de risco; razão de risco menor que 1: fator protetor. Analisando-se as incidências na Tabela vemos que a razão de risco de nascimento de bebês com baixo peso entre mães fumantes e não fumantes é de: \\[\\begin{align*} RR &amp; = \\frac{\\frac{(a)}{(e)}}{\\frac{(c)}{(f)}} \\\\ &amp; = \\frac{\\frac{275}{2.419}}{\\frac{311}{4.807}} \\\\ &amp; = \\frac{0,11368334}{0,064697316} \\\\ &amp; = 1,76 \\end{align*}\\] Interpretação: As crianças de mães tabagistas têm aproximadamente 1,76 vezes mais risco de nascer com baixo peso em comparação com as de mães não tabagistas. 8.6.6 Risco atribuível proporcional (Fração etiológica - FE)   Quando se acredita que uma determinada exposição é um fator de risco de uma determinada doença, a fração atribuível é a proporção da doença na população específica que seria eliminada se a exposição fosse evitada. As frações etiológicas (frações relacionadas à origem da doença) são úteis para avaliar as prioridades da ação de saúde pública.   O Risco atribuível proporcional (fração etiológica) é, assim, a proporção de todos os casos que podem ser atribuídos diretamente a uma exposição específica. Pode ser determinado pelo quociente da diferença de riscos das incidências pela incidência entre a população exposta.   Esta medida é útil para determinar a importância relativa das exposições para toda a população. É a proporção pela qual a taxa de incidência do desfecho em toda a população seria reduzido se a exposição fosse eliminada.   Observe como calcular o Risco atribuível proporcional (Fração etiológica - FE):   \\[ FE = \\frac{I_{e}-I_{o}}{I_{e}} \\times 100 \\] \\(I_{e}\\): é a incidência da doença no grupo exposto; \\(I_{o}\\): é a incidência da doença no grupo não exposto. Analisando-se as incidências na Tabela vemos que o risco atribuível proporcional de nascimento de bebês com baixo peso entre mães fumantes é de:   \\[\\begin{align*} FE = &amp; \\frac{\\left(\\frac{(a)}{(e)} - \\frac{(c)}{(f)}\\right)}{\\frac{(a)}{(e)}} \\\\ = &amp; \\frac{\\left(\\frac{275}{2.419} - \\frac{311}{4.807}\\right)}{\\frac{275}{2.419}} \\\\ = &amp; \\frac{\\left(0,11368334 - 0,064697316\\right)}{0,11368334} \\\\ = &amp; 43,09 \\% \\end{align*}\\]   Interpretação: 43,09% dos casos nascimentos de bebês com baixo peso podem ser atribuídos diretamente ao hábito tabagista das mães. "],["odds-ratio-razão-das-chances-em-studos-de-casos-e-controles.html", "8.7 Odds ratio (Razão das chances) em studos de casos e controles", " 8.7 Odds ratio (Razão das chances) em studos de casos e controles Em estudos de caso-controle os pacientes são incluídos de acordo com a presença ou não do desfecho. Geralmente são definidos um grupo de casos (com o desfecho) e outro de controles (sem o desfecho) e avalia-se uma eventual exposição, no passado a potenciais fatores de risco nestes dois grupos.   Devido ao fato de que o delineamento deste tipo de estudo baseia-se no próprio desfecho, não se pode estimar diretamente a incidência do desfecho de acordo com a presença ou ausência da exposição, como é usual em estudos de coorte.   Isto se deve ao fato de que a proporção casos/controles) (ou desfecho/não-desfecho) é determinada pelo próprio pesquisador (a proporção não é a mesma observada na população toda com possibilidade de exposição). Assim, a ocorrência de desfechos no grupo total estudado não é regida pela história natural da doença e depende de quantos casos e controles o pesquisador selecionou.   Apesar de não se poder estimar diretamente as incidências da doença (desfecho) entre expostos e não-expostos em estudos de caso-controle, é possível, entretanto, obter-se uma aproximação da Razão de risco (risco relativo - RR).   Se se o desfecho for suficientemente raro na população (10% ou menos), a Razão de risco (risco relativo - RR) pode ser estimada aproximadamente em estudos de caso-controle através da Razão de chances (odds ratio - OR) de exposição entre casos e controle: Casos e controles classificados em relação à exposição ao fator de risco Fator Grupos Casos (com o desfecho) Controles (sem o desfecho) Exposto (a) (b) Não exposto (c) (d) 8.7.1 Chance (odds) de observar um desfecho entre os casos: Grupo dos casos: a partir das proporções dos elementos desse grupo que foram ou não expostos ao fator (\\(P_{e},P_{0}\\)): \\[ P_{e}=\\frac{\\text{casos expostos}}{\\text{total de casos}}\\\\ P_{0}=\\frac{\\text{casos não expostos}}{\\text{total de casos}}\\\\ \\] a chance (odds) de se observar o desfecho entre os casos é a divisão dessas proporções: \\[ Odds_{casos}=\\frac{P_{e}}{P_{0}} \\\\ Odds_{casos}=\\frac{\\text{casos expostos}}{\\text{casos não expostos}} \\\\ Odds_{casos}= \\frac{a}{c} \\]   8.7.2 Chance (odds) de observar um desfecho entre os controles: Grupo dos controles: a partir das proporções dos elementos desse grupo que foram ou não expostos ao fator (\\(P_{e},P_{0}\\)): \\[ P_{e}=\\frac{\\text{controles expostos}}{\\text{total de controles}}\\\\ P_{0}=\\frac{\\text{controles não expostos}}{\\text{total de controless}}\\\\ \\] a chance (odds) de se observar o desfecho entre os controles é a divisão dessas proporções: \\[ Odds_{controles}=\\frac{P_{e}}{P_{0}} \\\\ Odds_{controles}=\\frac{\\text{controles expostos}}{\\text{controles não espostos}}\\\\ Odds_{controles}= \\frac{b}{d} \\] 8.7.3 A razão das chances entre os casos e controle (odds ratio): A razão das chances ( odds ratio - OR) de exposição entre casos e controles fica sendo \\[ OR = \\frac{Odds_{casos}}{Odds_{controles}} \\] OR ( odds ratio) maior que 1: fator de risco: a exposição ao fator aumenta a chance do desfecho OR ( odds ratio) menor que 1: fator protetor: a exposição ao fator reduza chance do desfecho. A razão de chances ( odds ratio) exprime numericamente quantas vezes a exposição a um determinado fator de risco implica na possibilidade do desfecho estudado. Exemplo: tanto o tabagismo quanto a poluição do ar são causas de câncer de pulmão, mas a fração devido ao fumo é geralmente muito maior do que a devido ao ar poluição. Apenas em comunidades com prevalência de tabagismo muito baixa e severos índices de poluição, esta seria a provável de ser a principal causa de câncer de pulmão. Assim, em muitos países, controle do tabagismo deve ter prioridade nos programas de prevenção do câncer de pulmão. Casos e controles classificados em relação à exposição ao fator de risco Fator Grupos Casos (com câncer) Controles (sem câncer) Exposto ao tabaco 200 (a) 50 (b) Não exposto ao tabaco 100 (c) 150 (d)   A chance (odds) de se observar o desfecho entre os casos : \\[ Odds_{casos}=\\frac{\\text{casos expostos}}{\\text{casos não expostos}} \\\\ Odds_{casos}= \\frac{a}{c}\\\\ Odss_{casos}= \\frac{200}{100}=2\\\\ \\] Interpretação: entre as pessoas com câncer (casos), a chance de terem sido espostos ao tabaco é 2 vezes maior do que a chance de não terem sido expostos. Ou seja, é muito provável que tenham sido expostas. A chance (odds) de se observar o desfecho entre os controles: \\[ Odds_{controles}=\\frac{\\text{controles expostos}}{\\text{controles não espostos}}\\\\ Odds_{controles}= \\frac{b}{d}\\\\ Odds_{controles}= \\frac{50}{150}=0,33 \\] Interpretação: entre as pessoas sem câncer (controles), a chance de terem sido expostos ao tabaco é 1/3 da chance de não terem sido expostas. Ou seja, é pouco provável que tenham sido expostas.   A razão das chances ( odds ratio - OR) de exposição entre casos e controle: \\[ OR = \\frac{Odds_{casos}}{Odds_{controles}} \\\\ OR = \\frac{2}{0,33}=6,06 \\]   Interpretação: uma odds ratio (OR) de aproximadamente 6,06 significa que a chance de uma pessoa exposta ao tabagismo desenvolver câncer de pulmão é cerca de 6 vezes maior do que a de uma pessoa não exposta, o que indica uma forte associação entre tabagismo e câncer de pulmão. "],["correlação-linear-de-pearson.html", "8.8 Correlação linear de Pearson", " 8.8 Correlação linear de Pearson Em estatística, a expressão correlação se refere à relação existente entre variáveis, digamos \\(X\\) e \\(Y\\). Essa relação pode assumir diferentes relações funcionais que, bsicamente podem ser: linear: positiva ou negativa não linear: logarítmica, cíclica (periódica), quadrática, cúbica . A correlação existente entre valores observados de uma mesma variável, digamos \\(X\\) em diferentes momentos de tempo \\(X_{(t_i-1)}, X_{(t_i)}\\) é denominada autocorrelação. É preciso sempre ter em mente que uma correlação estatística, por si só, não implica logicamente em causação. Para atribuir uma relação de causa-efeito deve-se lançar mão de considerações a priori ou teóricas acerca do objeto do estudo.   Figure 8.6: Diferentes diagramas de dispersão entre duas variáveis X e Y (Fonte: Introduction to Econometrics. Englewoods Cliffs, 1978)   Em (A), (B), (C) e (D) parece-nos que a relação observada entre as variáveis \\(X\\) e \\(Y\\) pode ser expressa por uma função linear (uma reta): em (A) e (C) vemos que a variação de ocorre no mesmo sentido: quando o valor da variável \\(X\\) sofre um incremento, também assim ocorre, em algum grau, na variável \\(Y\\); em (B) e (D) vemos que uma variação inversa: quando o valor da variável \\(X\\) sofre um incremento, a variável \\(Y\\) sofre um decremento em algum grau; em (A) e (B) parece-nos que uma função linear exprimiria uma relação entre as variáveis \\(X\\) e \\(Y\\) de modo exato quando comparada a (C) e (D).   Em (G) não se vislumbra um padrão linear no comportamento das variáveis \\(X\\) e \\(Y\\) e em (H) o padrão de comportamento observado entre as variáveis \\(X\\) e \\(Y\\) sugere haver uma boa relação, todavia não linear. O cálculo do Coeficiente de correlação linear de Pearson (r) envolve diversos somatórios dos valores das variáveis \\(X\\), \\(Y\\), seus quadrados e também de seu produto \\(X.Y\\).   \\[ r=\\frac{\\sum _{i=1}^{n}{x}_{i} \\cdot {y}_{i} - \\frac{\\sum _{i=1}^{n}{x}_{i}\\sum _{i=1}^{n}{y}_{i}}{n}}{\\sqrt{\\left(\\sum _{i=1}^{n}{x}_{i}^{2}-\\frac{{\\left(\\sum _{i=1}^{n}{x}_{i}\\right)}^{2}}{n}\\right)\\cdot \\left[\\sum_{i=1}^{n}{y}_{i}^{2}-\\frac{{\\left(\\sum _{i=1}^{n}{y}_{i}\\right)}^{2}}{n}\\right]}} \\] Na expressão acima: \\(x_{i}\\): é o i-ésimo valor observado de \\(X\\); \\(y_{i}\\): é o i-ésimo valor observado de \\(Y\\); e, \\(n\\) é o número de pares de valores observados. Outra apresentação de sua fórmula de estimação é: \\[ r=\\frac{\\sum _{i=1}^{n}(x_{i}-\\bar{x})\\cdot(y_{i}-\\bar{y})}{ \\sqrt{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\cdot \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}} } \\] Na expressão acima: \\(x_{i}\\): é o i-ésimo valor observado de \\(X\\); \\(y_{i}\\): é o i-ésimo valor observado de \\(Y\\); \\(\\bar{x}\\): é o valor médio das observações \\(x\\); e, \\(\\bar{y}\\): é o valor médio das obsrevações \\(y\\). Simplificadamente podemos também exprimir \\(r\\) na forma abaixo:   \\[ r=\\frac{{S}_{xy}}{\\sqrt{{S}_{xx}\\cdot {S}_{yy}}} \\] em que: \\[\\begin{align*} S_{xy} = &amp; \\sum_{i=1}^{n} x_{i}y_{i} - \\frac{\\sum_{i=1}^{n}x_{i}\\cdot\\sum_{i=1}^{n}y_{i}}{n} \\\\ S_{xx} = &amp; \\sum_{i=1}^{n} x_{i}^{2} - \\frac{(\\sum_{i=1}^{n} x_{i})^{2}}{n} \\\\ S_{yy} = &amp; \\sum_{i=1}^{n}y_{i}^{2} - \\frac{(\\sum_{i=1}^{n} y_{i})^{2}}{n} \\end{align*}\\] O coeficiente de correlação de Pearson quantifica a intensidade das relações lineares entre \\(x\\) e \\(y\\) e não estabelece per si nenhuma relação de causação. É apenas uma medida da associação linear entre duas variáveis e, portanto, não tem sentido usá-lo na quantificação de relações que não o sejam. O coeficiente de correlação linear de Pearson tem uma faixa limitada de variação e é simétrico; isto é, a correlação linear observada entre \\(X\\) e \\(Y\\) é a mesma que a medida entre \\(Y\\) e \\(X\\).   \\[ -1\\le r \\le 1 \\]   se \\(r&gt;0\\) dizemos que há uma relação linear positiva entre as variáveis estudadas: para um incremento na primeira variável observa-se também um incremento na segunda; se \\(r&lt;0\\) a relação linear é negativa: um incremento em uma das variáveis é acompanhado por um decremento na outra; e, quando \\(r=0\\) não há relação linear entre as variáveis consideradas.   Exemplo: onsidere as medidas obtidas de duas variáveis no quadro abaixo. Quadro de dados X Y 74 139 45 108 48 98 36 76 27 62 16 57 Quadro auxiliar para cálculo do coeficiente de correlação linear (r) X Y xi ⋅ yi xi2 yi2 74 139 10286 5476 19321 45 108 4860 2025 11664 48 98 4704 2304 9604 36 76 2736 1296 5776 27 62 1674 729 3844 16 57 912 256 3249 246 540 25172 12086 53458 Assim, sendo \\(n=6\\) obervações segue-se: \\[\\begin{align*} S_{xy} = &amp; \\sum_{i=1}^{n} x_{i}y_{i} - \\frac{\\sum_{i=1}^{n}x_{i}\\cdot\\sum_{i=1}^{n}y_{i}}{n} \\\\ = &amp; 25172 - \\frac{246 \\cdot 540}{6} \\\\ = &amp; 3032 \\\\ S_{xx} = &amp; \\sum_{i=1}^{n} x_{i}^{2} - \\frac{(\\sum_{i=1}^{n} x_{i})^{2}}{n} \\\\ = &amp; 12086 - \\frac{246^2}{6} \\\\ = &amp; 2000 \\\\ S_{yy} = &amp; \\sum_{i=1}^{n}y_{i}^{2} - \\frac{(\\sum_{i=1}^{n} y_{i})^{2}}{n} \\\\ = &amp; 53458 - \\frac{540^2}{6} \\\\ = &amp; 4858 \\end{align*}\\] Portanto: \\[\\begin{align*} r = &amp; \\frac{{s}_{xy}}{\\sqrt{{s}_{xx}\\cdot {s}_{yy}}} \\\\ = &amp; \\frac{3032}{\\sqrt{2000 \\cdot 4858}} \\\\ = &amp; 0,9727 \\end{align*}\\] "],["intervalos-de-confiança.html", "8.9 Intervalos de confiança", " 8.9 Intervalos de confiança As técnicas para obter intervalos de confiança para estimativas amostrais de riscos relativos e odds ratio que serão apresentadas estão descritas no livro Statistics with Confidence (Douglas Altman _et a_l) e, embora se constituam em aproximações para grandes amostras, são estimativas razoáveis para pequenos estudos.   Através de uma transformação logarítmica, obtém-se uma curva com forma aproximadamente Normal e assim esses intervalos podem ser delimitados a partir da função densidade de probabilidade da distribuição Normal padronizada.   Para o intervalo de confiança da estimativa amostral da diferença de risco (risco atribuível) a proposição se encontra no artigo Statistical algorithms in Review Manager 5 de Jonathan J. Deeks e Julian P. T. Higgins e está baseada na distribuição da diferença de proporções.   \\[ \\log(IC_{(medida)}) = \\log(medida) \\pm \\left[ z_{(1-\\frac{\\alpha}{2})} \\times EP(\\log(medida))\\right] \\]   em que:   \\(EP(\\log(medida))\\) é o erro padrão do logaritmo da medida e os valores mínimo e máximo do intervalo de confiança serão dados por \\(\\exp{[\\log((IC_{(medida)})]}\\); \\(\\alpha\\) é o nível de significânica tolerado e, por conseguinte, \\((1-\\alpha)\\) o nível de confiança pretendido; e, e os valores de \\(|z_{(1-\\frac{\\alpha}{2})}|\\) poderão ser obtidos em uma tabela da distribuição Normal padronizada, sendo os mais usuais:   Valores críticos zc correspondentes a vários níveis de significância (α) Níveis de significância (α) 0,10 0,05 0,01 0,005 0,002 Valores críticos de zc -1,28 -1,645 -2,33 -2,58 -2,88 para testes unilaterais ou 1,28 ou 1,645 ou 2,33 ou 2,58 ou 2,88 Valores críticos de zc -1,645 -1,96 -2,58 -2,81 -3,08 para testes bilaterais e 1,645 e 1,96 e 2,58 e 2,81 e 3,08 8.9.1 Razão de risco (Risco relativo - RR) Considere a estrutura dos dados presentes na Tabela para a estimação dos erros padrão a seguir.   \\[ EP(\\log(RR)) = \\sqrt{ \\left[ \\frac{1}{(a)} - \\frac{1}{(a) + (b)} \\right] + \\left[ \\frac{1}{(c)} - \\frac{1}{(c)+(d)} \\right]} \\]   O erro padrão do Risco Relativo - RR para os dados da Tabela poderá ser assim estimado:   \\[\\begin{align*} EP(\\log(RR)) = &amp; \\sqrt{ \\left[ \\frac{1}{(a)} - \\frac{1}{(a) + (b)} \\right] + \\left[ \\frac{1}{(c)} - \\frac{1}{(c)+(d)} \\right] }\\\\ EP(\\log(RR)) = &amp; \\sqrt{ \\left[ \\frac{1}{(275)} - \\frac{1}{2.419} \\right] + \\left[ \\frac{1}{311} - \\frac{1}{4.807} \\right] }\\\\ EP(\\log(RR)) = &amp; \\sqrt{0,006230374} \\\\ EP(\\log(RR)) = &amp; 0,078932718 \\end{align*}\\]   Para um nível de confiança de 95% (nível de significância de 0,05%) extraímos o valor crítico de \\(z_{(1-\\frac{\\alpha}{2})}\\) da Tabela (\\(z_{c}=|1,96|\\)).   A partir do Risco relativo previamente calculado (1,76), um intervalo com nível de confiança de (\\(1-\\alpha=95\\%\\)) fica assim delimitado:   \\[\\begin{align*} \\log(IC_{(RR)}) = &amp; \\log(RR) \\pm \\left[ z_{(1-\\frac{\\alpha}{2})} \\times EP(\\log(RR))\\right] \\\\ \\log(IC_{(RR)}) = &amp; \\log(1,76) \\pm \\left(1,96 \\times 0, 078932718 \\right) \\\\ \\log(IC_{(RR)}) = &amp; 0,565313809 \\pm 0,154708127 \\\\ \\text{Limite superior } IC_{(RR)} = &amp; \\exp{(0.7147081)} \\\\ = &amp; 2,04359 \\\\ \\text{Limite inferior } IC_{(RR)} = &amp; \\exp{(0.4052919)}\\\\ = &amp; 1,49974 \\end{align*}\\]   Assim, o intervalo com nível de confiança (\\(1-\\alpha\\)) estabelecido em 95% para a estimativa amostra do Risco relativo (RR) calculada em 1,76 é: \\[ IC_{RR (1-\\alpha=0,95)} = [1,49974 ; 2,04359] \\] 8.9.2 Razão de chances ( odds ratio - OR) Considere a estrutura dos dados presentes na Tabela para a estimação dos erros padrão a seguir.   \\[ EP(\\log(OR)) = \\sqrt{ \\frac{1}{(a)} + \\frac{1}{(b)} + \\frac{1}{(c)} +\\frac{1}{(d)} } \\]   O erro padrão da Razão das chances ( odds ratio - OR) para os dados da Tabela poderá ser assim estimado:   \\[\\begin{align*} EP(\\log(OR)) = &amp; \\sqrt{ \\frac{1}{(a)} + \\frac{1}{(b)} + \\frac{1}{(c)} +\\frac{1}{(d)} } \\\\ EP(\\log(OR)) = &amp; \\sqrt{ \\frac{1}{275} + \\frac{1}{2.144} + \\frac{1}{311} +\\frac{1}{4.496} }\\\\ EP(\\log(OR)) = &amp; \\sqrt{ 0,007540636}\\\\ EP(\\log(OR)) = &amp; 0,08683683 \\end{align*}\\]   Para um nível de confiança de 95% (nível de significância de 0,05%) extraímos o valor de \\(z_{(1-\\frac{\\alpha}{2})}\\) da Tabela (\\(z_{c}=|1,96|\\)).   A partir da Razão das chances previamente calculada (1,85), um intervalo com nível de confiança de (\\(1-\\alpha=95\\%\\)) fica assim delimitado:   \\[\\begin{align*} \\log(IC_{(OR)}) = &amp; \\log(OR) \\pm \\left[ z_{(1-\\frac{\\alpha}{2})} \\times EP(\\log(OR))\\right] \\\\ \\log(IC_{(OR)}) = &amp; \\log(1,85) \\pm \\left(1,96 \\times 0,08683683 \\right) \\\\ \\log(IC_{(OR)}) = &amp; 0,6151856 \\pm 0,1702002 \\\\ \\text{Limite superior } IC_{(OR)} = &amp; \\exp{( 0.7853858)}\\\\ = &amp; 2,193253 \\\\ \\text{Limite inferior } IC_{(OR)} = &amp; \\exp{(0.4449854)} \\\\ = &amp; 1,560467 \\\\ \\end{align*}\\]   Assim, o intervalo com nível de confiança (\\(1-\\alpha\\)) estabelecido em 95% para a estimativa amostra da Razão de chances (OR) calculada em 1,85 é: \\[ IC_{OR (1-\\alpha=0,95)} = [1, 560467 ; 2, 193253] \\] 8.9.3 Diferença de risco (Risco atribuível - RA) Considere a estrutura dos dados presentes na Tabela para a estimação dos erros padrão a seguir. \\[ EP(RA) = \\sqrt{ \\left [ \\frac{a \\times b}{(a+b)^3} \\right ] + \\left [ \\frac{c \\times d}{(c+d)^3} \\right ] } \\] \\[ IC_{(RA)} = RA \\pm \\left[ z_{(1-\\frac{\\alpha}{2})} \\times EP(RA))\\right] \\] O erro padrão da Diferença de Risco - RA para os dados da Tabela poderá ser assim estimado: \\[\\begin{align*} EP(RA) = &amp; \\sqrt{ \\left [ \\frac{a \\times b}{(a+b)^3} \\right ] + \\left [ \\frac{c \\times d}{(c+d)^3} \\right ] }\\\\ EP(RA) = &amp; \\sqrt{ \\left [ \\frac{275 \\times 2144}{(275+2.144)^3} \\right ] + \\left [ \\frac{311 \\times 4.496}{(311+4.496)^3} \\right ] }\\\\ EP(RA) = &amp; 0,007364887 \\end{align*}\\] Para um nível de confiança de 95% (nível de significância de 0,05%) extraímos o valor de \\(z_{(1-\\frac{\\alpha}{2})}\\) da Tabela (\\(z_{c}=|1,96|\\)). A partir da Diferença de risco previamente calculada (0,049), um intervalo com nível de confiança de (\\(1-\\alpha=95\\%\\)) fica assim delimitado: \\[\\begin{align*} IC_{(RA)} = &amp; RA \\pm \\left[ z_{(1-\\frac{\\alpha}{2})} \\times EP(RA))\\right] \\\\ IC_{(RA)} = &amp; 0,049 \\pm \\left[ 1,96 \\times 0,007364887 \\right] \\\\ \\text{Limite superior} = &amp; 0,06343518 \\\\ \\text{Limite inferior} = &amp; 0,03456482 \\end{align*}\\] Assim, o intervalo com nível de confiança (\\(1-\\alpha\\)) estabelecido em 95% para a estimativa amostras da Diferença de risco (RA) calculada em 4,9% é: \\[ IC_{RA (1-\\alpha=0,95)} = [3,46\\% ; 6,34\\%] \\] "],["introdução-à-distribuição-das-médias-e-diferenças-entre-médias-amostrais-e-seus-intervalos-de-confiança.html", "Capítulo 9 Introdução à distribuição das médias e diferenças entre médias amostrais e seus intervalos de confiança", " Capítulo 9 Introdução à distribuição das médias e diferenças entre médias amostrais e seus intervalos de confiança A finalidade de uma amostra é obter uma estimativa do valor de um ou mais parãmetros de uma população. Observa-se que os valores amostrais repetidamente extraídos de modo aleatório de uma mesma população variam de uma para outra amostra e também em relação ao verdadeiro parâmetro dessa população; todavia, demonstra-se que essa variabilidade pode ser descrita por meio de distribuições de probabilidade. Distribuições de probabilidade quando usadas para esse propósito são denominadas de distribuições amostrais e permitem responder para cada amostra o quão próxima está a estatística amostral do verdadeiro parâmetro populacional. Essa resposta depende fundamentalmente de três fatores: a estatistica que está sendo utilizada: diferentes estatísticas requerem diferentes distribuições de probabilidade para modelar sua variabilidade; o tamanho da amostra que implica de modo inverso na variabilidade entre as amostras; a variabilidade existente na própria população sob estudo e amostragem. "],["distribuições-amostrais.html", "9.1 Distribuições amostrais", " 9.1 Distribuições amostrais Parâmetro é toda medida numérica descritiva de uma população. Quando essas medidas são calculadas sobre amostras extraídas de uma população passam a ser denominadas como estatísticas da população de origem. A média, a mediana, a variância, a proporção amostrais, assim como outras estatísticas amostrais, são exemplos de variáveis aleatórias (v.a.) uma vez que seus valores sofrem variação a cada amostra extraída. Considere uma população com \\(N\\) elementos da qual se deseja extrair todas as possíveis amostras de tamanho \\(n\\). Para cada amostra extraída pode-se calcular uma mesma medida descritiva como, por exemplo, a média ( ou a variância, proporção ). O conjunto dos valores resultantes nos permite analisar como as estimativas amostrais se distribuem em comparação ao parâmetro que estão a estimar. Essas distribuições são denominadas distribuições amostrais. O estudo das distribuições amostrais é um elemento fundamental na inferência estatística posto possibilitar o estabelecimento de intervalos de confiança relacionados ao valor de um parâmetro que se deseja inferir, a partir de uma estatística proveniente de uma única amostra. O processo de extração de amostras pode ser com ou sem reposição. A extração com reposição assegura a independência entre os eventos e, eventos independentes são mais facilmente analisados. O quantidade possível de amostras de tamanho \\(n\\) extraídas de uma população de tamanho \\(N\\) é dado por : com reposição: \\(N^{n}\\); e, sem reposição: \\(C_{(N.n)}\\) Mais adiante veremos que processos de extração de amostras de tamanho \\(n\\), sem reposição de populações finitas com parâmetros \\(\\mu\\) (média) e \\(\\sigma^{2}\\) (variância) a esperança da v.a. de sua média amostral ainda é dada por: \\[ E(\\stackrel{-}{X})=\\mu \\] mas sua variância deve ser corrigida de: \\[ Var(\\stackrel{-}{X}) =\\frac{\\sigma^{2}}{n} \\] para: \\[ Var(\\stackrel{-}{X}) =\\frac{\\sigma^{2}}{n} \\cdot (\\frac{N-n}{N-1}) \\] em que \\((\\frac{N-n}{N-1})\\) é denominado como fator de correção para populações finitas. Para ilustrar o conceito de distribuição das médias amostrais considere uma situação onde uma empresa produz lâmpadas e a vida útil média, em horas, dessas lâmpadas segue uma distribuição Normal tal que \\(VU \\sim N (1600, 120)\\). Usando conceitos já explicados em uma unidade anterior podemos determinar o tamanho amostral em função de: um erro máximo: \\(\\varepsilon\\)=20 horas; um nível de significância estabelecido: \\(\\alpha\\)=0,05; e, e alguma informação sobre a medida da variabilidade da variável em estudo: \\(\\sigma\\)=120 horas (no caso, o desvio padrão populacional). Figure 9.1: Flutuação dos valores médios para diversas amostras extraídas de uma mesma população distribuição \\(\\sim N (\\mu; \\sigma)\\) ## mu media erro li ls ## 1 1600 1585 -14.9813 1565 1605 ## 2 1600 1584 -16.2000 1566 1602 ## 3 1600 1599 -1.0244 1581 1617 ## 4 1600 1622 22.0953 1601 1643 ## 5 1600 1589 -11.0680 1568 1610 ## 6 1600 1588 -11.9739 1569 1607 ## 7 1600 1576 -23.9815 1557 1595 ## 8 1600 1601 0.9897 1582 1620 ## 9 1600 1598 -1.9055 1580 1616 ## 10 1600 1615 15.0196 1594 1636 ## 11 1600 1599 -0.5864 1580 1619 ## 12 1600 1577 -22.6610 1559 1596 ## 13 1600 1601 0.6607 1579 1622 ## 14 1600 1594 -5.6916 1575 1614 ## 15 1600 1610 9.8407 1588 1631 ## 16 1600 1605 4.6067 1584 1625 ## 17 1600 1608 8.3771 1589 1628 ## 18 1600 1609 8.8751 1588 1630 ## 19 1600 1594 -6.0253 1575 1613 ## 20 1600 1607 7.4457 1587 1628 ## 21 1600 1624 23.8043 1603 1645 ## 22 1600 1614 14.0757 1595 1633 ## 23 1600 1605 5.4210 1584 1626 ## 24 1600 1605 4.9823 1586 1624 ## 25 1600 1609 9.3898 1589 1630 ## 26 1600 1594 -5.5167 1573 1615 ## 27 1600 1582 -17.7661 1563 1601 ## 28 1600 1582 -17.7996 1563 1601 ## 29 1600 1578 -21.5994 1558 1599 ## 30 1600 1598 -1.7933 1578 1618 ## 31 1600 1609 8.6857 1589 1628 ## 32 1600 1597 -3.2472 1578 1616 ## 33 1600 1608 7.6295 1588 1627 ## 34 1600 1589 -10.5025 1571 1608 ## 35 1600 1594 -5.5814 1576 1612 ## 36 1600 1603 3.3850 1583 1624 ## 37 1600 1606 6.3310 1587 1626 ## 38 1600 1601 1.0197 1581 1621 ## 39 1600 1592 -8.0783 1571 1613 ## 40 1600 1605 4.7390 1585 1625 ## 41 1600 1606 5.8224 1586 1625 ## 42 1600 1592 -8.4659 1571 1612 ## 43 1600 1619 19.2015 1599 1639 ## 44 1600 1600 0.4241 1580 1621 ## 45 1600 1606 5.9194 1586 1625 ## 46 1600 1605 5.2821 1587 1623 ## 47 1600 1582 -17.7433 1563 1601 ## 48 1600 1601 0.7213 1581 1621 ## 49 1600 1590 -9.8821 1572 1608 ## 50 1600 1588 -12.3993 1566 1609 ## 51 1600 1604 3.6936 1585 1623 ## 52 1600 1614 13.9442 1594 1634 ## 53 1600 1606 5.9998 1587 1625 ## 54 1600 1610 10.0105 1591 1629 ## 55 1600 1603 3.2759 1586 1620 ## 56 1600 1580 -20.3020 1559 1601 ## 57 1600 1592 -8.2945 1572 1612 ## 58 1600 1607 6.9028 1585 1629 ## 59 1600 1591 -8.7883 1573 1609 ## 60 1600 1593 -6.9865 1573 1613 ## 61 1600 1595 -5.0127 1575 1615 ## 62 1600 1579 -21.3720 1560 1597 ## 63 1600 1608 7.8765 1588 1628 ## 64 1600 1593 -7.1947 1572 1614 ## 65 1600 1601 1.4671 1582 1620 ## 66 1600 1601 1.3069 1583 1620 ## 67 1600 1594 -5.8781 1575 1613 ## 68 1600 1601 1.1923 1582 1621 ## 69 1600 1594 -6.4466 1574 1614 ## 70 1600 1616 16.1669 1597 1635 ## 71 1600 1609 9.4674 1589 1630 ## 72 1600 1597 -2.5783 1579 1616 ## 73 1600 1591 -8.7745 1570 1613 ## 74 1600 1595 -5.2115 1574 1615 ## 75 1600 1595 -4.5656 1576 1615 ## 76 1600 1581 -19.3446 1564 1598 ## 77 1600 1593 -7.3117 1573 1613 ## 78 1600 1593 -6.7009 1572 1615 ## 79 1600 1601 0.8384 1581 1621 ## 80 1600 1604 4.0373 1584 1624 ## 81 1600 1588 -12.0571 1567 1609 ## 82 1600 1602 2.3596 1580 1625 ## 83 1600 1589 -11.4028 1569 1608 ## 84 1600 1601 0.6315 1581 1620 ## 85 1600 1611 11.3110 1590 1633 ## 86 1600 1584 -15.5906 1563 1605 ## 87 1600 1603 2.6825 1583 1622 ## 88 1600 1601 1.0167 1581 1621 ## 89 1600 1616 15.5013 1596 1635 ## 90 1600 1608 7.7793 1589 1627 ## 91 1600 1580 -19.5227 1562 1599 ## 92 1600 1594 -5.6247 1575 1614 ## 93 1600 1598 -1.6468 1580 1617 ## 94 1600 1605 5.1298 1585 1625 ## 95 1600 1601 0.5400 1581 1620 ## 96 1600 1587 -13.4212 1567 1606 ## 97 1600 1611 11.0280 1589 1633 ## 98 1600 1605 5.2357 1584 1626 ## 99 1600 1609 9.1832 1589 1629 ## 100 1600 1596 -3.8433 1574 1618 Observa-se no gráfico acima que algumas das amostras (em vermelho), numa proporção igual ao nível de significância estabelecido quando do dimensionamento (5%), geram médias (amostrais) se afastam do valor médio na população mais que o erro estabelecido (20 h). "],["intervalos-de-confiança-1.html", "9.2 Intervalos de confiança", " 9.2 Intervalos de confiança Um intervalo de confiança (\\(IC\\)) pode ser entendido com a faixa de valores delimitada por um mínimo e um máximo, calculados como função direta de um nível de confiança e da variabilidade e inversa da tamanho amostral. \\[ \\text{estimativa amostral} \\pm confiança.\\sqrt\\frac{variabilidade}{n} \\] Raramente se dispõe de informação a respeito da variabilidade (\\(\\sigma^{2}\\)) da população estudada. Assim, a variabilidade populacional será frequentemente incorporado na expressão acima, com ligeiras modificações, na forma de sua estimativa amostral (\\(S^{2}\\)). De certo modo, um intervalo de confiança reflete uma estimativa objetiva da (im)precisão e do tamanho da amostra de determinada pesquisa e, assim, podemos considerá-lo como uma medida da qualidade da amostra e da pesquisa. O nível de confiança é designado pela quantidade \\((1-\\alpha)\\) na qual \\(\\alpha\\) é denominado de nível de significância, uma medida da probabilidade de erro. Dependendo do nível de confiança que escolhemos os limites superior e inferior do intervalo mudam para uma mesma estimativa amostral. Os intervalos de confiança mais utilizados na literatura são os de 90%, 95%, 99% e menos de 99,9%. O intervalo de confiança de 95% é tradicionalmente o intervalo mais utilizado na literatura e isso está relacionado ao nível de significância estatística (\\(P&lt;0,05\\)) geralmente mais aceito. Quanto menor for a amplitude de um intervalo, maior será a precisão da estimativa. Todavia, somente estudos com amostras razoavelmente grandes resultarão em um intervalo de confiança estreito, indicando simultaneamentente com alta precisão e alto grau de confianla a estimativa do parâmetro. Intervalos de confiança podem ser construídos a quase todas as quantidades estatísticas e suas diferenças (quando se procura estudar se há ou não diferenças entre os parâmetros de duas populaçoes) como, por exemplo: médias; proporções; e, variâncias. Um intervalo de confiança estabelecido sob certa probabilidade não deve ser interpretado como sendo a faixa de valores, delimitada por um mínimo e máximo, entre os quais o parâmetro da população (o qual se estima ou sobre o qual se infere) se insere. Mas sim que, extraíndo-se um grande número de amostras de igual tamanho e da mesma população, e construindo-se para cada uma dessas amostras um intervalo de confiança de um mesmo nível de significância (\\(\\alpha\\)), observaremos que uma determinada proporção desses intervalos, chamada de nível de confiança (\\(1-\\alpha\\)) irá, de fato, conter o parâmetro sobre o qual se estima ou sobre o qual se infere. Por conseguinte, uma proporção desses intervalos chamada de nível de significância (\\(\\alpha\\)) não irá conter o verdadeiro valor do parâmetro populacional. Assim, \\((1-\\alpha)\\) traduz o grau de confiança que se tem que um intervalo de confiança, calculado sobre uma estatística advinda de uma particular amostra de tamanho \\(n\\) da variável aleatória \\(X\\), inclua o verdadeiro valor do parâmetro da população: IC.N = function (N, n, mu, sigma, conf) { dados=data.frame() plot(0, 0, type=&quot;n&quot;, xlim=c(mu-0.4*mu,mu+0.4*mu), ylim=c(0,N), bty=&quot;l&quot;, xlab=&quot;Escala de valores da variável&quot;, ylab=&quot;Intervalos amostrais construídos&quot;, main=paste0(&quot;Intervalos com iguais níveis de confiança fixados em &quot;, 100*conf, &quot;% \\n(&quot;,N,&quot; amostras de tamanho &quot;,n,&quot;)&quot;) , sub=paste0(&quot;Parâmetros da distribuição da população Normal ( \\u03bc, \\u03c3) = (&quot;,mu,&quot;, &quot;, sigma,&quot;)&quot;)) abline(v=mu, col=&#39;red&#39;, lwd=2, lty=2) #axis(1, at = c(mu-1*mu, mu, mu+1*mu)) zc = qnorm(1-((1-conf)/2)) #sigma.xbarra = sigma/sqrt(n) for (i in 1:N) { x = rnorm(n, mu, sigma) media = mean(x) erro= media-mu sd = sd(x) li = media - zc * sd/(sqrt(n)) ls = media + zc * sd/(sqrt(n)) temp=cbind(mu, media, erro, li, ls) dados=rbind(dados, temp) plotx = c(li,ls) ploty = c(i,i) if (li &gt; mu | ls &lt; mu) lines(plotx,ploty, col=&quot;red&quot;, lwd=2, lend=0) else lines(plotx,ploty, lend=0) if (li &gt; mu | ls &lt; mu) points(media, i, col=&quot;red&quot;, cex=1)+text(y=i+3,x=media, labels=round(media,1), cex=1, col=&#39;red&#39;) else points(media, i, col=&quot;black&quot;, cex=1) } colnames(dados)=c(&quot;mu&quot;, &quot;media&quot;, &quot;erro&quot;, &quot;li&quot;, &quot;ls&quot;) return(dados) } N=100 n=64 mu=9.421 sigma=4.1681 conf=0.95 IC.N(N, n, mu, sigma, conf) ## mu media erro li ls ## 1 9.421 9.244 -0.177319 8.152 10.335 ## 2 9.421 9.880 0.458545 8.866 10.893 ## 3 9.421 8.351 -1.070003 7.246 9.456 ## 4 9.421 9.018 -0.403447 7.904 10.131 ## 5 9.421 10.079 0.657647 9.216 10.942 ## 6 9.421 9.204 -0.217367 8.167 10.240 ## 7 9.421 8.648 -0.772758 7.659 9.638 ## 8 9.421 9.324 -0.097164 8.244 10.404 ## 9 9.421 9.024 -0.397249 7.866 10.181 ## 10 9.421 8.594 -0.826982 7.610 9.579 ## 11 9.421 9.493 0.071698 8.438 10.547 ## 12 9.421 9.703 0.281677 8.622 10.783 ## 13 9.421 8.886 -0.535061 7.930 9.842 ## 14 9.421 8.621 -0.800246 7.584 9.657 ## 15 9.421 8.253 -1.167502 7.206 9.301 ## 16 9.421 9.615 0.193735 8.518 10.712 ## 17 9.421 9.402 -0.018746 8.376 10.428 ## 18 9.421 9.514 0.092828 8.528 10.500 ## 19 9.421 9.495 0.073559 8.709 10.280 ## 20 9.421 9.814 0.392683 8.983 10.645 ## 21 9.421 9.577 0.155885 8.439 10.715 ## 22 9.421 9.290 -0.131205 8.301 10.278 ## 23 9.421 9.010 -0.411102 7.988 10.032 ## 24 9.421 9.898 0.476941 8.904 10.891 ## 25 9.421 9.264 -0.157300 8.368 10.159 ## 26 9.421 9.284 -0.137361 8.353 10.214 ## 27 9.421 8.895 -0.525816 7.897 9.893 ## 28 9.421 8.879 -0.541691 7.787 9.972 ## 29 9.421 8.910 -0.510990 7.978 9.842 ## 30 9.421 9.161 -0.260295 8.100 10.221 ## 31 9.421 9.386 -0.034720 8.466 10.307 ## 32 9.421 9.671 0.249552 8.644 10.697 ## 33 9.421 9.104 -0.316989 8.033 10.175 ## 34 9.421 9.490 0.068919 8.569 10.411 ## 35 9.421 9.155 -0.265513 8.117 10.194 ## 36 9.421 10.136 0.715068 9.110 11.162 ## 37 9.421 8.543 -0.878348 7.476 9.609 ## 38 9.421 9.302 -0.119131 8.294 10.309 ## 39 9.421 9.059 -0.362281 8.022 10.096 ## 40 9.421 9.687 0.266056 8.735 10.639 ## 41 9.421 9.519 0.097880 8.385 10.653 ## 42 9.421 9.208 -0.212942 8.029 10.387 ## 43 9.421 9.616 0.195313 8.724 10.508 ## 44 9.421 8.764 -0.657142 7.602 9.926 ## 45 9.421 8.607 -0.814475 7.429 9.784 ## 46 9.421 9.341 -0.080004 8.350 10.332 ## 47 9.421 9.567 0.145856 8.437 10.696 ## 48 9.421 9.165 -0.256413 8.266 10.064 ## 49 9.421 9.916 0.494968 8.735 11.097 ## 50 9.421 10.722 1.300814 9.711 11.733 ## 51 9.421 9.137 -0.284429 8.155 10.118 ## 52 9.421 9.660 0.239429 8.572 10.748 ## 53 9.421 9.072 -0.348630 8.028 10.117 ## 54 9.421 7.944 -1.476566 6.798 9.090 ## 55 9.421 8.936 -0.485077 7.968 9.904 ## 56 9.421 8.833 -0.588111 7.754 9.912 ## 57 9.421 9.047 -0.373783 8.083 10.011 ## 58 9.421 9.865 0.444031 8.897 10.833 ## 59 9.421 9.637 0.215867 8.740 10.534 ## 60 9.421 8.798 -0.622623 7.816 9.781 ## 61 9.421 9.890 0.469257 8.840 10.940 ## 62 9.421 8.715 -0.705511 7.575 9.856 ## 63 9.421 9.536 0.114832 8.523 10.548 ## 64 9.421 8.851 -0.569827 7.919 9.783 ## 65 9.421 9.170 -0.251068 8.123 10.217 ## 66 9.421 9.964 0.543092 9.006 10.923 ## 67 9.421 10.441 1.019803 9.368 11.514 ## 68 9.421 8.643 -0.778048 7.427 9.859 ## 69 9.421 8.779 -0.641807 7.777 9.781 ## 70 9.421 9.415 -0.005898 8.446 10.384 ## 71 9.421 10.180 0.759440 9.165 11.196 ## 72 9.421 9.212 -0.209363 8.323 10.100 ## 73 9.421 9.854 0.432965 8.862 10.846 ## 74 9.421 9.560 0.139245 8.573 10.547 ## 75 9.421 8.352 -1.068511 7.233 9.472 ## 76 9.421 9.837 0.415707 8.887 10.786 ## 77 9.421 10.263 0.842142 9.219 11.307 ## 78 9.421 8.758 -0.662571 7.766 9.750 ## 79 9.421 9.537 0.115980 8.409 10.665 ## 80 9.421 10.208 0.787225 9.222 11.195 ## 81 9.421 9.148 -0.273203 8.150 10.145 ## 82 9.421 9.764 0.343266 8.743 10.785 ## 83 9.421 9.904 0.483303 8.878 10.930 ## 84 9.421 9.198 -0.223319 8.138 10.258 ## 85 9.421 9.480 0.059024 8.413 10.547 ## 86 9.421 10.601 1.179622 9.449 11.753 ## 87 9.421 9.655 0.233956 8.826 10.483 ## 88 9.421 9.584 0.163163 8.497 10.671 ## 89 9.421 8.970 -0.450762 8.123 9.818 ## 90 9.421 9.086 -0.335465 8.115 10.056 ## 91 9.421 9.723 0.301922 8.590 10.856 ## 92 9.421 9.361 -0.059650 8.394 10.328 ## 93 9.421 8.924 -0.496761 7.878 9.971 ## 94 9.421 8.598 -0.822915 7.613 9.584 ## 95 9.421 9.150 -0.270833 8.102 10.198 ## 96 9.421 8.343 -1.078388 7.404 9.281 ## 97 9.421 9.358 -0.063233 8.482 10.233 ## 98 9.421 9.768 0.347276 8.720 10.816 ## 99 9.421 9.130 -0.291183 8.154 10.106 ## 100 9.421 9.550 0.128584 8.638 10.461 O gráfico acima expõe os intervalos de confiança: \\((1-\\alpha)\\)=95% produzidos para as 100 médias de amostras de tamanho 64 extraídas de uma população com parâmetros \\(\\mu:\\) 9.421 e \\(\\sigma:\\) 4.1681. A proporção de intervalos amostrais que não contém o verdadeiro valor do parâmetro populacional pode ser visualmente inspecionada pelas linhas em vermelho. Intervalos de confiança bilaterais: intervalos delimitados por dois valores: mínimo e máximo, para a proporção amostral, dentro do qual todos os valores possuem um mesmo nível de confiança de ocorrência. Intervalos de confiança unilaterais: intervalos delimitados apenas em um de seus lados, nos quais todos os valores possuem um mesmo nível de confiança. Podem ser limitados à direita por um valor máximo ou limitados à esquerda por um valor mínimo. "],["distribuição-das-médias-amostrais-e-seus-intervalos-de-confiança.html", "9.3 Distribuição das médias amostrais e seus intervalos de confiança", " 9.3 Distribuição das médias amostrais e seus intervalos de confiança Figure 9.2: Ilustração esquemática de \\(n\\) amostras extraídas de uma mesma população de parâmetros \\(\\mu\\) e \\(\\sigma\\), cada uma apresentando as respectivas estatísticas calculadas Para estudarmos a distribuição das médias amostrais considerem uma população com parâmetros \\(\\mu\\) (média) e \\(\\sigma^{2}\\) (variância). A distribuição das médias amostrais expressa como se distribuem os valores dessa estatística calculada para todas as possíveis amostras de tamanho n extraídas de uma população cujo valor desse parãmetro é desconhecido.   A convergência da forma de distribuição e dos parâmetros da distribuição das médias amostrais são elucidadas pelas Leis (fraca e forte) dos Grandes Números e pelo Teorema Central do Limite (George Pólya, 1920). De acordo com a teoria, pelo uso de simulações computacionais consegue-se ilustrar que para uma amostra de tamanho n (onde \\(x_{1},x_{1},...,x_{n}\\) são os valores assumidos das variáveis aleatórias \\(X_{1},X_{1},...,X_{n}\\)) em amostras extraídas de uma população infinita de tamanho N com média \\(\\mu\\) e variância \\(\\sigma^{2}\\)) a distribuição das médias amostrais (v.a. \\(\\stackrel{-}{X}\\)) segue uma distribuição com os média \\(=\\mu\\) e variância \\(=\\frac{\\sigma^{2}}{n}\\) pois: \\[\\begin{align*} E(\\stackrel{-}{X}) &amp; = \\frac{1}{n} \\cdot \\{E(X_{1})+E(X_{2})+...+E(X_{n})\\} \\\\ &amp; = (\\frac{1}{n})\\cdot\\{\\mu+\\mu+...+\\mu\\} = \\frac{n\\cdot\\mu}{n} = \\mu \\end{align*}\\] \\[\\begin{align*} Var(\\stackrel{-}{X}) &amp; = \\frac{1}{n^{2}} \\cdot \\{Var(X_{1})+Var(X_{2}+...+Var(X_{n})\\} \\\\ &amp; = (\\frac{1}{n^{2}}) \\cdot \\{\\sigma^{2}+\\sigma^{2}+...+\\sigma^{2}\\} = n \\cdot \\frac{\\sigma^{2}}{n^{2}} = \\frac{\\sigma^{2}}{n} \\end{align*}\\] Equivale afirmar que, independentemente da forma de distribuição da população de origem da qual são extraídas as amostras, a distribuição dos valores da variável aleatória \\(\\stackrel{-}{X}\\) tenderá a seguir uma distribuição \\(\\sim N(\\mu;\\frac{\\sigma^{2}}{n}\\)) à medida que n , o tamanho da amostra aumenta, como ilustrado nas Figuras 9.3 e 9.5. O TCL garante a aproximação da distribuição de \\(\\stackrel{-}{X}\\) a uma distribuição Normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^{2}}{n}\\) quando \\(n\\) é grande, independentemente da distribuição da população de origem. Na prática, essa aproximação é usada quando \\(n\\ge 30\\). Portanto, para populações infinitas ou amostragem com reposição: \\[ \\stackrel{-}{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n}) \\] Demostração usando amostras extraídas de uma população com distribuição \\(\\sim U (v_{min}; v_{max})\\) # Definindo os parãmetros e a amostra min_1=2 max_1=6 NN=5000 pop_1=runif(NN, min=min_1, max=max_1) df=as.data.frame(pop_1) # A distribuição da população ilustrada em um histograma ggplot(df, aes(x=pop_1)) + geom_histogram( binwidth=1,color=&quot;black&quot;, fill=&quot;lightblue&quot;)+ scale_y_continuous(name=&quot;Frequência&quot;) + scale_x_continuous(name=&quot;Valores&quot;)+ labs(title= paste(&quot;Histograma de uma população com Distribuição Uniforme&quot;), subtitle = paste(&quot;Parâmetros: valor min =&quot;,min_1,&quot;; valor max =&quot;, max_1))+ theme(plot.title = element_text(size = 10, face = &quot;bold&quot;), axis.text.x = element_text(angle=0, hjust=1, size=10), axis.text.y = element_text(angle=0, hjust=1, size=10), axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10)) Figure 9.3: Histograma de uma população cuja característica de interesse segue uma Distribuição Uniforme A Figura 9.3 mostra o histograma de uma amostra de 5000 elementos de uma população com Distribuição Uniforme de parâmetros \\(v_{min}:\\) 2 e \\(v_{max}:\\) 6. Figure 9.4: Intervalos de confiança construídos para diversas estimativas amostrais de uma população com Distribuição \\(\\sim N (\\mu= \\frac{max-min}{2}; \\sigma^2=\\frac{1}{12}(max-min)^2)\\) A Figura 9.4 expõe os intervalos sob nível de confiança de \\((1-\\alpha)\\)=95% produzidos para as 100 médias de amostras de tamanho 30 extraídas de uma população Uniforme com parâmetros \\(v_{max}:\\) 6 e \\(v_{min}:\\) 2 e, conforme assegura o TCL, o valor médio das médias amostrais (linha tracejada preta) converge assintoticamente para a média da população de origem (linha tracejada em vermelho) com o incremento do tamanho das amostras. meu_titulo1=paste(&quot;Distribuição das médias de&quot;, N, &quot;amostras de tamanho n=&quot;,n,&quot;\\n população de origem sob Dist. Unif. (min: &quot;, min_1, &quot;; max: &quot;, max_1, &quot;)&quot;) meu_titulo2=paste(&quot;As médias amostrais ~ N( x=&quot;,round(mean(m),2),&quot;;sd=&quot;,round(sd(m),2),&quot;)&quot;) dados=as.data.frame(m) ggplot(dados, aes(m)) + geom_histogram(aes(y = stat(density)), bins=10, fill=&quot;lightblue&quot;, col=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, args = list(mean=mean(m), sd=sd(m)), fill = NA, colour=&quot;red&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores das médias amostrais&quot;) + labs(title=meu_titulo1)+ geom_segment(aes(x = mean(m), y = 0, xend = mean(m), yend = max(dnorm(m))), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=mean(m), y=max(dnorm(m)), label=meu_titulo2, angle=0, vjust=-0.5, hjust=0.5, color=&quot;blue&quot;,size=6)+ theme(plot.title = element_text(size = 10, face = &quot;bold&quot;), axis.text.x = element_text(angle=0, hjust=1, size=10), axis.text.y = element_text(angle=0, hjust=1, size=10), axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10)) Figure 9.5: Histograma da distribuição das médias de amostras extraidas de uma população com Distribuição Uniforme mostra que as mesmas seguem uma Distribuição \\(\\sim N (\\mu= \\frac{max-min}{2};\\sigma^2=\\frac{1}{12}(max-min)^2)\\) O histograma da Figura 9.5 ilustra que os valores das médias calculadas de 30 amostras extraídas de uma população com distribuição Uniforme \\(\\sim U (v_{min}, v_{max}\\)) seguem uma distribuição Normal \\(\\sim N (\\mu= \\frac{v_{max}-v_{min}}{2}; \\sigma^2=\\frac{1}{12}(v_{max}-v_{min})^2)\\). Demostração usando amostras extraídas de uma população com distribuição \\(\\sim N (\\mu;\\sigma)\\) # Definindo os parãmetros e a amostra media=80 desvio=4 NN=5000 pop_2=rnorm(n=NN, mean = media, sd = desvio) df=as.data.frame(pop_2) # A distribuição da população ilustrada em um histograma ggplot(df, aes(x=pop_2)) + geom_histogram( binwidth=1,color=&quot;black&quot;, fill=&quot;lightblue&quot;)+ scale_y_continuous(name=&quot;Frequêcia&quot;) + scale_x_continuous(name=&quot;Valores&quot;)+ labs(title= paste(&quot;Histograma de uma população com Distribuição Normal&quot;), subtitle = paste(&quot;Parâmetros: média =&quot;,media,&quot;; desv. padrão =&quot;, desvio))+ theme(plot.title = element_text(size = 10, face = &quot;bold&quot;), axis.text.x = element_text(angle=0, hjust=1, size=10), axis.text.y = element_text(angle=0, hjust=1, size=10), axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10)) Figure 9.6: Histograma de uma população cuja característica de interesse segue uma Distribuição Normal A Figura 9.6 mostra o histograma de uma amostra de 5000 elementos de uma população com Distribuição Normal de parâmetros média= 80 e desvio padrão =4. Figure 9.7: Intervalos de confiança construídos para diversas estimativas amostrais de uma população com Distribuição \\(\\sim N (\\mu; \\sigma)\\) A Figura 9.7 expõe os intervalos sob nível de confiança de \\((1-\\alpha)\\)=95% produzidos para as 100 médias de amostras de tamanho 5000 extraídas de uma população Uniforme com parâmetros \\(v_{max}:\\) 6 e \\(v_{min}:\\) 2 e, conforme assegura o TCL, o valor médio das médias amostrais (linha tracejada preta) converge assintoticamente para a média da população de origem (linha tracejada em vermelho) com o incremento do tamanho das amostras. meu_titulo1=paste(&quot;Distribuição das médias de&quot;, N, &quot;amostras de tamanho n=&quot;,n,&quot;\\n população de origem sob Dist. Normal ( \\u03bc: &quot;, media, &quot;, \\u03c3: &quot;, desvio, &quot;)&quot;) meu_titulo2=paste(&quot;As médias amostrais ~ N( x\\u0304=&quot;,round(mean(m),2),&quot;;sd=&quot;,round(sd(m),2),&quot;)&quot;) dados=as.data.frame(m) ggplot(dados, aes(m)) + geom_histogram(aes(y = stat(density)), bins=10, fill=&quot;lightblue&quot;, col=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, args = list(mean=mean(m), sd=sd(m)), fill = NA, colour=&quot;red&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores das médias amostrais&quot;) + labs(title=meu_titulo1)+ geom_segment(aes(x = mean(m), y = 0, xend = mean(m), yend = max(dnorm(m))), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=mean(m), y=max(dnorm(m)), label=meu_titulo2, angle=0, vjust=-0.5, hjust=0.5, color=&quot;blue&quot;,size=6)+ theme(plot.title = element_text(size = 10, face = &quot;bold&quot;), axis.text.x = element_text(angle=0, hjust=1, size=10), axis.text.y = element_text(angle=0, hjust=1, size=10), axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10)) Figure 9.8: Histograma da distribuição das médias de amostras extraidas de uma população Normal mostra que as mesmas seguem uma Distribuição \\(\\sim N (\\stackrel{-}{x}= \\mu; s=\\frac{\\sigma}{\\sqrt{n}})\\) O histograma da Figura 9.8 ilustra que os valores das médias calculadas de 5000 amostras extraídas de uma população com distribuição Normal \\(\\sim N (\\mu, \\sigma)\\) seguem uma distribuição Normal \\(\\sim N (\\mu= \\mu; \\sigma=\\frac{\\sigma}{\\sqrt{n}})\\). Sendo o erro amostral expresso como: \\(\\varepsilon=\\stackrel{-}{X} - \\mu\\), o histograma abaixo ilustra que os valores dos erros calculados de 5000 amostras extraídas de uma população com distribuição Normal \\(\\sim N (\\mu, \\sigma)\\) seguem uma distribuição Normal \\(\\sim N (\\mu= \\mu; \\sigma=\\frac{\\sigma}{\\sqrt{n}})\\). N=100 n=50 mu=80 sigma=4 conf=0.95 matriz=IC.Na(N, n, mu, sigma, conf) Figure 9.9: Histograma da distribuição dos erros de amostras de tamanho n, extraidas de uma população com distribuição \\(\\sim N(\\mu; \\sigma)\\) mostra que os mesmos seguem uma distribuição \\(\\sim N (0; s=\\frac{\\sigma}{\\sqrt{n}})\\) erro_min=min(matriz$erro) erro_max=max(matriz$erro) meu_titulo1=paste(&quot;Distribuição dos erros de&quot;, N, &quot;amostras de tamanho n=&quot;,n,&quot;\\n extraídas de uma população Normal ( \\u03bc: &quot;, mu, &quot;, \\u03c3: &quot;, sigma, &quot;)&quot;) meu_titulo2=paste(&quot;Os erros amostrais ~ N( x\\u0304=&quot;,round(mean(matriz$erro),2),&quot;~0 ; sd=&quot;,round(sd(matriz$erro),2),&quot; ~\\u03c3/sqrt(n))&quot;) ggplot(matriz, aes(x=erro)) + geom_histogram(aes(y = stat(density)), bins=round(sqrt(N),0), fill=&quot;lightblue&quot;, col=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, args = list(mean=mean(matriz$erro), sd=sd(matriz$erro)), fill = NA, colour=&quot;red&quot;) + scale_y_continuous(name=&quot;Frequência&quot;) + scale_x_continuous(name=&quot;Valores dos erros amostrais&quot;, limits=c(-2,2) )+ labs(title=meu_titulo1)+ annotate(geom=&quot;text&quot;, label=meu_titulo2, x=-0.7,y= 0.9, angle=0, vjust=-0.5, hjust=0.5, color=&quot;blue&quot;,size=4)+ theme(plot.title = element_text(size = 10, face = &quot;bold&quot;), axis.text.x = element_text(angle=0, hjust=1, size=10), axis.text.y = element_text(angle=0, hjust=1, size=10), axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10)) Figure 9.10: Histograma da distribuição dos erros de amostras de tamanho n, extraidas de uma população com distribuição \\(\\sim N(\\mu; \\sigma)\\) mostra que os mesmos seguem uma distribuição \\(\\sim N (0; s=\\frac{\\sigma}{\\sqrt{n}})\\) Corolário: se \\((X_{1}, X_{2},...,X{n})\\) for uma amostra aleatória simples da população \\(X\\) de média \\(\\mu\\) e variância \\(\\sigma^{2}\\) conhecida, e \\(\\stackrel{-}{X}= \\frac{(X_{1}+X_{2}+...+X{n})}{n}\\), tal que \\(n\\ge 30\\), então a estatística \\(Z\\) pode ser definida, bem como sua correspondente distribuição: \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0 ,1) \\] Uma vez que a estatística \\(Z \\sim N(0 ,1)\\) (ela ``decorre’’ da padronização da variável aleatória \\(\\stackrel{-}{X}\\)) as probabilidades para os intervalos desejados de valores \\(Z\\) podem ser facilmente encontrados em tabelas, como mais adiante se verá na constução de intervalos de confiança. 9.3.1 Fator de correção para populações finitas Se amostras de tamanho \\(n\\) sem reposição são extraídas de uma população finita de tamanho N aplica-se o fator de correção para populações finitas (\\(\\sqrt{\\frac{(N-n)}{(N-1)}}\\)) junto ao desvio padrão das expressões do erro máximo \\(\\varepsilon\\) anteriormente expostas: \\[\\begin{align*} \\varepsilon &amp; =(\\stackrel{-}{x}-\\mu)={z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\cdot \\sqrt{\\frac{(N-n)}{(N-1)}} \\\\ &amp; =(\\stackrel{-}{x}-\\mu)={z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}} \\cdot \\sqrt{\\frac{(N-n)}{(N-1)}}\\\\ &amp; =(\\stackrel{-}{x}-\\mu)= ({t}_{(1-\\frac{\\alpha }{2}, (n-1))} \\cdot \\frac{S}{\\sqrt{n}} \\cdot \\sqrt{\\frac{(N-n)}{(N-1)}})\\\\ \\end{align*}\\] Portanto, para populações finitas com amostragem sem reposição (com \\(n&lt;N\\)): \\[ \\stackrel{-}{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n} \\cdot \\frac{(N-n)}{(N-1)} ) \\] 9.3.2 Intervalo de confiança para médias amostrais Se, por alguma razão, a variância populacional (\\(\\sigma^{2}\\)) é conhecida, podemos utilizar \\(\\stackrel{-}{X}\\) como estimador pontual da média. Assim, \\(X\\) seguirá uma distribuição Normal tal que:   \\[ \\stackrel{-}{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n}) \\] Segue também que a estatística \\(Z\\), como antes definida, seguirá uma distribuição Normal tal que:   \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0 ,1) \\] com: \\(\\stackrel{-}{X}\\) é a média da amostra; \\(\\mu\\) é a média populacional; \\(\\sigma\\) é o desvio padrão populacional; e, \\(n\\) é o tamanho da amostra extraída. Entretanto, a situação mais usual é aquela na qual não termos informação alguma sobre a variância populacional (\\(\\sigma^{2}\\)). Nessas situações, se o tamanho da amostra é grande (na prática \\(n\\ge 30\\)), podemos substituir \\(\\sigma\\) na estatística \\(Z\\) por \\(S\\): substituir o desvio padrão populacional pelo desvio padrão da amostra extraída, sem que o erro cometido com esta substituição seja grande. Com tal substituição, a estatística \\(Z\\) e passa a ser tal que: \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{S}{\\sqrt{n}}} \\sim N(0 , 1) \\] em que: \\(\\stackrel{-}{X}\\) é a média amostral; \\(\\mu\\) é a média populacional; \\(S\\) é o desvio padrão da amostra; e, \\(n\\) é o tamanho da amostra. Caso a variância populacional (\\(\\sigma^{2}\\)) não seja conhecida e o tamanho da amostra não possa ser admitido como grande (\\(n&lt;30\\)) e sendo o estimador da variância amostral assim definido:   \\[ {S}^{2}=\\frac{1}{\\left(n-1\\right)}\\sum _{i=1}^{n}{\\left({X}_{i}-\\stackrel{-}{{X}_{1}}\\right)}^{2} \\] Definindo-se a variável \\(Y = \\frac{(n-1)\\cdot s^{2}}{\\sigma^{2}}\\) tem uma distribuição \\(\\chi^{2}\\) com (n-1) graus de liberdade tal que:   \\[ Y = \\frac{(n-1)\\cdot s^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{(n-1)}, \\]   e considerando-se que \\(Z\\) é tal que:   \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0 ,1) \\] segue a estatística \\(T\\) e sua correspondente distribuição, denominada por t de Student :   \\[ T=\\frac{Z}{\\sqrt{\\frac{Y}{\\left(n-1\\right)}}} \\sim {t}_{\\left(n-1\\right)}. \\] Para essa situação na qual a variância populacional não é conhecida e o tamanho amostral é pequeno, com alguma manipulação chega-se à estatística \\(T\\) e sua correspondente distribuição:   \\[ T = \\frac{(\\stackrel{-}{X} - \\mu)}{ \\frac{S}{\\sqrt{n}} } \\sim t_{(n-1)} \\] em que: \\(\\stackrel{-}{X}\\) é a média amostral; \\(\\mu\\) é a média populacional; \\(S\\) é o desvio padrão da amostra; e, \\(n\\) é o tamanho da amostra; e, \\((n-1)\\) é uma quantidade denominada como graus de liberdade. As probabilidades associadas a um intervalo para um determinado valor da estatística ``t’’ da distribuição de Student encontram-se tabeladas para variados graus de liberdade , como mais adiante se verá na constução de intervalos de confiança. 9.3.3 Intervalo de confiança bilateral para uma média amostral sob variância populacional conhecida (Figura 6.9)   \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0 ,1) \\]   em que: \\(\\stackrel{-}{X}\\) é a média amostral; \\(\\mu\\) é a média populacional; \\(\\sigma\\) é o desvio padrão populacional; \\(n\\) é o tamanho da amostra; e, \\(Z\\) é a estatística a ser calculada para a construção do intervalo de confiança sob o nível de significância \\(\\alpha\\) estabelecido. alfa=0.05 prob_desejada1=alfa/2 z_desejado1=round(qnorm(prob_desejada1),4) d_desejada1=dnorm(z_desejado1, 0, 1) prob_desejada2=1-alfa/2 z_desejado2=round(qnorm(prob_desejada2),4) d_desejada2=dnorm(z_desejado2, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``z&#39;&#39; da distribuição Normal padrão&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição Normal Padrão&quot;, subtitle = &quot;P(-z, z)=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; -z)= P(z; \\U221e)= \\u03b1/2 em vermelho &quot;)+ geom_segment(aes(x = z_desejado1, y = 0, xend = z_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = z_desejado2, y = 0, xend = z_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-0.1, y=d_desejada1, label=&quot;-z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.3, y=d_desejada2, label=&quot;z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado1-1.8, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Intervalo fechado \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.11: Regiões críticas, aquém e além das quais, a probabilidade associada aos valores \\(Z\\) é inferior a \\(\\frac{\\alpha}{2}\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 9.11 observa-se:   o nível de significância \\(\\alpha\\); o nível de confiança \\((1-\\alpha)\\); e, o valor tabelado da estatística \\(Z(z)\\) para o nível de confiança fixado. Assim, \\[\\begin{align*} P\\left[-{Z}_{(1-\\frac{\\alpha }{2})}\\le Z \\le {Z }_{(1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P\\left[-{z}_{(1-\\frac{\\alpha }{2})}\\le \\frac{\\stackrel{-}{x}-\\mu }{\\frac{\\sigma}{\\sqrt{n}}} \\le {z}_{(1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P[\\stackrel{-}{x}-({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{\\sigma}{\\sqrt{n}}) \\le \\mu \\le \\stackrel{-}{x}+({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{\\sigma}{\\sqrt{n}}) ] &amp; = (1-\\alpha) \\end{align*}\\] \\[ IC(\\mu)_{(1-\\alpha)} = [\\stackrel{-}{x} \\pm {z}_{c} \\cdot \\frac{\\sigma}{\\sqrt{n}}] \\] Assim, se \\(\\stackrel{-}{x}\\) é usado como estimativa de \\(\\mu\\), podemos afirmar estar \\(100.(1-\\alpha)\\)% confiantes de que o erro não excederá \\(({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{\\sigma}{\\sqrt{n}})\\).   A quantidade \\(\\varepsilon=(\\stackrel{-}{x}-\\mu)={z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\) é chamada de Erro máximo da estimativa ao se arbitrar um nível de confiança \\(\\alpha\\) para um determinado tamanho amostral. Exemplo: As vendas de 15 lojas de uma região do país apresentam uma média igual a US$ 20.000,00. Sabendo-se que as vendas de todas as lojas da região é uma variável aleatória que segue uma distribuição Normal, com desvio padrão igual a US$ 8.300,00, construa o intervalo de confiança para a média ao nível de confiança de 95%. Dados do problema:   o tamanho da amostra: \\(n=15\\); a média amostral: \\(\\stackrel{-}{x}\\) = US$ 20.000; o desvio padrão populacional: \\(\\sigma\\)= US$ 8.300; nível de confiança: \\((1-\\alpha) = 0,95\\); e, valor extraído da tabela \\(z=1,96\\) correspondente ao nível de confiança estipulado \\((1-\\alpha)=95\\%\\). \\[\\begin{align*} P[\\stackrel{-}{x}-({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{\\sigma}{\\sqrt{n}}) \\le \\mu \\le \\stackrel{-}{x}+({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{\\sigma}{\\sqrt{n}}) ] &amp; = (1-\\alpha) \\\\ P[20.000 - (1,96 \\cdot \\frac{8.300}{\\sqrt{15}}) \\le \\mu \\le 20000 + ( 1,96 \\cdot \\frac{8.300}{\\sqrt{15}}) ] &amp; = 0,95 \\\\ P[20.000 - 4.200,38 \\le \\mu \\le 20.000 + 4.200,38 ] &amp; = 0,95 \\\\ \\end{align*}\\] \\[ IC_{(1-\\alpha=0,95)} = [US\\$ 15.799,62; US\\$ 24.200,38] \\] Se quisermos ser rigorosos na interpretação do intervalo de confiança calculado podemos explicar que, se extrairmos um grande número de amostras de tamanho 15 dessa população, e para todas elas calcularmos intervalos de confiança como o acima definido, a proporção desses intervalos onde poderemos encontrar a média populacional de vendas será de 0,95 (95 intervalos em 100). De uma forma mais sintética, podemos afirmar que o intervalo aleatório ]US$ 15.799,62; US$ 24.200,38[, é um intervalo de confiança a 95% para a média de vendas. De forma mais corrente, embora menos correta em termos teóricos, é usual afirmar que, com 95% de confiança a média de vendas se situa entre os valores US$ 15.799,62 e US$ 24.200,38. Intervalos de confiança unilaterais para uma média amostral sob variância populacional conhecida. A Figura 6.10 ilustra um intervalo de confiança unilateral limitado à direita por um valor máximo, dde tal sorte que a probabilidade associada ao intervalo de valores da estatística \\(Z\\) inferiores a esse limitante é \\[ P\\left [\\mu \\le \\bar{x} + {z}_{c} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right ] = (1- \\alpha) \\] prob_desejada=0.95 z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, 0), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``z&#39;&#39; da distribuição Normal padrão&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado), colour=&quot;black&quot;)+ geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c( z_desejado, 4), colour=&quot;black&quot;)+ labs(title= &quot;Curva da função densidade \\nDistribuição Normal Padrão&quot;, subtitle = &quot;P(-\\U221e; z)=(1-\\u03b1) em cinza (nível de confiança) \\nP(z, + \\U221e)= \\u03b1, em vermelho &quot;)+ annotate(geom=&quot;text&quot;, x=z_desejado1+3.5, y=d_desejada1, label=&quot;z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado1+4.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Intervalo aberto à esq. \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.12: Região crítica, além da qual, a probabilidade associada aos valores \\(Z\\) é inferior a \\(\\alpha\\), delimitando assim, à esquerda, um intervalo aberto com nível de confiança igual a \\((1-\\alpha)\\) A Figura 9.13 ilustra um intervalo de confiança unilateral limitado à esquerda por um valor mínimo, de tal sorte que a probabilidade associada ao intervalo de valores da estatística \\(Z\\) superiores a esse limitante é \\[ P\\left [\\mu \\ge \\bar{x} - {z}_{c} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right ] = (1- \\alpha) \\] prob_desejada=0.05 z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, 0), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``z&#39;&#39; da distribuição Normal padrão&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;)+ geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c( z_desejado, 4), colour=&quot;black&quot;)+ labs(title= &quot;Curva da função densidade \\nDistribuição Normal Padrão&quot;, subtitle = &quot;P(-\\U221e; z)=\\u03b1, em vermelho \\nP(z, + \\U221e)= (1-\\u03b1) em cinza&quot;)+ annotate(geom=&quot;text&quot;, x=z_desejado1+0.5, y=d_desejada1, label=&quot;-z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado1-2, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Intervalo aberto à dir. \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.13: Região crítica, aquém da qual, a probabilidade associada aos valores \\(Z\\) é inferior a \\(\\alpha\\), delimitando assim, à direita, um intervalo aberto com nível de confiança igual a \\((1-\\alpha)\\) 9.3.4 Intervalo de confiança para uma média amostral sob variância populacional desconhecida mas amostras não tão pequenas: \\(n \\ge 30\\) (Figura 9.14)   \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{S}{\\sqrt{n}}} \\sim N(0 , 1) \\]   em que: \\(\\stackrel{-}{X}\\) é a média amostral; \\(\\mu\\) é a média populacional; \\(S\\) é o desvio padrão amostral; \\(n\\) é o tamanho da amostra; e, \\(Z\\) é a estatística a ser calculada para a construção do intervalo de confiança sob o nível de significância \\(\\alpha\\) estabelecido. alfa=0.05 prob_desejada1=alfa/2 z_desejado1=round(qnorm(prob_desejada1),4) d_desejada1=dnorm(z_desejado1, 0, 1) prob_desejada2=1-alfa/2 z_desejado2=round(qnorm(prob_desejada2),4) d_desejada2=dnorm(z_desejado2, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``z&#39;&#39; da distribuição Normal padrão&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição Normal Padrão&quot;, subtitle = &quot;P(-z; z)=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; -z)= P(z; \\U221e)= \\u03b1/2 em vermelho&quot;)+ geom_segment(aes(x = z_desejado1, y = 0, xend = z_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = z_desejado2, y = 0, xend = z_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-0.1, y=d_desejada1, label=&quot;-z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.3, y=d_desejada2, label=&quot;z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado1-1.8, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Intervalo fechado \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.14: Regiões críticas, aquém e além das quais, a probabilidade associada aos valores \\(Z\\) é inferior a \\(\\frac{\\alpha}{2}\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 9.14 observa-se:   o nível de significância \\(\\alpha\\); o nível de confiança \\((1-\\alpha)\\); e, o valor tabelado da estatística \\(Z(z)\\) para o nível de confiança fixado. Assim,   \\[\\begin{align*} P\\left[-{Z}_{(1-\\frac{\\alpha }{2})}\\le Z \\le {Z }_{(1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P\\left[-{z}_{(1-\\frac{\\alpha }{2})}\\le \\frac{\\stackrel{-}{x}-\\mu }{(\\frac{S}{\\sqrt{n})}} \\le {z}_{(1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P[\\stackrel{-}{x}-({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}}) \\le \\mu \\le \\stackrel{-}{x}+({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}}) ] &amp; = (1-\\alpha) \\end{align*}\\] \\[ IC(\\mu)_{(1-\\alpha)} = [\\stackrel{-}{x} \\pm {z}_{c} \\cdot \\frac{S}{\\sqrt{n}} ] \\] Assim, se \\(\\stackrel{-}{x}\\) é usado como estimativa de \\(\\mu\\) podemos afirmar estar \\(100(1-\\alpha)\\)% confiantes de que o erro não excederá \\(({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}})\\).   A quantidade \\(\\varepsilon=(\\stackrel{-}{x}-\\mu)={z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}}\\) é chamada de Erro máximo da estimativa ao se arbitrar um nível de confiança \\(\\alpha\\) para um determinado tamanho amostral. Exemplo: As vendas de 60 lojas de uma região do país apresentam uma média igual a US$ 20.000,00 e desvio padrão de US$ 8.300,00. Construa o intervalo de confiança para a média ao nível de confiança de 95%. Dados do problema:   o tamanho da amostra: \\(n=60\\); a média amostral: \\(\\stackrel{-}{x}=US\\$ 20.000\\); o desvio padrão amostral: \\(s=US\\$ 8.300\\); nível de confiança: \\((1-\\alpha)=0,95\\); e, valor extraído da tabela \\(z=1,96\\) correspondente ao nível de confiança estipulado \\((1-\\alpha)=95\\%\\). \\[\\begin{align*} P[\\stackrel{-}{x}-({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}}) \\le \\mu \\le \\stackrel{-}{x}+({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}}) ] &amp; = (1-\\alpha) \\\\ P[20.000 - (1,96 \\cdot \\frac{8.300}{\\sqrt{60}}) \\le \\mu \\le 20.000 + ( 1,96 \\cdot \\frac{8.300}{\\sqrt{60}}) ] &amp; = 0,95 \\\\ P[20.000 - 2.100,19 \\le \\mu \\le 20.000 + 2.100,19 ] &amp; = 0,95 \\end{align*}\\] \\[ IC_{(1-\\alpha=0,95)} = [US\\$ 17.899,81;US\\$ 22.100,19] \\] Se quisermos ser rigorosos na interpretação do intervalo de confiança calculado podemos explicar que se extrairmos um grande número de amostras de tamanho 60 dessa população, e para todas elas calcularmos intervalos de confiança como o acima definido, a proporção desses intervalos onde poderemos encontrar a média populacional de vendas será de 0,95 (95 intervalos em 100). De uma forma mais sintética, podemos afirmar que o intervalo aleatório ]US$ 17.899,81; US$ 22.100,19[, é um intervalo de confiança a 95% para a média de vendas. De forma mais corrente, embora menos correta em termos teóricos, é usual afirmar que, com 95% de confiança a média de vendas se situa entre os valores US$ 17.899,81 e US$ 22.100,19. Intervalos de confiança unilaterais para uma média amostral sob variância populacional desconhecida mas amostras não tão pequenas: \\(n \\ge 30\\). A Figura 9.15 ilustra um intervalo de confiança unilateral limitado à direita por um valor máximo, de tal sorte que a probabilidade associada ao intervalo de valores da estatística \\(Z\\) inferiores a esse limitante é \\[ P\\left [\\mu \\le \\bar{x} + {z}_{c} \\cdot \\frac{S}{\\sqrt{n}} \\right ] = (1- \\alpha) \\] prob_desejada=0.95 z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, 0), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``z&#39;&#39; da distribuição Normal padrão&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado), colour=&quot;black&quot;)+ geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c( z_desejado, 4), colour=&quot;black&quot;)+ labs(title= &quot;Curva da função densidade \\nDistribuição Normal Padrão&quot;, subtitle = &quot;P(-\\U221e; z)=(1-\\u03b1) em cinza (nível de confiança) \\nP(z, + \\U221e)= \\u03b1, em vermelho &quot;)+ annotate(geom=&quot;text&quot;, x=z_desejado1+3.5, y=d_desejada1, label=&quot;z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado1+4.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Intervalo aberto à esq. \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.15: Região crítica, além da qual, a probabilidade associada aos valores \\(Z\\) é inferior a \\(\\alpha\\), delimitando assim, à esquerda, um intervalo aberto com nível de confiança igual a \\((1-\\alpha)\\) A Figura 9.16 ilustra um intervalo de confiança unilateral limitado à esquerda por um valor mínimo, de tal sorte que a probabilidade associada ao intervalo de valores da estatística \\(Z\\) superiores a esse limitante é \\[ P\\left [\\mu \\ge \\bar{x} - {z}_{c} \\cdot \\frac{S}{\\sqrt{n}} \\right ] = (1- \\alpha) \\] prob_desejada=0.05 z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, 0), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``z&#39;&#39; da distribuição Normal padrão&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;)+ geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c( z_desejado, 4), colour=&quot;black&quot;)+ labs(title= &quot;Curva da função densidade \\nDistribuição Normal Padrão&quot;, subtitle = &quot;P(-\\U221e; z)=\\u03b1, em vermelho \\nP(z, + \\U221e)= (1-\\u03b1) em cinza&quot;)+ annotate(geom=&quot;text&quot;, x=z_desejado1+0.5, y=d_desejada1, label=&quot;-z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado1-1.5, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Intervalo aberto à dir. \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.16: Região crítica, aquém da qual, a probabilidade associada aos valores \\(Z\\) é inferior a \\(\\alpha\\), delimitando assim, à direita, um intervalo aberto com nível de confiança igual a \\((1-\\alpha)\\) 9.3.5 Intervalo de confiança para uma média amostral sob variância populacional desconhecida e amostras de qualquer tamanho (Figura 9.17)   \\[ T = \\frac{(\\stackrel{-}{X} - \\mu)}{ \\frac{S}{\\sqrt{n}} } \\sim t_{(n-1)} \\]   em que: \\(\\stackrel{-}{X}\\) é a média amostral; \\(\\mu\\) é a média populacional; \\(S\\) é o desvio padrão amostral; \\(n\\) é o tamanho da amostra; e, \\(T\\) é a estatística a ser calculada para a construção do intervalo de confiança sob o nível de significância \\(\\alpha\\) estabelecido. alfa=0.05 prob_desejada1=alfa/2 df=20 t_desejado1=round(qt(prob_desejada1,df ),4) d_desejada1=dt(t_desejado1,df) prob_desejada2=1-alfa/2 df=20 t_desejado2=round(qt(prob_desejada2, df),4) d_desejada2=dt(t_desejado2,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``t&#39;&#39; da distribuição de Student com gl=n-1&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição t &quot;, subtitle = &quot;P(-t; t)=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; -t)= P(t; \\U221e)= \\u03b1/2 em vermelho &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = t_desejado2, y = 0, xend = t_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-0.1, y=d_desejada1, label=&quot;-t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.3, y=d_desejada2, label=&quot;t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado1-1.8, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+1.3, y=0.2, label=&quot;Intervalo fechado \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.17: Regiões críticas, aquém e além das quais, a probabilidade associada aos valores \\(T\\) (\\((n-1)\\) graus de liberdade) é inferior a \\(\\frac{\\alpha}{2}\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 9.17 observa-se:   o nível de significância \\(\\alpha\\); o nível de confiança \\((1-\\alpha)\\); e, o valor tabelado da estatística \\(T(t)\\) sob \\(n-1\\) graus de liberdade para o nível de confiança fixado. Assim,   \\[\\begin{align*} P\\left[-{T}_{(1-\\frac{\\alpha }{2}, (n-1))}\\le T \\le {T }_{(1-\\frac{\\alpha }{2}, (n-1))}\\right] &amp; = (1-\\alpha) \\\\ P\\left[-{t}_{(1-\\frac{\\alpha }{2}, (n-1))}\\le \\frac{\\stackrel{-}{x}-\\mu }{\\frac{S}{\\sqrt{n}}} \\le {t}_{(1-\\frac{\\alpha }{2}, (n-1))}\\right] &amp; = (1-\\alpha) \\\\ P[\\stackrel{-}{x}-({t}_{(1-\\frac{\\alpha }{2}, (n-1))} \\cdot \\frac{S}{\\sqrt{n}}) \\le \\mu \\le \\stackrel{-}{x}+({t}_{(1-\\frac{\\alpha }{2}, (n-1))} \\cdot \\frac{S}{\\sqrt{n}}) ] &amp; = (1-\\alpha) \\end{align*}\\] \\[ IC(\\mu)_{(1-\\alpha)}= [\\stackrel{-}{x} \\pm {t}_{c_{(n-1)}} \\cdot \\frac{S}{\\sqrt{n}}] \\] Assim, se \\(\\stackrel{-}{x}\\) é usado como estimativa de \\(\\mu\\) podemos afirmar estar \\(100(1-\\alpha)\\)% confiantes de que o erro não excederá \\(({t}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}})\\).   A quantidade \\(\\varepsilon=(\\stackrel{-}{x}-\\mu)= ({t}_{(1-\\frac{\\alpha }{2}, (n-1))} \\cdot \\frac{S}{\\sqrt{n}})\\) é chamada de Erro máximo da estimativa ao se arbitrar um nível de confiança \\(\\alpha\\), (n-1) graus de liberdade e um determinado tamanho amostral. Exemplo: As vendas de 15 lojas de uma região do país apresentam uma média igual a US$ 20.000,00 e desvio padrão de US$ 8.300,00. Construa o intervalo de confiança para a média ao nível de confiança de 95%. Dados do problema: o tamanho da amostra: \\(n=15\\); a média amostral: \\(\\stackrel{-}{x}=US\\$ 20.000\\); o desvio padrão amostral: \\(s=US\\$ 8.300\\); nível de confiança: \\((1-\\alpha)=0,95\\); e, valor extraído da tabela da distribuição de sob \\((n-1=15-1=14)\\) graus de liberdade \\(t_{c}=2,1448\\) associado ao nível de confiança estipulado \\((1-\\alpha)=95\\%\\). \\[\\begin{align*} P[\\stackrel{-}{x}-({t}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}}) \\le \\mu \\le \\stackrel{-}{x}+({t}_{(1-\\frac{\\alpha }{2})} \\cdot \\frac{S}{\\sqrt{n}}) ] &amp; = (1-\\alpha) \\\\ P[20000 - ( 2,1448 \\cdot \\frac{8300}{\\sqrt{15}}) \\le \\mu \\le 20000 + ( 2,1448 \\cdot \\frac{8300}{\\sqrt{15}}) ] &amp; = 0,95\\\\ P[20000 - 4596,41 \\le \\mu \\le 20000 + 4596,41 ] &amp; = 0,95 \\end{align*}\\] \\[ IC_{(1-\\alpha=0,95)} = [US\\$ 15403,59 ; US\\$ 24496,41] \\] Se quisermos ser rigorosos na interpretação do intervalo de confiança calculado podemos explicar que se extrairmos um grande número de amostras de tamanho 15 dessa população, e para todas elas calcularmos intervalos de confiança como o acima definido, a proporção desses intervalos onde poderemos encontrar a média populacional de vendas será de 0,95 (95 intervalos em 100). De uma forma mais sintética, podemos afirmar que o intervalo aleatório ]US$ 15.403,59; US$ 24.496,41[, é um intervalo de confiança a 95% para a média de vendas. De uma forma mais corrente, embora menos correta em termos teóricos, é usual afirmar que, com 95% de confiança a média de vendas se situa entre os valores US$ 15.403,59 e US$ 24.496,41. Intervalos de confiança unilaterais para uma média amostral sob variância populacional desconhecida e amostras de qualquer tamanho A Figura 9.18 ilustra um intervalo de confiança unilateral limitado à direita por um valor máximo, de tal sorte que a probabilidade associada ao intervalo de valores da estatística \\(T\\) inferiores a esse limitante é \\[ P\\left [\\mu \\le \\bar{x} + {t}_{c_{(n-1)}} \\cdot \\frac{S}{\\sqrt{n}} \\right ] = (1- \\alpha) \\] alfa=0.95 prob_desejada1=alfa df=20 t_desejado1=round(qt(prob_desejada1,df ),4) d_desejada1=dt(t_desejado1,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c( t_desejado1, 4), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(-4, 0), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``t&#39;&#39; da distribuição de Student com gl=n-1&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição t &quot;, subtitle = &quot;P(-\\U221e, t)=(1-\\u03b1) em cinza \\nP(t, \\U221e)= \\u03b1 em vermelho &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+0.5, y=d_desejada1, label=&quot;t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado1+1, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-2.5, y=0.2, label=&quot;Intervalo aberto \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.18: Região crítica, além da qual, a probabilidade associada aos valores \\(T\\) (\\((n-1)\\) graus de liberdade) é inferior a \\(\\alpha\\), delimitando assim, à esquerda, um intervalo aberto com nível de confiança igual a \\((1-\\alpha)\\) A Figura 9.19 ilustra um intervalo de confiança unilateral limitado à esquerda por um valor mínimo, de tal sorte que a probabilidade associada ao intervalo de valores da estatística \\(T\\) superiores a esse limitante é \\[ P\\left [\\mu \\ge \\bar{x} - {t}_{c} \\cdot \\frac{S}{\\sqrt{n}} \\right ] = (1- \\alpha) \\] alfa=0.05 prob_desejada1=alfa df=20 t_desejado1=round(qt(prob_desejada1,df ),4) d_desejada1=dt(t_desejado1,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, 4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``t&#39;&#39; da distribuição de Student com gl=n-1&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição t &quot;, subtitle = &quot;P(-t, \\U221e)=(1-\\u03b1) em cinza \\nP(-\\U221e; -t)= \\u03b1 em vermelho &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-0.1, y=d_desejada1, label=&quot;-t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado1-2.5, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+1, y=0.2, label=&quot;Intervalo aberto \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.19: Região crítica, aquém da qual, a probabilidade associada aos valores \\(T\\) (\\((n-1)\\) graus de liberdade) é inferior a \\(\\alpha\\), delimitando assim, à direita, um intervalo aberto com nível de confiança igual a \\((1-\\alpha)\\) "],["distribuição-das-diferenças-de-médias-amostrais-independentes-e-seus-intervalos-de-confiança.html", "9.4 Distribuição das diferenças de médias amostrais independentes e seus intervalos de confiança", " 9.4 Distribuição das diferenças de médias amostrais independentes e seus intervalos de confiança Consideremos duas populações \\(X\\) e \\(Y\\) com médias \\(\\mu_{1}\\) e \\(\\mu_{2}\\) e variâncias \\(\\sigma_{1}^{2}\\) e \\(\\sigma_{2}^{2}\\), respectivamente.   Conforme seções anteriores, as médias amostrais \\(\\stackrel{-}{X}\\) e \\(\\stackrel{-}{Y}\\) são duas variáveis aleatórias tais que: \\[\\begin{align*} \\stackrel{-}{X} &amp; \\sim N(\\mu_{1}, \\frac{\\sigma^{2}_{1}}{n_{1}} )\\\\ \\stackrel{-}{Y} &amp; \\sim N(\\mu_{2}, \\frac{\\sigma^{2}_{2}}{n_{2}} ) \\end{align*}\\] Pode-se demonstrar, pelas propriedades da esperança e da variância, que a média e a variância de uma variável aleatória (população) que resulta da soma ou diferença de duas outras, \\(X\\) e \\(Y\\), é: \\[\\begin{align*} \\mu_{(X \\pm Y)} &amp; = \\mu_{1} \\pm \\mu_{2}\\\\ \\sigma^{2}_{(X \\pm Y)} &amp; = \\sigma_{1}^{2} + \\sigma_{2}^{2} \\end{align*}\\] E a média e variância da soma ou diferença das distribuições amostrais das médias de \\(X\\) e \\(Y\\) é: \\[\\begin{align*} \\mu_{(\\stackrel{-}{X} \\pm \\stackrel{-}{Y})} &amp; = \\mu_{1} \\pm \\mu_{2} \\\\ \\sigma^{2}_{(\\stackrel{-}{X} \\pm \\stackrel{-}{Y})} &amp; = \\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{\\sigma_{2}^{2}}{n_{2}} \\end{align*}\\] 9.4.1 Intervalos de confiança para a diferença entre duas médias amostrais com variâncias populacionais conhecidas Se \\((X_{1}, X_{2},...,X{n_{1}})\\) e \\((Y_{1}, Y_{2},...,Y{n_{2}})\\) forem amostras aleatórias simples das populações \\(X\\) e \\(Y\\) com médias \\(\\mu_{1}\\) e \\(\\mu_{2}\\), e variâncias \\(\\sigma_{1}^{2}\\) e \\(\\sigma_{2}^{2}\\) conhecidas, e \\(\\stackrel{-}{X}=\\frac{(X_{1}+X_{2}+...+X{n_{1}})}{n}\\) e \\(\\stackrel{-}{Y}=\\frac{(Y_{1}+Y_{2}+...+Y{n_{2}})}{n_{2}}\\), então: \\[\\begin{align*} {X} &amp; \\sim N( \\mu_{1} , \\frac{\\sigma_{1}}{\\sqrt{n_{1}}} ) \\\\ {Y} &amp; \\sim N( \\mu_{2} , \\frac{\\sigma_{2}}{\\sqrt{n_{2}}} ) \\end{align*}\\] Demonstra-se que a diferença entre \\(\\stackrel{-}{X} e \\stackrel{-}{Y}\\) é tal que: \\[ \\stackrel{-}{X} - \\stackrel{-}{Y} \\sim N((\\mu_{1}-\\mu_{2}) , \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ) \\] Demonstra-se que a estatística \\(Z\\) pode ser assim definida, bem como sua correspondente distribuição (cf.Figura 9.20): \\[ Z = \\frac{ (\\stackrel{-}{X}-\\stackrel{-}{Y}) - (\\mu_{1}-\\mu_{2})}{ \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } } \\sim N(0 ,1) \\] em que: \\(\\stackrel{-}{X}\\)e \\(\\stackrel{-}{Y}\\) são as médias amostrais; \\(\\mu_{1}\\) e \\(\\mu_{2}\\) são as médias populacionais; \\(\\sigma_{1}^{2}\\) e \\(\\sigma_{2}^{2}\\) são as variâncias populacionais; e, \\(n_{1}\\) e \\(n_{2}\\) são os tamanhos das amostras alfa=0.05 prob_desejada1=alfa/2 z_desejado1=round(qnorm(prob_desejada1),4) d_desejada1=dnorm(z_desejado1, 0, 1) prob_desejada2=1-alfa/2 z_desejado2=round(qnorm(prob_desejada2),4) d_desejada2=dnorm(z_desejado2, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``z&#39;&#39; da distribuição Normal padrão&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição Normal Padrão&quot;, subtitle = &quot;P(-z; z)=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; -z)= P(z; \\U221e)= \\u03b1/2 em vermelho&quot;)+ geom_segment(aes(x = z_desejado1, y = 0, xend = z_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = z_desejado2, y = 0, xend = z_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-0.1, y=d_desejada1, label=&quot;-z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.3, y=d_desejada2, label=&quot;z&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=z_desejado1-1.8, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Intervalo fechado \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.20: Regiões críticas, aquém e além das quais, a probabilidade associada aos valores da estatística \\(Z\\) é inferior a \\(\\frac{\\alpha}{2}\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 9.20 observa-se:   o nível de significância \\(\\alpha\\); o nível de confiança \\((1-\\alpha)\\); e, o valor tabelado da estatística \\(Z(z)\\) para o nível de confiança fixado. Assim, \\[\\begin{align*} P\\left[-{Z}_{(1-\\frac{\\alpha }{2})}\\le Z \\le {Z }_{(1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P\\left[-{z}_{(1-\\frac{\\alpha }{2})}\\le \\frac{ (\\stackrel{-}{x}-\\stackrel{-}{y}) - (\\mu_{1}-\\mu_{2})}{ \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } } \\le {z}_{(1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ) ] &amp; = (1-\\alpha) \\end{align*}\\]   \\[ IC(\\mu_{1}-\\mu_{2})_{(1-\\alpha)}=[ (\\stackrel{-}{x}-\\stackrel{-}{y} ) \\pm {z}_{c} \\cdot \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ] \\] Exemplo: Uma empresa possui duas filiais (A e B). Uma amostra das vendas de 20 dias forneceu uma venda média diária de 40 unidades dessa peça a filial A e de 30 unidades da mesma peça para a filial B. Os desvios padrão das vendas diárias dessa peça são de 5 e 3, respectivamente. Admitindo que a distribuição diária das vendas dessa peça siga uma distribuição Normal, qual o intervalo de confiança para a diferença de médias das vendas nas duas filiais com um nível de confiança de 95%? Dados do problema: \\(\\stackrel{-}{X}=40\\) e \\(\\stackrel{-}{Y}=30\\) são as médias amostrais (vendas médias diárias nas filiais A e B, respectivamente); \\(\\sigma_{1}^{2}=25\\) e \\(\\sigma_{2}^{2}=9\\) são as variâncias populacionais; \\(n_{1} = n_{2}=20\\) são os tamanhos das amostras; e, valor extraído da tabela \\(z=1,96\\) correspondente ao nível de confiança estipulado \\((1-\\alpha)=95\\%\\). \\[\\begin{align*} P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ) ] &amp; =(1-\\alpha) \\\\ P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({z}_{(1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} } ) ] &amp; = (1-\\alpha) \\\\ P[10 - ( 1,96 \\cdot \\sqrt{\\frac{25}{20} + \\frac{9}{20}} ) \\le (\\mu_{1}-\\mu_{2}) \\le ( 10 + ( 1,96 \\cdot \\sqrt{\\frac{25}{20} + \\frac{9}{20} } ) ] &amp; = 0,95 \\\\ P[10 - (1,96 \\times 1,3038) \\le (\\mu_{1}-\\mu_{2}) \\le 10 + (1,96 \\times 1,3038) ] &amp; = 0,95 \\end{align*}\\]   \\[ IC (\\mu_{1} - \\mu_{2})_{0,95} = [7; 13] \\] Se quisermos ser rigorosos na interpretação do intervalo de confiança calculado podemos explicar que, se extrairmos um grande número de amostras dessas mesmas dimensões das vendas dessa peça nas duas empresas, e para cada uma delas calcularmos suas médias e as diferenças entre elas, e calcularmos os intervalos de confiança como o acima definido, a proporção desses intervalos onde podemos encontrar a diferença das médias de vendas dessa peça da filial A para a filial B será de 0,95 (95 intervalos em 100). De uma forma mais sintética podemos afirmar que, o anterior intervalo aleatório [7 ; 13], é um intervalo de confiança a 95% para a diferença das médias de vendas dessa peça nas duas empresa De uma forma mais corrente, embora menos correta em termos teóricos, é usual afirmar que, com 95% de confiança a diferença das médias de vendas dessa peça da filial A para a filial B se situa entre os valores 7 e 13. Uma segunda observação se faz pertinente e se refere à natureza dos dados analisados e a forma de apresentação do resultado. Por serem dados discretos, o intervalo de confiança deverá ser apresentado em igual forma, sem ultrapassar os limites estabelecidos. Isto posto: \\(IC (\\mu_{1} - \\mu_{2})_{0,95} = [7; 13]\\) . 9.4.2 Intervalos de confiança para a diferença entre duas médias amostrais com variâncias populacionais desconhecidas mas admitidas iguais Se \\((X_{1}, X_{2},...,X{n_{1}})\\) e \\((Y_{1}, Y_{2},...,Y{n_{2}})\\) forem amostras aleatórias simples das populações \\(X\\) e \\(Y\\) com médias \\(\\mu_{1}\\) e \\(\\mu_{2}\\), e variâncias \\(\\sigma_{1}^{2}\\) e \\(\\sigma_{2}^{2}\\) desconhecidas porém iguais (\\(\\sigma_{1}^{2}=\\sigma_{2}^{2}=\\sigma^{2}\\)), e \\(\\stackrel{-}{X}=\\frac{(X_{1}+X_{2}+...+X{n_{1}})}{n}\\) e \\(\\stackrel{-}{Y}=\\frac{(Y_{1}+Y_{2}+...+Y{n_{2}})}{n_{2}}\\), então:   \\[\\begin{align*} {X} &amp; \\sim N( \\mu_{1} , \\frac{\\sigma}{\\sqrt{n_{1}}} )\\\\ {Y} &amp; \\sim N( \\mu_{2} , \\frac{\\sigma}{\\sqrt{n_{2}}} ) \\end{align*}\\] Demonstra-se que a estatística \\(T\\) pode ser assim definida, bem como sua correspondente distribuição (cf. Figura \\(\\ref{fig62}\\)): \\[ T = \\frac{ (\\stackrel{-}{X}-\\stackrel{-}{Y}) - (\\mu_{1}-\\mu_{2})}{S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } } \\sim t(n_{1}+n_{2}-2) \\] em que: \\(\\stackrel{-}{X}\\)e \\(\\stackrel{-}{Y}\\) são as médias amostrais; \\(S_{1}^{2}\\) e \\(S_{2}^{2}\\) são as variâncias amostrais; \\(\\mu_{1}\\) e \\(\\mu_{2}\\) são as médias populacionais; \\(S_{p}\\) é um desvio padrão amostral ponderado para as duas amostras; \\(n_{1}\\) e \\(n_{2}\\) são os tamanhos das amostras; O desvio padrão ponderado \\(S_{p}\\) é dado por: \\[ S_{p} = \\sqrt{\\frac{(n_{1}-1)\\cdot S^{2}_{1} + (n_{2}-1)\\cdot S^{2}_{2}}{n_{1}+n_{2}-2}} \\] alfa=0.05 prob_desejada1=alfa/2 df=20 t_desejado1=round(qt(prob_desejada1,df ),4) d_desejada1=dt(t_desejado1,df) prob_desejada2=1-alfa/2 df=20 t_desejado2=round(qt(prob_desejada2, df),4) d_desejada2=dt(t_desejado2,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``t&#39;&#39; da distribuição de Student&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição t (df=20)&quot;, subtitle = &quot;P(-t; t)=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; -t)= P(t; \\U221e)= \\u03b1/2 em vermelho &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = t_desejado2, y = 0, xend = t_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-0.1, y=d_desejada1, label=&quot;-t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.3, y=d_desejada2, label=&quot;t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado1-1.8, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+1.3, y=0.2, label=&quot;Intervalo fechado \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.21: Regiões críticas, aquém e além das quais, a probabilidade associada aos valores da estatística \\(T\\) (\\((n-1)\\) graus de liberdade) é inferior a \\(\\frac{\\alpha}{2}\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 9.21 observa-se:   o nível de significância \\(\\alpha\\); o nível de confiança \\((1-\\alpha)\\); e, o valor tabelado da estatística \\(T(t)\\) sob \\((n_{1}+n_{2}-2)\\) graus de liberdade para o nível de confiança fixado.   Assim,   \\[\\begin{align*} P\\left[-{T}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})}\\le T \\le {T}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P\\left[-{t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})}\\le \\frac{ (\\stackrel{-}{x}-\\stackrel{-}{y}) - (\\mu_{1}-\\mu_{2})}{S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } } \\le {t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})}\\right] &amp; =(1-\\alpha) \\\\ P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})} \\cdot S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})} \\cdot S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ) ] &amp; =(1-\\alpha) \\end{align*}\\] \\[ IC(\\mu_{1}-\\mu_{2})_{(1-\\alpha)}=[ (\\stackrel{-}{x}-\\stackrel{-}{y} ) \\pm {t}_{c(n_{1}+n_{2}-2)} \\cdot S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ] \\] Exemplo: De uma grande turma extraiu-se uma pequena amostra de quatro notas de uma prova: 64, 66, 89, 77. De uma outra turma, extraiu-se uma outra amostra, independente, de três notas: 56, 71, 53. Se for razoável admitir que as variâncias das duas turmas (\\(\\sigma^{2}_{1}\\) e \\(\\sigma^{2}_{2}\\)) sejam iguais, qual seria o intervalo de confiança para a diferença observada entre essas médias, a um nível de confiança de 95%? Dados do problema:   \\(\\stackrel{-}{X}=74\\) e \\(\\stackrel{-}{Y}=60\\) são as médias calculadas sobre as duas amostras (notas nas turmas); \\(S_{1}^{2}=132,67\\) e \\(S_{2}^{2}=93\\) são as variâncias calculadas sobre as duas amostras; \\(n_{1} = 4\\) e \\(n_{2}=3\\) são os tamanhos das amostras; \\(n_{1}+ n_{2}-2=5\\) são os graus de liberdade; e, \\(t=2,57\\) o valor tabelado da estatística para um nível de significância \\(\\alpha=5\\%\\) e graus de liberdade \\(gl=5\\). \\[\\begin{align*} P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})} \\cdot S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})} \\cdot S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ) ]=(1-\\alpha) \\end{align*}\\] O desvio padrão ponderado \\(S_{p}\\) é dado por: \\[\\begin{align*} S_{p} &amp; = \\sqrt{\\frac{(n_{1}-1)\\cdot S^{2}_{1} + (n_{2}-1)\\cdot S^{2}_{2}}{n_{1}+n_{2}-2}} \\\\ S_{p} &amp; = \\sqrt{\\frac{( 4-1)\\cdot 132,67 + ( 3 -1)\\cdot 93 }{4 + 3 - 2}} \\\\ S_{p} &amp; = 10,81 \\end{align*}\\] \\[\\begin{align*} P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})} \\cdot S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({t}_{(n_{1}+n_{2}-2, 1-\\frac{\\alpha }{2})} \\cdot S_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ) ] &amp; = (1-\\alpha) \\\\ P[ 14 - ( 2,57 \\cdot 10,81 \\cdot \\sqrt{\\frac{1}{4} + \\frac{1}{3} } ) \\le (\\mu_{1}-\\mu_{2}) \\le 14 +( 2,57 \\cdot 10,81 \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}} } ) ] &amp; = 0,95 \\\\ P[ 14 - 21,23 \\le (\\mu_{1}-\\mu_{2}) \\le 14 + 21,23 ] &amp; =0,95 \\end{align*}\\] \\[ IC (\\mu_{1} - \\mu_{2})_{0,95} = [-7,23; 35,23 ] \\] Se quisermos ser rigorosos na interpretação do intervalo de confiança calculado podemos explicar que, se extrairmos um grande número de amostras dessas mesmas dimensões das vendas dessa peça nas duas empresas, e para cada uma delas calcularmos suas médias e as diferenças entre elas, e calcularmos os intervalos de confiança como o acima definido, a proporção desses intervalos onde podemos encontrar a diferença das médias de vendas dessa peça da filial A para a filial B será de 0,95 (95 intervalos em 100). De uma forma mais sintética podemos afirmar que o intervalo aleatório [-7,23; 35,23], é um intervalo de confiança a 95% para a diferença das médias das notas dessas provas nas duas turmas. De uma forma mais corrente, embora menos correta em termos teóricos, é usual afirmar que, com 95% de confiança a diferença das médias das notas da primeira turma para a segunda turma se situa entre os valores -7,23 e 35,23. Uma importante conclusão pode ser extraída ao se analisar um pouco mais atentamente o intervalo calculado [-7,23 ; 35,23]. Vê-se que encontra-se dentro desse intervalo o valor 0 indicando que a diferença entre as médias amopstrais pode ser zero sob esse nível de confiança, o que equivale dizer que sob esse nível de confiança não se pode afirmar existir diferença significativa (i.e. sob o nível de significância) entre as médias das notas dessas duas turmas. 9.4.3 Intervalos de confiança para a diferença entre duas médias amostrais com variâncias populacionais desconhecidas e desiguais Se \\((X_{1}, X_{2},...,X{n_{1}})\\) e \\((Y_{1}, Y_{2},...,Y{n_{2}})\\) forem amostras aleatórias simples das populações \\(X\\) e \\(Y\\) com médias \\(\\mu_{1}\\) e \\(\\mu_{2}\\), e variâncias \\(\\sigma_{1}^{2}\\) e \\(\\sigma_{2}^{2}\\) desconhecidas porém iguais (\\(\\sigma_{1}^{2}=\\sigma_{2}^{2}=\\sigma^{2}\\)), e \\(\\stackrel{-}{X}=\\frac{(X_{1}+X_{2}+...+X{n_{1}})}{n}\\) e \\(\\stackrel{-}{Y}=\\frac{(Y_{1}+Y_{2}+...+Y{n_{2}})}{n_{2}}\\), então:   \\[\\begin{align*} {X} &amp; \\sim N( \\mu_{1} , \\frac{\\sigma}{\\sqrt{n_{1}}} ) \\\\ {Y} &amp; \\sim N( \\mu_{2} , \\frac{\\sigma}{\\sqrt{n_{2}}} ) \\end{align*}\\] Demonstra-se que a estatística \\(T\\) pode ser assim definida, bem como sua correspondente distribuição (cf. Figura \\(\\ref{fig63}\\)):   \\[ T = \\frac{ (\\stackrel{-}{X}-\\stackrel{-}{Y}) - (\\mu_{1}-\\mu_{2})}{ \\sqrt{\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}}}} \\sim t_{\\nu} \\]   em que:   \\(\\stackrel{-}{X}\\)e \\(\\stackrel{-}{Y}\\) são as médias das amostras extraídas; \\(\\mu_{1}\\) e \\(\\mu_{2}\\) são as médias populacionais; \\(n_{1}\\) e \\(n_{2}\\) são os tamanhos das amostras; e, \\(S_{1}^{2}\\) e \\(S_{2}^{2}\\) são as variâncias das amostras. O número de graus de liberdade (\\(\\nu\\)) é dado por:   \\[ \\nu = \\frac{ (\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}})^{2} } { \\frac{(\\frac{S^{2}_{1}}{n_{1}})^{2}}{n_{1}-1} + \\frac{(\\frac{S^{2}_{2}}{n_{2}})^{2}}{n_{2}-1} } \\] alfa=0.05 prob_desejada1=alfa/2 df=20 t_desejado1=round(qt(prob_desejada1,df ),4) d_desejada1=dt(t_desejado1,df) prob_desejada2=1-alfa/2 df=20 t_desejado2=round(qt(prob_desejada2, df),4) d_desejada2=dt(t_desejado2,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores ``t&#39;&#39; da distribuição de Student&quot;) + labs(title= &quot;Curva da função densidade \\nDistribuição t (df=20)&quot;, subtitle = &quot;P(-t; t)=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; -t)= P(t; \\U221e)= \\u03b1/2 em vermelho &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = t_desejado2, y = 0, xend = t_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-0.1, y=d_desejada1, label=&quot;-t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.3, y=d_desejada2, label=&quot;t&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=6)+ annotate(geom=&quot;text&quot;, x=t_desejado1-1.8, y=0.1, label=&quot;Intervalo aberto à esq. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.5, y=0.1, label=&quot;Intervalo aberto à dir. \\n(probabilidade=\\u03b1/2)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+1.3, y=0.2, label=&quot;Intervalo fechado \\n(probabilidade= (1-\\u03b1))&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 9.22: Regiões críticas, aquém e além das quais, a probabilidade associada aos valores da estatística \\(T\\) (com \\(\\nu\\) graus de liberdade) é inferior a \\(\\frac{\\alpha}{2}\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 9.22 observa-se:   o nível de significância \\(\\alpha\\); o nível de confiança \\((1-\\alpha)\\); e, o valor tabelado da estatística \\(T(t)\\) sob \\(\\nu\\) graus de liberdade para o nível de confiança fixado. Assim, \\[\\begin{align*} P\\left[-{T}_{(\\nu, 1-\\frac{\\alpha }{2})}\\le T \\le {T}_{( \\nu, 1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P\\left[-{t}_{( \\nu, 1-\\frac{\\alpha }{2})}\\le \\frac{ (\\stackrel{-}{x}-\\stackrel{-}{y}) - (\\mu_{1}-\\mu_{2})}{\\sqrt{\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}} } } \\le {t}_{( \\nu, 1-\\frac{\\alpha }{2})}\\right] &amp; = (1-\\alpha) \\\\ P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({t}_{( \\nu, 1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({t}_{( \\nu, 1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}} } ) ] &amp; = (1-\\alpha) \\end{align*}\\] \\[ IC(\\mu_{1}-\\mu_{2})_{(1-\\alpha)} = [(\\stackrel{-}{x}-\\stackrel{-}{y} ) \\pm {t}_{c (\\nu)} \\cdot \\sqrt{\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}} } ] \\] Exemplo: De uma pequena classe do curso de ensino médio tomou-se uma amostra de 4 provas de matemática, obtendo-se um valor médio de 81 sob uma variância de 2. Outra amostra, de 6 provas de biologia, forneceu um valor médio de 77 sob uma variância de 14,4. Qual seria o intervalo de confiança para a diferença observada entre essas médias, sob um nível de confiança de 95%? Dados do problema:   Dados do problema:   \\(\\stackrel{-}{X}=81\\) e \\(\\stackrel{-}{Y}=77\\) são as médias calculadas sobre as duas amostras (notas nas turmas); \\(S_{1}^{2}=2\\) e \\(S_{2}^{2}=14,40\\) são as variâncias calculadas sobre as duas amostras;e, \\(n_{1} = 4\\) e \\(n_{2}=6\\) são os tamanhos das amostras. O número de graus de liberdade (\\(\\nu\\)) é dado por: \\[\\begin{align*} \\nu &amp; = \\frac{ (\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}})^{2} } { \\frac{(\\frac{S^{2}_{1}}{n_{1}})^{2}}{n_{1}-1} + \\frac{(\\frac{S^{2}_{2}}{n_{2}})^{2}}{n_{2}-1} } \\\\ \\nu &amp; = \\frac{ (\\frac{2}{4} + \\frac{14,40}{6})^{2}}{\\frac{(\\frac{2}{4})^{2}}{4-1} + \\frac{(\\frac{14,40}{6})^{2}}{6-1} } \\\\ \\nu &amp; = \\frac{ 2,90^{2}}{0,083 + 1,152} \\\\ \\nu &amp; = \\frac{ 8,41}{1,23} = 6,83 \\sim 7 \\\\ \\end{align*}\\] Portanto, \\(t=2,36\\) é o valor tabelado da estatística para um nível de significância \\(\\alpha=5\\%\\) e graus de liberdade \\(gl=7\\). \\[\\begin{align*} P[(\\stackrel{-}{x}-\\stackrel{-}{y} ) - ({t}_{( \\nu, 1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}} } ) \\le (\\mu_{1}-\\mu_{2}) \\le (\\stackrel{-}{x}-\\stackrel{-}{y}) +({t}_{( \\nu, 1-\\frac{\\alpha }{2})} \\cdot \\sqrt{\\frac{S^{2}_{1}}{n_{1}} + \\frac{S^{2}_{2}}{n_{2}} } ) ] &amp; = (1-\\alpha) \\\\ P[ 4 - ( 2,36 \\cdot \\sqrt{\\frac{ 2}{4} + \\frac{14,40}{6}} ) \\le (\\mu_{1}-\\mu_{2}) \\le 4 +( 2,36 \\cdot \\sqrt{\\frac{ 2}{4} + \\frac{14,40}{6}} ) ] &amp; = 0,95 \\\\ P[ 4 - ( 2,36 \\cdot 1,70 ) \\le (\\mu_{1}-\\mu_{2}) \\le 4 +( 2,36 \\cdot 1,70 ) ] &amp; = 0,95 \\\\ P[ 4 - 4,01 \\le (\\mu_{1}-\\mu_{2}) \\le 4 + 4,01 ) ] &amp; = 0,95 \\\\ \\end{align*}\\] \\(IC (\\mu_{1} - \\mu_{2})_{0,95} = [-0,01 ; 8,01]\\) Se quisermos ser rigorosos na interpretação do intervalo de confiança calculado podemos explicar que, se extrairmos um grande número de amostras dessas mesmas dimensões das notas dessas provas nas duas turmas, e para cada uma delas calcularmos suas médias e as diferenças entre elas, e calcularmos os intervalos de confiança como o acima definido, a proporção desses intervalos onde podemos encontrar a diferença das notas notas da prova de matemática para a prova de biologia será de 0,95 (95 intervalos em 100). De uma forma mais sintética podemos afirmar que, o anterior intervalo aleatório [-0,01; 8,01], é um intervalo de confiança a 95% para a diferença das médias das notas dessas provas nas duas turmas. De uma forma mais corrente, embora menos correta em termos teóricos, é usual afirmar que, com 95% de confiança a diferença das médias das notas da prova de matemática para a prova de biologia situa entre os valores -0,01 e 8,01. Uma importante conclusão pode ser extraída ao se analisar um pouco mais atentamente o intervalo calculado [-0,01 ; 8,01]. Vê-se que encontra-se dentro desse intervalo o valor 0 indicando que a diferença entre as médias amopstrais pode ser zero sob esse nível de confiança, o que equivale dizer que sob esse nível de confiança não se pode afirmar existir diferença significativa (i.e. sob o nível de significância) entre as médias dessas notas. "],["distribuição-das-diferenças-de-médias-amostrais-dependentes-e-seus-intervalos-de-confiança.html", "9.5 Distribuição das diferenças de médias amostrais dependentes e seus intervalos de confiança", " 9.5 Distribuição das diferenças de médias amostrais dependentes e seus intervalos de confiança Na prática temos algumas situações onde as amostras não são independentes com, por exemplo, em situações quando são extraídas de uma mesma população em dois momentos distintos (antes e depois de algum fato), ou como numa situação de comparação inter laboratorial, onde dois laboratórios medem a mesma peça, as medidas entre os laboratórios não são independentes. Nestes casos diz-se que os dados são pareados. Considere duas amostras dependentes \\((X_{1}, \\dots X_{n})\\) e \\((Y_{1}, \\dots Y_{n})\\). O pareamento das observações será considerado tomando-se \\((X_{1}, Y_{1}), \\dots, (X_{n}, Y_{n})\\) e as diferenças serão tomadas a cada par \\(D_{i}=X_{i} - Y_{i}\\), para \\(i=1, \\dots, n\\). Assim obtemos uma amostra \\((D_{1}, \\dots, D_{n})\\), resultante das diferenças entre os valores de cada par. A variável aleatória será admitida tal que \\[ D \\sim N (\\mu_{D}, \\sigma^{2}_{D}) \\] O parâmetro da média dessa distribuição (\\(\\mu_{D}\\)) será estimado a partir da própria amostra das diferenças, tal que: \\[ \\mu_{D}=\\stackrel{-}{D}=\\sum_{i=1}^{n}D_{i} \\] e a variância populacional desconhecida será aproximada por: \\[ S^{2}_{D}=\\sum_{i=1}^{n}\\frac{(D{i}-\\stackrel{-}{D})^{2}}{n-1} \\] Demonstra-se que a estatística \\(T\\) pode ser assim definida, bem como sua correspondente distribuição   \\[ T = \\frac{\\stackrel{-}{D} -\\mu_{D}}{\\frac{S_{D}}{\\sqrt{n}}} \\sim t_{(n-1)} \\] Assim, \\[ IC(\\mu_{D})_{(1-\\alpha)} = [\\stackrel{-}{D} \\pm {t}_{c (n-1)} \\cdot \\sqrt{\\frac{S_{D}^{2}}{n} } ] \\] Exemplo: Determinar o intervalo de confiança sob um nível de confiança de 95% para a diferença de médias do resultados dos testes de um grupo de 15 alunos submetidos a um vídeo instrutivo tais que a primeira amostra foi tomada antes de assistirem ao vídeo e a segunda depois, mediante a aplicação de um novo teste, similar ao primeiro. Aluno Primeira nota (X) Segunda nota (Y) 1 74 80 2 64 74 3 79 83 4 90 92 5 89 96 6 94 98 7 55 59 8 75 77 9 88 93 10 66 78 11 70 75 12 60 59 13 59 61 14 67 70 15 69 74 \\[ \\stackrel{-}{D}=\\sum_{i=1}^{n}D_{i}=-4,667 \\] \\[\\begin{align*} S^{2}_{D} &amp; =\\sum_{i=1}^{n}\\frac{(D{i}-\\stackrel{-}{D})^{2}}{n-1}=10,52354 \\end{align*}\\] Sendo o valor crítico tabelado da estatística para um nível de significância \\(\\alpha=5\\%\\) e graus de liberdade \\(gl=(n-1)=14\\) igua a 1,761, o intervalo de confiança será: \\[\\begin{align*} IC(\\mu_{D})_{(1-\\alpha)} &amp; = [\\stackrel{-}{D} \\pm {t}_{c (n-1)} \\cdot \\sqrt{\\frac{S_{D}^{2}}{n} } ]\\\\ IC(\\mu_{D})_{(1-\\alpha)} &amp; = [-4,667 \\pm 1,761 \\cdot \\sqrt{\\frac{10,52354}{15} } ]\\\\ IC(\\mu_{D})_{(1-\\alpha)} &amp; = [-5,396; -3,937] \\end{align*}\\] Sendo negativos os valores desse intervalo de confinaça deduz-se que a primeira nota é menor que a segunda nota (\\(X-Y &lt; 0\\)) e assim, o vídeo que os alunos assistiram melhorou sua compreensão do assunto e seu desempenho no segundo teste (similar ao primeiro). Caso o valor “zero” estivesse contemplado nesse intervalo, a interpretação seria de que não há diferença estatisticamente significativa nas notas dos alunos nos dois testes (o vídeo não os ajudou em coisa alguma). "],["introdução-à-distribuição-das-proporções-amostrais-e-seus-intervalos-de-confiança.html", "Capítulo 10 Introdução à distribuição das proporções amostrais e seus intervalos de confiança", " Capítulo 10 Introdução à distribuição das proporções amostrais e seus intervalos de confiança A finalidade de uma amostra reside em obter uma estimativa do valor de um ou mais parâmetros associados a uma população. Verifica-se que, ao se extrair repetidamente valores amostrais de forma aleatória da mesma população, estes variam de uma amostra para outra, assim como em relação ao verdadeiro parâmetro dessa população. No entanto, é possível demonstrar que essa variabilidade pode ser caracterizada por meio de distribuições de probabilidade. Quando utilizadas com esse propósito, essas distribuições de probabilidade são chamadas de distribuições amostrais. Elas permitem avaliar, para cada amostra, quão próximo está o valor da estatística amostral em relação ao verdadeiro parâmetro da população. A resposta a essa questão depende essencialmente de três fatores: A estatística específica que está sendo empregada: diferentes estatísticas demandam diferentes distribuições de probabilidade para modelar sua variabilidade. O tamanho da amostra, que exerce uma influência inversa na variabilidade entre os valores amostrais. A variabilidade intrínseca da população em estudo e do processo de amostragem. "],["conceito-elementar-de-uma-proporção.html", "10.1 Conceito elementar de uma proporção", " 10.1 Conceito elementar de uma proporção O conceito básico de proporção remete à razão entre duas grandezas. Vejam os exemplos: segundo dados demográficos de 2012 (IBGE), a cidade de Recife possui proporcionalmente mais mulheres que homens; em 18 dias de campanha, somente 25,09% do público-alvo se vacinou contra gripe no País, segundo dados divulgados pelo Ministério da Saúde. De 17 de abril, quando a imunização foi iniciada, até 5 de maio, 13,6 milhões de brasileiros procuraram os postos de saúde para se vacinar. Na primeira afirmação, a ideia de proporcionalidade advém do quociente do número habitantes do sexo feminino pelo numero total de habitantes naquele ano (\\(\\frac{827.885}{1.537.704}=0,5384\\)). Já na segunda, a afirmação resulta do quociente do número de brasileiros vacinados pelo total da população-alvo (\\(\\frac{13.600.000}{54.200.000}=0,2509\\)). "],["distribuição-das-proporções-amostrais.html", "10.2 Distribuição das proporções amostrais", " 10.2 Distribuição das proporções amostrais Figure 10.1: Ilustração de \\(m\\) amostras de mesmo tamanho (\\(n\\)) extraídas de uma mesma população onde a característica de interesse se manifesta sob uma proporção populacional \\(\\pi\\) Para estudarmos a distribuição das proporções amostrais (\\(\\hat{p}\\)) considerem uma população apresentando uma determinada característica de interesse com proporção \\(\\pi\\). Essa característica de interesse assume apenas duas possibilidades em cada elemento da população: ela pode ou não estar presente: \\[ X_{i}= \\begin{cases} 1, \\text{ se o i-ésimo elemento é portador da característica}\\\\ 0, \\text{ se o i-ésimo elemento não é portador da característica}\\\\ \\end{cases} \\] Assim, ao se escolher ao acaso um elemento da população, a probabilidade dessa característica estar presente pode ser estimada seguindo o modelo teórico de uma variável de Bernoulli e assim \\(X_{i} \\sim Ber(\\pi)\\) e, como tal, \\(E(X)=\\pi\\) e \\(Var(X)=\\pi(1-\\pi)\\). Repetindo-se essa ``extração’’ por \\(n\\) vezes podemos definir a variável aleatória \\(Y_{n}\\) como sendo o número de sucessos observados em \\(n\\) repetições de Bernoulli: \\[ Y_{n}= X_{1} + \\dots + X_{n} \\] e assim, \\(Y_{n} \\sim Bin(n, \\pi)\\) e a proporção amostral observada de sucessos ao final das \\(n\\) repetições será a média: \\[ \\hat{p}=\\frac{Y}{n}=\\frac{X_{1} + \\dots + X_{n}}{n} \\]. em que \\(\\hat{p}\\) é uma estimativa amostral da proporção populacional \\(\\pi\\). Demonstra-se que para: um razoável número de repetições: \\(n \\ge 30\\); de uma população onde a proporção \\(\\pi\\) não é extrema: próximas a 0 ou 1; e tal que \\((n \\cdot \\pi)\\) e \\((n \\cdot (1-\\pi))\\) sejam maiores que 15 (alguns autores consideram limites mais brandos, iguais a 10 ou ainda a 5), ao se repetir o experimento anotando-se as proporções amostrais \\(\\hat{p}\\) obtida em cada uma das \\(n\\) repetições de Bernoulli , o perfil da curva de distribuição dessas proporções amostrais torna-se razoavelmente simétrico à medida que o número \\(n\\) de repetições de Bernoulli cresce, para qualquer que seja a proporção populacional, e oscila em torno de \\(\\pi\\). Pelo Teorema de DeMoivre e Laplace (anteriores ao Teorema do Limite Central), demonstra-se que, para um grande número de repetições (\\(n\\)), o valor esperado e a variância das proporções amostrais são: \\[\\begin{align*} E(Y) &amp; =n \\cdot \\pi \\\\ Var(Y) &amp; =n \\cdot \\pi \\cdot (1-\\pi) \\end{align*}\\] e a distribuição das proporções amostrais será aproximadamente Normal com parâmetros \\(\\mu=n.\\pi\\) e \\(\\sigma^{2}=n.\\pi.(1-\\pi)\\): \\[ Y \\sim N \\left( n\\cdot\\pi ; n\\cdot\\pi\\cdot(1-\\pi) \\right) \\] Uma vez que a proporção amostral está definida como: \\(\\hat{p} = \\frac{Y_{n}}{n}\\) segue-se que o valor esperado \\(\\hat{p}=\\mu\\): \\[\\begin{align*} E(\\hat{p}) &amp; = E(\\frac{Y}{n}) \\\\ &amp; = \\frac{1}{n} \\cdot E(Y) \\\\ &amp; = \\frac{1}{n} \\cdot n \\cdot \\pi \\\\ &amp; = \\pi \\end{align*}\\] e a variância \\(Var(\\hat{p}=\\frac{1}{n}.\\pi.(1-\\pi)\\)): \\[\\begin{align*} Var(\\hat{p})&amp; = Var(\\frac{Y}{n}) \\\\ &amp; = \\frac{1}{n^{2}} \\cdot Var(Y)\\\\ &amp; = \\frac{1}{n^{2}} \\cdot n \\cdot \\pi \\cdot (1-\\pi)\\\\ &amp; = \\frac{1}{n} \\cdot \\pi \\cdot (1-\\pi ) \\end{align*}\\] Assim, as proporções amostrais se distribuem de modo aproximadamente Normal sob uma média \\(\\mu=\\pi\\) e com uma variância \\(\\sigma^{2}=\\frac{\\pi \\cdot (1- \\pi)}{n}\\): \\[ \\hat{p} \\sim N \\left(\\pi ; \\frac{\\pi \\cdot (1- \\pi) }{n} \\right) \\] 10.2.1 Simulações ilustrativas da aproximação da distribuição das proporções amostrais pela distribuição Normal Para exemplificar considere o lançamento de um dado de seis faces,. A probabilidade de que uma certa face caia voltada para cima é de \\(\\frac{1}{6}=0,167\\). Se lançarmos esse dado um número crescente de vezes e anotarmos a proporção delas em que a face escolhida caiu voltada para cima comprova-se que o valor esperado das proporções amostrais aproxima-se da proporção populacional. As Figuras 10.2 (tamanho de cada amostra \\(n=n_1\\)) e 10.3 (tamanho de cada amostra \\(n=n_2\\)) mostram o perfil assumido pela distribuição de 100 proporções amostrais obtidas de uma população que apresenta uma proporção \\(\\pi=p_1\\) da característica de interesse. ############################################################################# # Considere uma população cuja característica de interesse (A) se manifesta de modo dicotômico: # sim/não, sob uma probabilidade p_1 e (1-p_1). # A probabilidade de se obter um elemento com a característica de interesse # - ao se sortear aleatoriamente um indivíduo qualquer - pode ser modelada como uma variável de Bernoulli. # A probabilidade de se observar a característica de interesse ao se # repetir a amostragem (com reposição) por n_1 (n_2) vezes pode ser modelada como uma variável binomial (repetição de um experimento de Bernoulli n_1/n_2 vezes) # Repetindo-se esses experimentos binomiais por N vezes, as proporções amostrais de # elementos com a característica de interesse (sucesso) nas N amostras obtidas será # dada pelo número de elementos de cada conjunto nas n_1 (n_2) repetições dividido por # n_1 (n_2). # Desse modo, obtemos N proporções de amostras de tamanho n_1 (n_2) # # # Selecionando-se aleatoriamente um elemento desta população # resulta em uma variável aleatória dicotômica/Bernoulli que assume # o valor 1 caso o elemento selecionado possua a propriedade A (sucesso) # e assume o valor 0 caso não possua a propriedade A. # # A retirada (com reposição) de `n_1` elementos dessa população poderemos observar a frequência absoluta com que a propriedade A (sucesso) se manifesta na amostra, # a qual pode ser expressa como uma variável aleatória (X) que segue o modelo teórico Binomial de probabilidade. # # A frequência relativa, o quociente entre o número de sucessos por `n_1` expressa a # proporção com que a propriedade &quot;A&quot; foi observada na &#39;amostra&#39; de tamanho `n_1` é também uma variável aleatória (p) com distribuição altamente relacionada à variável X pois é a média de `n_1` ensaios (repetições) de Bernoulli. # # Repetindo-se sucessivamente `N` vezes extrações de tamanho `n_1` # a anotando-se a proporção de sucesso em cada uma dessas amostras poderemos analisar como eles se distribuem em relação à quantidade de elementos extraídos `n_1` (repetições de Bernoulli) # e à verdadeira proporção com que a propriedade A se manifesta na população (pi) # # Demonstra-se que: # para `n_1` suficientemente grande (repetições de Bernoulli com reposição);] # n_1 * pi &gt; 5 e # n_1*(1-pi) 5 # a distribuição de p pode ser aproximada pela distribuição Normal # tal que p ~N (mu,sigma) # onde mu e sigma são aproximados por: # mu = E(p) = pi # sigma^2 = sigma^2*p &gt;&gt;&gt;&gt; sigma = sqrt[ p*(1-p)/(n_1) ] # ############################################################################# # Proporção escolhida para a manifestação da característica: sim/não (probabilidade de cada evento de Bernoulli) p_1=round(1/6,2) # Número de amostras N=100 # Tamanho escolhido para cada amostra: repetições de Bernoulli n_1=10 # Vetor com o número de sucessos observados (a frequência absoluta) nas N amostras de n_1 elementos dicotômicos (repetições de Bernoulli, sob uma probabilidade individual de sucesso igual a p_1) suc_10rep=rbinom(n=N, size = n_1, prob = p_1) suc_10rep # Vendo a proporção de sucessos (a frequência relativa) em cada uma das N_1 amostras de n_1 elementos dicotômicos prop_10rep=suc_10rep/n_1 mean(prop_10rep) # ~ pi sd(prop_10rep) # ~ sqrt(pi*(1-pi)/n_1) # Dataframe com as N proporções amostrais sob n_1 dados_10=as.data.frame(prop_10rep) ############################################################################# # O mesmo procedimento, mas agora com amostras com um maior número de elementos em cada uma ############################################################################# # Tamanho escolhido para cada amostra: repetições de Bernoulli n_2=100 # Vetor com o número de sucessos observados (a frequência absoluta) nas N amostras de n_2 elementos dicotômicos (repetições de Bernoulli, sob uma probabilidade individual de sucesso igual a p_1) suc_100rep=rbinom(n=N, size = n_2, prob = p_1) suc_100rep # Vendo a proporção de sucessos (a frequência relativa) em cada uma das N_1 amostras de n_1 elementos dicotômicos prop_100rep=suc_100rep/n_2 mean(prop_100rep) # ~ pi sd(prop_100rep) # ~ sqrt(pi*(1-pi)/n_2) # Dataframe com as N proporções amostrais sob n_2 dados_100=as.data.frame(prop_100rep) meu_titulo1=paste(&quot;Distribuição de frequência das proporções de sucesso observadas em \\n&quot;,N, &quot;amostras de n=&quot;, n_1, &quot;elementos dicotômicos extraídos (com reposição) da população&quot;,&quot;\\n(proporção de sucesso na população \\u03c0=&quot;, p_1,&quot;)&quot;) meu_titulo2=paste(&quot;As proporções amostrais ~ \\nN(\\u03bc= \\u03c0=&quot;,round(mean(dados_10$prop_10rep),3),&quot;;\\u03c3 =sqrt(\\u03c0*(1- \\u03c0)/n)=&quot;,round(sd(dados_10$prop_10rep),3),&quot;)&quot;) ggplot(dados_10, aes(x = prop_10rep)) + geom_histogram(aes(y =..density..), breaks = seq(0, 0.4, by = 0.05), colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + stat_function(fun = dnorm, args = list(mean = p_1, sd = sqrt(p_1*(1-p_1)/n_1)), colour=&quot;red&quot;) + scale_y_continuous(name=&quot;&quot;,breaks = NULL) + scale_x_continuous(name=&quot;Valores das proporções amostrais&quot;) + #labs(title=meu_titulo1)+ annotate(geom=&quot;text&quot;, x=mean(prop_10rep), y=max(dnorm(prop_10rep)), label=meu_titulo2, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ theme(plot.title = element_text(size = 10, face = &quot;bold&quot;), axis.text.x = element_text(angle=0, hjust=1, size=10), axis.text.y = element_text(angle=0, hjust=1, size=10), axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10)) Figure 10.2: Distribuição das frequências das proporções de sucesso observadas em 100 amostras de tamanho n=10 elementos dicotômicos extraídos (com reposição) de uma população (a proporção de sucesso na população é π=1/6) meu_titulo1=paste(&quot;Distribuição de frequências das proporções de sucesso observadas em \\n&quot;,N, &quot;amostras de n=&quot;, n_2, &quot;elementos dicotômicos extraídos (com reposição) da população&quot;,&quot;\\n(proporção de sucesso na população \\u03c0=&quot;, p_1,&quot;)&quot;) meu_titulo2=paste(&quot;As proporções amostrais ~ \\nN(\\u03bc= \\u03c0=&quot;,round(mean(dados_100$prop_100rep),3),&quot;;\\u03c3=sqrt(\\u03c0*(1- \\u03c0)/n)=&quot;,round(sd(dados_100$prop_100rep),3),&quot;)&quot;) ggplot(dados_100, aes(x = prop_100rep)) + geom_histogram(aes(y =..density..), breaks = seq(0, 0.4, by = 0.03), colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + stat_function(fun = dnorm, args = list(mean = p_1, sd = sqrt(p_1*(1-p_1)/n_2)), colour=&quot;red&quot;) + scale_y_continuous(name=&quot;&quot;,breaks = NULL) + scale_x_continuous(name=&quot;Valores das proporções amostrais&quot;) + #labs(title=meu_titulo1)+ annotate(geom=&quot;text&quot;, x=mean(prop_100rep), y=max(dnorm(prop_100rep)), label=meu_titulo2, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ theme(plot.title = element_text(size = 10, face = &quot;bold&quot;), axis.text.x = element_text(angle=0, hjust=1, size=10), axis.text.y = element_text(angle=0, hjust=1, size=10), axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10)) Figure 10.3: Distribuição das frequências das proporções de sucesso observadas em 100 amostras de tamanho n=100 elementos dicotômicos extraídos (com reposição) de uma população (a proporção de sucesso na população é π=1/6) "],["pobabilidades-associadas-à-observação-de-uma-proporção-amostral-hatp.html", "10.3 Pobabilidades associadas à observação de uma proporção amostral \\(\\hat{p}\\)", " 10.3 Pobabilidades associadas à observação de uma proporção amostral \\(\\hat{p}\\) Ao se definir a estatística \\(Z\\) como a simples padronização da variável \\(\\hat{p}\\) vemos que esta seguirá uma distribuição normal com média \\(0\\) e desvio-padrão \\(1\\): \\[ Z=\\frac{\\hat{p}-\\pi }{\\sqrt{\\frac{\\pi \\left(1-\\pi \\right)}{n}}} \\sim N\\left(0,1\\right) \\] Essa aproximação da distribuição de uma variável binomial (proporções amostrais \\(\\hat{p}\\)) pela distribuição Normal será tanto mais simétrica e com perfil de um sino quanto vier a atender (\\(n\\) grande e \\(\\pi\\) não próximo de 0 ou 1) e nos permite determinar probabilidades associadas a proporções amostrais. Exemplo: um sistema de produção opera de tal maneira que 10% das peças produzidas são defeituosas. Suponha que os itens sejam vendidos em caixas com 100 unidades e calcule as probabilidades de que em uma caixa: - tenha mais do que 10% de defeituosas? - tenha menos do que 15% de defeituosas? Dados do problema: \\(\\pi=0,10\\) e \\(n=100\\). Considerando que a proporção populacional \\(\\pi\\) não é extrema (próxima a 0 ou 1) e \\((n \\cdot \\pi)\\) e \\((n \\cdot (1-\\pi))\\) são maiores que 5, as proporções amostrais \\(\\hat{p}\\) se distribuem, aproximadamente, do modo: \\[ \\hat{p} \\sim N \\left(\\mu: \\pi ; \\sigma^{2}: \\frac{\\pi \\cdot (1- \\pi) }{n} \\right)\\\\ \\hat{p} \\sim N \\left(0,10 ; \\frac{0,10 \\cdot (1- 0,10) }{100} \\right)\\\\ \\hat{p} \\sim N (0,10; 0,0009) \\] Para se calcular as probabilidades de serem observadas proporções amostrais \\(\\hat{p}&gt;0,10\\) e \\(\\hat{p}&lt;0,15\\), basta-se mapear essas proporções amostrais à distribuição Normal padronizada. Assim, denotando-se uma variável aleatória (as proporções amostrais) \\(X \\sim n(\\mu: 0,1; \\sigma^{2}: 0,0009 (\\sigma: 0,03))\\) segue-se: \\[\\begin{align*} P(\\hat{p}&gt; 0,10) &amp; = P(X &gt; 0,10 ) \\\\ &amp; = P\\left(\\frac{X-0,10}{0,03} &gt; \\frac{0,10-0,10}{0,03}\\right ) \\\\ &amp; = P\\left(Z &gt;0 \\right ) \\\\ &amp; = 0,50 \\end{align*}\\] e \\[\\begin{align*} P(\\hat{p} &lt; 0,15) &amp; = P(X &lt; 0,15 ) \\\\ &amp; = P\\left(\\frac{X-0,15}{0,03} &lt; \\frac{0,15-0,10}{0,03}\\right ) \\\\ &amp; = P\\left(Z &lt; 1,67\\right ) \\\\ &amp; = 0,9525 \\end{align*}\\] "],["a-aleatoriedade-das-proporções-amostrais-e-o-tamanho-amostral.html", "10.4 A aleatoriedade das proporções amostrais e o tamanho amostral", " 10.4 A aleatoriedade das proporções amostrais e o tamanho amostral No módulo ``Introdução ao planejamento de pesquisas’’ explicamos que quando não se dispõe de nenhuma informação a priori sobre a proporção populacional (\\(\\pi\\)) a adoção do máximo valor possível ao produto: \\(\\pi.(1-\\pi)=\\frac{1}{4}\\) assegura que o o tamanho de amostra obtido será suficiente para a estimação qualquer que seja a proporção populacional \\(\\pi\\). Trazendo a variável \\(Z\\) antes definida: \\[ Z=\\frac{\\hat{p}-\\pi }{\\sqrt{\\frac{\\pi \\left(1-\\pi \\right)}{n}}} \\sim N\\left(0,1\\right) \\] podemos reescrevê-la de modo a se obter o dimensionamento amostral em função do nível de confiança e um erro máximo estabelecidos: \\[ z_{(1-\\alpha)}=\\frac{\\hat{p}-\\pi }{\\sqrt{\\frac{\\pi \\left(1-\\pi \\right)}{n}}} \\\\ z_{(1-\\alpha)}.\\sqrt{\\frac{\\pi \\left(1-\\pi \\right)}{n}}=\\hat{p}-\\pi \\\\ \\frac{\\pi \\left(1-\\pi \\right)}{n}=(\\frac{\\varepsilon}{z_{(1-\\alpha)}})^{2}\\\\ n = \\frac{z_{(1-\\alpha)}^{2}}{\\varepsilon^{2}} \\cdot \\pi \\left(1-\\pi \\right)\\\\ \\] Deste modo podemos simular a flutuação dos valores das proporções obtidas em sucessivas amostras, ilustrando simultaneamente as proporções amostrais observadas e a proporção das amostras que apresentam um erro amostral (\\(\\varepsilon\\)) superior ao estipulado pelo nível de confiança (\\(1-\\alpha\\)). Desconhecendo-se qualquer informação acerca da proporção populacional (\\(\\pi\\)), a dimensão da amostra pode ser estipulada tomando-se o maior valor do produto \\(\\pi \\left(1-\\pi \\right)\\) como sendo igual a \\(\\frac{1}{4}\\) pois: p &lt;- seq(0, 1, by = 0.01) y &lt;- p * (1 - p) plot(p, y, type = &quot;l&quot;, xlab = &quot;\\u03c0&quot;, ylab = &quot;\\u03c0*(1- \\u03c0)&quot;, main = &quot;Possíveis valores assumidos pelo produto: \\u03c0*(1- \\u03c0)&quot;) Figure 10.4: Possíveis valores assumidos pelo produto: π*(1- π) Assim, a dimensão conservadora para a amostra será dada por: \\[ n = \\frac{z_{(1-\\alpha)}^{2}}{\\varepsilon^{2}} \\cdot \\frac{1}{4}\\\\ \\] 10.4.1 Simulações ilustrativas sobre as flutuações das proporções amostrais e o erro amostral fixado As próximas figuras ilustram a flutuação das proporções amostrais obtidas de amostragens (com reposição) de elementos de uma população que apresentam a característica de interesse se manifestando de modo dicotômico, sob variados tamanhos amostrais (385, 210 e 100). # Flutuação das proporções amostrais observadas flut.N = function (N, n, p, conf, er) { zc = qnorm(1-((1-conf)/2)) suc=rbinom(n=N, size = n, prob = p) prop_suc=suc/n dados=as.data.frame(prop_suc) names=c(&quot;Proporção amostral&quot;) colnames(dados)=names row.names(dados)=NULL meu_titulo01=paste0(&quot;Flutuação das proporções amostrais \\n&quot;, N,&quot; amostras de tamanho &quot;,n,&quot; (dimensionamento sob um nível de confiança (1-\\u03b1)= &quot;,conf,&quot; e um erro amostral \\u03b5= &quot;, er,&quot; \\nAs linhas verticais mostram a propoção populacional em azul (\\u03c0= &quot;, p , &quot;) \\ne os valores limites estabelecidos pelo erro arbitrado em vermelho (\\u03c0 +/-\\u03b5= &quot;, p, &quot;+/-&quot;, er ,&quot;)&quot;) meu_titulo02=paste0(&quot;Os valores das proporçoes amostrais seguem uma distribuição ~ N ( \\u03bc, \\u03c3) = (&quot;, round(mean(dados$`Proporção amostral`),4) ,&quot;, &quot;, round(sqrt(p*(1-p)/n),4) ,&quot;)&quot;) plot(0, 0, type=&quot;n&quot;, xlim=c( 0.5*min(dados$`Proporção amostral`) , 1.1*max(dados$`Proporção amostral`) ), ylim=c(0,N), bty=&quot;l&quot;, xlab=&quot;Proporções amostrais observadas&quot;, ylab=&quot;Amostras extraídas&quot;, main=&quot;&quot;, #meu_titulo01 sub=&quot;&quot;) #meu_titulo02 for (i in 1:N) { prop_amostral=dados$`Proporção amostral`[i] ploty = c(i,i) if (prop_amostral &gt; p+er || prop_amostral &lt; p-er) points(prop_amostral, i, col=&quot;red&quot;, cex=1)+text(y=i+3,x=prop_amostral, labels=round(prop_amostral,2), cex=1, col=&#39;red&#39;) else points(prop_amostral, i, col=&quot;black&quot;, cex=1) segments(x0=p , y0=0, x1=p ,y1=N,col=&quot;blue&quot;, lwd=2, lty=2) segments(x0=p-er , y0=0, x1=p-er ,y1=N,col=&quot;red&quot;, lwd=1, lty=2) segments(x0=p+er , y0=0, x1=p+er ,y1=N,col=&quot;red&quot;, lwd=1, lty=2) } } Figure 10.5: Flutuação das diversas proporções amostrais obtidas de amostragens cujo dimensionamento (385 elementos ) foi estimado ignorando-se o conhecimento da proporção populacional (π) para um nível de confiança (1-α)=0,95 e um erro amostral ε=0,05 (em preto as proporções amostrais dentro da tolerância fixada e, em vermelho, as que aleatoriamente ultrapassam a tolerância fixada em π +/-ε). Figure 10.6: Flutuação das diversas proporções amostrais obtidas de amostragens cujo dimensionamento (217 elementos) foi estimado admitindo-se o conhecimento da proporção populacional (π) para um nível de confiança (1-α)=0,95 e um erro amostral ε=0,05 (em preto as proporções amostrais dentro da tolerância fixada e, em vermelho, as que aleatoriamente ultrapassam a tolerância fixada em π +/-ε). Figure 10.7: Flutuação das diversas proporções amostrais obtidas de amostragens cujo dimensionamento foi arbitrariamente fixado (100 elementos) para um nível de confiança (1-α)=0,95 e um erro amostral ε=0,05 (em preto as proporções amostrais dentro da tolerância fixada e, em vermelho, as que aleatoriamente ultrapassam a tolerância fixada em π +/-ε). "],["intervalos-de-confiança-para-proporções-amostrais.html", "10.5 Intervalos de confiança para proporções amostrais", " 10.5 Intervalos de confiança para proporções amostrais Podemos escrever o parâmetro (\\(\\pi\\)) da proporção populacional em função da proporção amostral observada \\(\\hat{p}\\) e de seu desvio padrão \\(\\sigma_{\\hat{p}}\\): \\[ Z=\\frac{\\hat{p}-\\pi }{\\sqrt{\\frac{\\pi \\left(1-\\pi \\right)}{n}}} \\sim N\\left(0,1\\right), \\] ou \\[ Z=\\frac{\\hat{p}-\\pi }{{\\sigma }_{\\hat{p}}} \\] com \\(Z \\sim N\\left(0,1\\right)\\). Assim, \\[ \\hat{p} - \\pi = Z \\cdot {\\sigma }_{\\hat{p}} \\] e \\[ \\pi = \\hat{p} + Z \\cdot {\\sigma }_{\\hat{p}} \\] Observa-se, todavia, que a variância da distribuição Normal da aproximação da distribuição das proporções amostrais é expressa em termos do parâmetro da proporção populacional \\(\\pi\\) que não é conhecido: \\[ \\hat{p} \\sim N [\\pi ; \\frac{\\pi \\cdot (1- \\pi) }{n} ] \\] \\[ {\\sigma }_{\\hat{p}}=\\sqrt{\\frac{\\pi \\left(1-\\pi \\right)}{n}}. \\] Demonstra-se que para: um razoável número de repetições: \\(n \\ge 30\\); de uma população onde a proporção \\(\\pi\\) não é extrema: próximas a 0 ou 1; e tal que \\((n \\cdot \\pi)\\) e \\((n \\cdot (1-\\pi))\\) sejam maiores que 15 (alguns autores consideram limites mais brandos, iguais a 10 ou ainda a 5), Podemos tomar a proporção amostral \\(\\hat{p}\\) como uma aproximação direta da proporção populacional \\(\\pi\\) na expressão da variância da distribuição Normal que modela a distribuição das proporções amostrais sem que isso resulte em grande alteração na distribuição da variável \\(Z\\). Ou ainda, alternativamente, fazendo-se antes uma aproximação com correção de continuidade, onde definimos uma nova estimativa amostral da proporção populacional \\(\\hat{p}_{c}\\) corrigida: \\[ \\hat{p}_{c} = \\hat{p}+\\frac{1}{2n} \\] se \\(\\hat{p} &lt; 0,50\\), ou \\[ \\hat{p}_{c} = \\hat{p}- \\frac{1}{2n} \\] se \\(\\hat{p} &gt; 0,50\\). As probabilidades associadas aos valores assumidos pela variável \\(Z \\sim N\\left(0,1\\right)\\): a área sob a curva, encontram-se tabelados e podem ser utilizados para construir intervalos de confiança para o parâmetro da proporção populacional \\(\\pi\\) associados a probabilidades desejadas. \\[ P [ \\hat{p} - Z \\cdot {\\sigma }_{\\hat{p}} &lt; \\pi &lt; \\hat{p} + Z \\cdot {\\sigma }_{\\hat{p}} ] = (1-\\alpha) \\] Assim (com \\(\\hat{p}\\) ou \\(\\hat{p}_{c}\\)) podemos construir intervalos de confiança em torno da proporção populacional \\(\\pi\\) associados a um nível de significância estabelecido: Bilaterais: intervalo delimitado por dois valores: mínimo e máximo, para a proporção amostral, dentro do qual todos os valores possuem um mesmo nível de significância: \\[ P[\\hat{p} - {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1- \\hat{p} \\right)}{n}} \\hspace{0.1cm} \\le \\hspace{0.1cm} \\pi \\hspace{0.1cm} \\le \\hspace{0.1cm} \\hat{p} + {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1-\\hat{p} \\right)}{n}}] = (1-\\alpha) \\] Unilaterais: intervalos delimitados apenas em um de seus lados nos quais todos os valores possuem um mesmo nível de significância: Valor máximo (limitando à direita): \\[ P[\\pi \\le \\hat{p} + {z}_{\\alpha} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1- \\hat{p} \\right)}{n}} ] = (1- \\alpha) \\] Valor mínimo (limitando à esquerda): \\[ P [\\pi \\hspace{0.1cm} \\ge \\hat{p} - {z}_{\\alpha} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1- \\hat{p} \\right)}{n}} \\hspace{0.1cm}] = (1-\\alpha) \\] # Intervalos de confiança das proporções amostrais observadas IC.N = function (N, n, p, conf, er) { zc = qnorm(1-((1-conf)/2)) #Z=1,96 suc=rbinom(n=N, size = n, prob = p) prop_suc=suc/n dados=as.data.frame(prop_suc) dados$lim_sup=dados$prop_suc + zc*sqrt(dados$prop_suc*(1-dados$prop_suc)*(1/n)) dados$lim_inf=dados$prop_suc - zc*sqrt(dados$prop_suc*(1-dados$prop_suc)*(1/n)) names=c(&quot;Proporção amostral&quot;, &quot;lim superior&quot;, &quot;lim inferior&quot;) colnames(dados)=names row.names(dados)=NULL meu_titulo001=paste0(&quot;Intervalos com iguais níveis de confiança fixados em &quot;, 100*conf, &quot;% \\n(&quot;,N,&quot; amostras de tamanho &quot;,n,&quot;) \\nAs linhas verticais mostram a propoção populacional em azul (\\u03c0: &quot;, p , &quot;) \\ne a média das proporções amostrais em vermelho ( \\u0070\\u0302: &quot;,round(mean(dados$`Proporção amostral`),4) , &quot;).&quot;) meu_titulo002=paste0(&quot;Parâmetros da distribuição da população Normal aproximada ( \\u03bc, \\u03c3) = (&quot;, round(mean(dados$`Proporção amostral`),4) ,&quot;, &quot;, round(sqrt(p*(1-p)/n),4) ,&quot;)&quot;) plot(0, 0, type=&quot;n&quot;, xlim=c( 0.5*min(dados$`lim inferior`) , 1.1*max(dados$`lim superior`) ), ylim=c(0,N), bty=&quot;l&quot;, xlab=&quot;Proporções amostrais observadas&quot;, ylab=&quot;Amostras extraídas&quot;, main=&quot;&quot;, #meu_titulo001 sub=&quot;&quot;) #meu_titulo002 for (i in 1:N) { prop_amostral=dados$`Proporção amostral`[i] li = dados$`lim inferior`[i] ls = dados$`lim superior`[i] plotx = c(li,ls) ploty = c(i,i) if (li &gt; p | ls &lt; p) lines(plotx,ploty, col=&quot;red&quot;, lwd=2, lend=0) else lines(plotx,ploty, lend=0) if (li &gt; p | ls &lt; p) points(prop_amostral, i, col=&quot;red&quot;, cex=1)+text(y=i+3,x=prop_amostral, labels=round(prop_amostral,1), cex=1, col=&#39;red&#39;) else points(prop_amostral, i, col=&quot;black&quot;, cex=1) segments(x0=mean(dados$`Proporção amostral`) , y0=0, x1=mean(dados$`Proporção amostral`) ,y1=N,col=&quot;red&quot;, lwd=2, lty=2) segments(x0=p , y0=0, x1=p ,y1=N,col=&quot;blue&quot;, lwd=2, lty=1) } } Figure 10.8: Intervalos de confiança construídos para as diversas proporções amostrais obtidas de amostragens (com reposição) de elementos de uma população que apresentam a característica de interesse se manifestando de modo dicotômico. O dimensionamento foi estimado ignorando-se o conhecimento da proporção populacional (π) para um nível de confiança (1-α) e um erro amostral (ε) estipulados: 385 elementos. Exemplo: Em uma amostra aleatória, 136 pessoas de um grupo de 400 que receberam a vacina contra gripe, declararam haver sentido algum efeito colateral. Construa um intervalo com 95% de confiança para a verdadeira proporção populacional da ocorrência de efeitos colaterais vacinais . Dados do problema: \\(\\hat{p}=\\frac{136}{400}=0,34\\) é a proporção amostral observada; o tamanho amostral (\\(n=400\\)) é grande e a proporção amostral (\\(\\hat{p}=0,34\\)) não é extrema (próxima a zero ou um); \\(\\pi\\) é a proporção populacional (desconhecida); e, para o nível de confiança solicitado (\\((1-\\alpha)=0,95\\)) temos da tabela \\({z}_{\\left(\\frac{\\alpha }{2}\\right)}= +/-1,96\\). Um intervalo bilateral (fechado) para a proporção populacional desconhecida (\\(\\pi\\)) sob um nível de confiança (\\(1-\\alpha\\)) de 0,95 estará delimitado: \\[\\begin{align*} \\hat{p} - {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1- \\hat{p} \\right)}{n}} \\le &amp; \\pi \\le \\hat{p} + {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1-\\hat{p} \\right)}{n}}\\\\ 0,34 - 1,96 \\cdot \\sqrt{ \\frac{0,34 \\cdot (1-0,34)}{400} } \\le &amp; \\pi \\le 0,34 + 1,96 \\cdot \\sqrt{ \\frac{0,34 \\cdot (1-0,34)}{n} }\\\\ 0,2936\\le &amp; \\pi \\le 0,3864 \\end{align*}\\] Exemplo: Em uma amostra aleatória de 2000 eleitores do Brasil constatou-se uma intenção de voto de 43% para um candidato à presidência. Realizada a eleição, deseja-se inferir qual o intervalo de variação da proporção populacional a um nível de confiança de 99%. Dados do problema: \\(\\hat{p}=0,43\\) é a proporção amostral observada; o tamanho amostral (\\(n=2000\\)) é grande e a proporção amostral (\\(\\hat{p}=0,43\\)) não é extrema (próxima a zero ou um); \\(\\pi\\) é a proporção populacional (desconhecida); e, para o nível de confiança solicitado (\\((1-\\alpha)=0,99\\)) temos da tabela \\({z}_{\\left(\\frac{\\alpha }{2}\\right)}= +/-2,58\\). Um intervalo bilateral (fechado) para a proporção populacional desconhecida (\\(\\pi\\)) sob um nível de confiança (\\(1-\\alpha\\)) de 0,99 estará delimitado: \\[\\begin{align*} \\hat{p} - {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1- \\hat{p} \\right)}{n}} \\le &amp; \\pi \\le \\hat{p} + {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{\\frac{\\hat{p} \\cdot \\left(1-\\hat{p} \\right)}{n}}\\\\ 0,43 - 2,58 \\cdot \\sqrt{ \\frac{0,43 \\cdot (1-0,43)}{2000} } \\le &amp; \\pi \\le 0,43 + 2,58 \\cdot \\sqrt{ \\frac{0,43 \\cdot (1-0,43)}{2000} }\\\\ 0,4014\\le &amp; \\pi \\le 0,4586\\\\ \\end{align*}\\] 10.5.1 Intervalos de confiança para a diferença entre duas proporções amostrais Para a construção de um intervalo de confiança para a diferença de duas proporções populacionais \\(\\pi_{X}\\) e \\(\\pi_{Y}\\) a partir das proporções obtidas em duas amostras de razoável tamanho (\\(n_{X} \\ge 30\\) e \\(n_{Y} \\ge 30\\)) e proporções amostrais \\(\\hat{p}_{X}\\) e \\(\\hat{p}_{Y}\\) não extremas (próximos a zero ou um) demosntra-se que a variável aleatória dessa diferença é tal que \\[ Z=\\frac{(\\hat{p}_{X}-\\hat{p}_{Y} )- (\\pi_{X}-\\pi_{Y}) }{\\sqrt{ \\frac{\\pi_{X}(1-\\pi_{X})}{n_{X}}+ \\frac{\\pi_{Y}(1-\\pi_{Y})}{n_{Y}}}} \\sim N\\left(0,1\\right), \\] Sob as condições anunciadas, demostran-se que se pode tomar as proporções amostrais \\(\\hat{p}_{X}\\) e \\(\\hat{p}_{Y}\\) como aproximações diretas das proporções populacionais \\(\\pi_{X}\\) e \\(\\pi_{Y}\\) na expressão da variância da distribuição Normal que modela a distribuição das diferenças das proporções amostrais sem que isso resulte em grande alteração na distribuição da variável \\(Z\\). \\[ Z=\\frac{(\\hat{p}_{X}-\\hat{p}_{Y} )- (\\pi_{X}-\\pi_{Y}) }{\\sqrt{ \\frac{\\hat{p}_{X}(1-\\hat{p}_{X})}{n_{X}}+ \\frac{\\hat{p}_{Y}(1-\\hat{p}_{Y})}{n_{Y}}}} \\sim N\\left(0,1\\right), \\] Assim podemos construir intervalos de confiança em torno da diferença das proporções populacionais \\(\\pi_{X}\\) e \\(\\pi_{Y}\\) associados a um nível de significância estabelecido: Bilaterais: intervalo delimitado por dois valores: mínimo e máximo, para a proporção amostral, dentro do qual todos os valores possuem um mesmo nível de significância: \\[ P\\left[(\\hat{p}_{X}-\\hat{p}_{Y}) - {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{{\\frac{\\hat{p}_{X}(1-\\hat{p}_{X})}{n_{X}}+ \\frac{\\hat{p}_{Y}(1-\\hat{p}_{Y})}{n_{Y}}}} \\\\ \\le \\hspace{0.1cm} (\\pi_{X}-\\pi_{Y}) \\hspace{0.1cm} \\le \\hspace{0.1cm} \\\\ (\\hat{p}_{X}-\\hat{p}_{Y}) + {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{{\\frac{\\hat{p}_{X}(1-\\hat{p}_{X})}{n_{X}}+ \\frac{\\hat{p}_{Y}(1-\\hat{p}_{Y})}{n_{Y}}}}\\right] = (1-\\alpha) \\] Unilaterais: intervalos delimitados apenas em um de seus lados nos quais todos os valores possuem um mesmo nível de significância: Valor máximo (limitando à direita): \\[ P\\left[(\\pi_{X}-\\pi_{Y}) \\hspace{0.1cm} \\le \\hspace{0.1cm} (\\hat{p}_{X}-\\hat{p}_{Y}) + {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{{\\frac{\\hat{p}_{X}(1-\\hat{p}_{X})}{n_{X}}+ \\frac{\\hat{p}_{Y}(1-\\hat{p}_{Y})}{n_{Y}}}}\\right] = (1-\\alpha) \\] Valor mínimo (limitando à esquerda): \\[ P\\left[(\\pi_{X}-\\pi_{Y}) \\hspace{0.1cm} \\ge \\hspace{0.1cm} (\\hat{p}_{X}-\\hat{p}_{Y}) - {z}_{\\left(\\frac{\\alpha }{2}\\right)} \\cdot \\sqrt{{\\frac{\\hat{p}_{X}(1-\\hat{p}_{X})}{n_{X}}+ \\frac{\\hat{p}_{Y}(1-\\hat{p}_{Y})}{n_{Y}}}}\\right] = (1-\\alpha) \\] "],["teste_hipoteses.html", "Capítulo 11 Introdução a testes de hipóteses ", " Capítulo 11 Introdução a testes de hipóteses "],["filosofia-da-ciência.html", "11.1 Filosofia da ciência", " 11.1 Filosofia da ciência Estritamente falando, todo o conhecimento fora da matemática, da lógica demonstrativa (um ramo da mesma) e da taxonomia encontra-se fundamentado em hipóteses (naturalmente há inúmeros tipos de hipóteses, mas as que estamos a nos referir são altamente confiáveis, como as expressas em certas leis gerais da física e da química como, por exemplo, a Lei de Hooke as Leis de Kepler dentre tantas outras). O raciocínio lógico demonstrativo permeia as ciências até onde a matemática lhe suporta; todavia, em si (assim como também a matemática), é incapaz de gerar novos conhecimentos sobre o mundo que nos rodeia. O método lógico demonstrativo é próprio para objetos que existem apenas idealmente, que são construídos inteiramente pelo nosso pensamento. O método hipotético experimental é próprio das ciências naturais (física, química, biologia, etc.), que observam seus objetos e realizam experimentos. Figure 11.1: Método demonstrativo e Método experimental hipotético (George Polya, 1954) Hipotético porque os cientistas partem de hipóteses sobre os objetos que guiam os experimentos e a avaliação dos resultados e experimental porque se baseia em observações e em experimentos, tanto para formular quanto para verificar as teorias. O método hipotético experimental pode ser indutivo (fatos \\(\\to\\) lei geral) ou dedutivo (lei geral \\(\\to\\) fatos): Hipotético-indutivo porque o cientista observa inúmeros fatos variando as condições da observação; elabora uma hipótese e realiza novos experimentos (ou induções) para confirmar ou negar a hipótese; se esta não for negada, chega-se à lei do fenômeno estudado. Hipotético-dedutivo porque tendo chegado à lei, o cientista pode formular novas hipóteses, deduzidas do conhecimento já adquirido, e com elas prever novos fatos, ou formular novas experiências, que o levam a conhecimentos novos. Em muitos processos de investigação científica é frequente ao pesquisador formular perguntas que deverão ser apropriadamente respondidas. comparar esses resultados a outros valores; ou, comparar resultados obtidos pela aplicação de diferentes métodos/ou produtos (valores centrais, variabilidade, proporções) observados em diferentes amostras. Figure 11.2: Método experimental hipotético Uma hipótese é uma conjectura racional feita após um grande número de observações e experimentos; é uma tese que precisa ser confirmada ou verificada por meio de novas observações e experimentos. Uma hipótese estatística é uma suposição feita sobre uma determinada característica de interesse de uma população sob estudo (um parâmetro) que subsiste (perdura, sobrevive, permanece incontestável) até que alguma informação sobre essa população seja estatisticamente significativa para contradizê-la. ``A ciência não consegue provar coisa alguma. Ela pode apenas refutar as coisas’’ (Karl Popper) Uma teoria científica é, portanto, transitória. Uma conjectura temporariamente sustentada que um dia poderá ser refutada e substituída por outra. Conclusões baseadas em raciocínios plausíveis são provisórias, ao contrário daquelas produzidas por raciocínios lógico demonstrativos. Um teste de hipóteses refere-se, portanto, a um método quantitativo subsidiário em processos de decisão, baseado na inferência estatística e de ampla aplicabilidade na experimentação e pesquisa; virtualmente, em qualquer área do conhecimento. "],["história.html", "11.2 História", " 11.2 História Figure 11.3: Oriatrike or, physick refined. The common errors therein refuted, and the whole art reformed and rectified: being a new rise and progress of phylosophy and medicine, for the destruction of diseases and prolongation of life (p. 526) Antigas referências relativas a testes de valores remontam aos séculos XVIII e XIX. Historicamente podemos retroceder a 1662, quando o médico flamengo Jean Baptista Van Helmont escreveu um desafio (aposta de 300 florins) em seu livro (Figura 11.4), sobre um procedimento teste que consistiria em se dividir 200 ou 500 pacientes com febre e pleurite em dois grupos iguais e aplicar a eles diferentes tratamentos: os habitualmente adotados pelos médicos da época e os seus próprios métodos. Ao final de um período de tempo (não foi especificado) verificar quantos funerais ocorreriam num e no outro (o livro foi publicado após sua morte, ocorrida em 1944, e não se tem registro sobre sua realização efetiva). Figure 11.4: Tratamento mais utilizado à época (sangria) Figure 11.5: John Arbuthnot, FRS (1667-1735) Outro registro histórico é o artigo publicado em 1710 na Royal Society’s Philosophical Transactions pelo médico escocês John Arbuthnot (1667-1735, Figura 11.5): An argument for Divine Providence (link). Este artigo foi um marco na história da estatística; em termos modernos, ele realizou testes de hipóteses estatísticas, calculando o p-valor através de um teste de sinais e interpretou-o como estatisticamente significante e assim rejeitou a hipótese nula. Isso é creditado como “[…] o primeiro uso de testes de significância […]” ( in “Estatísticos do século”, David Bellhouse, 2001). A estruturação dos testes de hipóteses, tal como são promovidos atualmente, é devida à metodológia empreendida por alguns dos mais destacados cientistas da área do final do século XIX e começo do XX (Figura 11.6). Figure 11.6: Personagens históricos   Em 1932 Karl Pearson se aposentou com professor da University College London e diretor do Laboratório Galton de eugenia. Apesar das objeções de Fisher, o laboratório de estatística foi dividido em dois departamentos. O Departamento de estatística (criado em 1901, o primeiro do gênero em uma universidade), assumido pelo filho mais novo de Karl, Egon; e o Laboratório de eugenia, assumido por seu sucessor na cadeira de Eugenia, Ronald Fisher.   O artigo de Henry F. Inman (Karl Pearson and R. A. Fisher on Statistical Tests: A 1935 Exchange From Nature, 1994) registra uma intensa troca de correspondências entre Fisher e Pearson tendo por assunto suas diferenças conceituais matemáticas e estatísticas, pela contrariedade de Pearson ante a continuidade de Fisher em lecionar teoria estatística e até mesmo por espaço físico para os experimentos científicos de Fisher, ao remover material do Museu de eugenia deixado por Pearson.   O pensamento estatístico da primeira metade do século XXI tem seu interesse voltado à solução dos problemas de testes de hipóteses e sua formulação e filosofia, tal como hoje são conhecidos, foi em grande parte criada por Ronald Aylmer Fisher (1890-1962), Jerzy Neyman (1894-1981) e Egon Sharpe Pearson (1895-1980) no período compreendido entre 1915-1933: Estudo biológico realizado por Karl Pearson para tentar associar informações coletadas a distribuições de probabilidade apresentava os componentes básicos de um teste de hipóteses; Ronald Fisher (1925): Statistical Methods for Research Workers; George Waddel Snedecor (1940): Statistical Methods; e, Erich Leo Lehmann (1959): Testing Statistical Hypotheses condensando os estudos desenvolvidos em 1920 pelo filho de Pearson, Egon, e o matemático polonês, Jerzy Neyman (formulação de Neyman-Pearson). "],["conceitos.html", "11.3 Conceitos", " 11.3 Conceitos A metodologia analisada na estruturação do método dos testes de hipóteses no fornece elementos auxiliares da decisão de rejeitar ou não - sob um prisma probabilístico - determinada conjectura postulada acerca de um parâmetro da população estudada. A conclusão de um teste de hipóteses resume-se a: aceitar ou rejeitar uma hipótese. Muitos estatísticos não adotam a expressão aceitar uma hipótese preferindo, no lugar, usar a expressão não rejeitar a hipótese sob um certo nível de significância. Por que essa distinção entre aceitar e não rejeitar? Ao se usar a expressão aceitar pode haver uma pré-concepção de que a hipótese é universalmente verdadeira (lembrando que a conclusão encontra-se alicerçada simplesmente em uma amostra). Utiliando-se a expressão não rejeitar salienta-se que a informação trazida pelos dados (a amostra) não foi suficientemente robusta para que pudéssemos abandonar essa hipótese em favor de uma outra. Alguns dizem que os estatísticos não se perguntam qual a probabilidade de estarem certos; mas de não estarem errados. Um teste de hipóteses guarda uma certa semelhança a um julgamento. Caso não haja indício forte o suficiente que comprove a culpa do acusado ele é declarado como inocente (mesmo que não o seja de fato). No contexto estatístico, os indícios que nos levam a rejeitar uma hipótese provêm da análise de informações observadas na amostra. A hipótese nula (H0) é a proposição ``tradicional’’ que reflete a situação na qual não há mudança. É, pois, uma hipótese conservadora, resultado de experimentos anteriores. A hipótese alternativa (H1) contradiz aquilo anunciado pela hipótese nula, é uma hipótese inovadora. Inicialmente a hipótese nula ela é assumida como verdadeira para, logo a seguir, ser confrontada novas evidências amostrais para se verificar a sustentabilidade de sua afirmação: caso a informação amostral demonstre a consistência de hipótese nula tudo o que pode ser feito é se decidir por sua manutenção (falho na tentativa de se derrubar a hipótese conservadora); e, caso não seja, analisa-se quão improvável pode ser a informação amostral além de uma dúvida razoável ou mera coincidência (nível de significância). ``Em relação a qualquer experimento não devemos falar desta hipótese como a hipótese nula, e deve-se atentar que a hipótese nula nunca é provada ou estabelecida, mas é, possivelmente, refutada, no decorrer da experimentação. Todo experimento deve existir apenas para das aos fatos a chance de refutar a hipótese nula…’’ (The Design of Experiments, Ronald Aylmer Fisher, 1935, p. 19) O objetivo de um teste de hipóteses é, pois, o de tomar uma decisão no sentido de verificar se existem razões para rejeitar ou não a hipótese nula. Esta decisão é baseada na informação disponível, obtida a partir de uma amostra, que se recolhe da população. Teste de hipóteses nos possibilitam associar um nível de significância (\\(\\alpha\\)) como medida probabilística do erro que se pode incorrer ao se concluir pela rejeição de uma hipótese verdadeira, na tomada de decisão. Nível de significância (\\(\\alpha\\)) é estabelecido pelo pesquisador (baseado tanto na expertise dele, quanto no campo a que o estudo pertence) antes do experimento ser realizado e corresponde ao grau do risco que se deseja incorrer ao se “rejeitar” uma hipótese verdadeira.   Nível de confiança (\\(1-\\alpha\\)) é a medida da confiabilidade de nossa conclusão no teste de hipóteses: “não rejeitar” uma hipótese verdadeira. "],["natureza-dos-erros.html", "11.4 Natureza dos erros", " 11.4 Natureza dos erros Para introduzir os conceitos relacionados aos erros considere uma situação onde uma empresa produz lâmpadas e a vida útil média, em horas, dessas lâmpadas segue uma distribuição Normal tal que \\(VU \\sim N (1600, 120)\\). Se não temos conhecimento algum sobre a real vida útil média dessas lâmpadas e alguém nos afirma que a vida útil é de 1.600 h, para confirmar ou não essa proposição (de um modo ``científico’’) devemos extrair uma amostra. Usando conceitos já explicados em uma unidade anterior podemos determinar o tamanho amostral em função de: um erro máximo tolerado: \\(\\varepsilon\\)=20 horas; um nível de significância estabelecido: \\(\\alpha\\)=0,05; e, e alguma informação sobre a medida da variabilidade da variável em estudo: \\(\\sigma\\)=120 horas (no caso, o desvio padrão populacional). Figure 11.7: Flutuação dos valores médios para diversas amostras extraídas de uma mesma população distribuição \\(\\sim N (\\mu; \\sigma)\\) ## mu media erro li ls ## 1 1600 1605 5.476431 1586 1625 ## 2 1600 1600 -0.421541 1579 1620 ## 3 1600 1619 19.252194 1597 1642 ## 4 1600 1612 12.374561 1595 1630 ## 5 1600 1605 5.305563 1585 1626 ## 6 1600 1593 -6.746075 1575 1611 ## 7 1600 1600 -0.416496 1580 1619 ## 8 1600 1592 -8.402743 1571 1612 ## 9 1600 1598 -2.423604 1579 1617 ## 10 1600 1591 -9.478302 1570 1611 ## 11 1600 1603 3.251126 1584 1622 ## 12 1600 1596 -4.394379 1577 1614 ## 13 1600 1590 -10.388268 1570 1609 ## 14 1600 1591 -9.225006 1571 1611 ## 15 1600 1605 4.659590 1584 1625 ## 16 1600 1613 13.341665 1594 1633 ## 17 1600 1609 9.231734 1590 1628 ## 18 1600 1587 -12.843802 1566 1609 ## 19 1600 1613 12.923539 1591 1634 ## 20 1600 1594 -5.752952 1577 1611 ## 21 1600 1606 6.414097 1587 1625 ## 22 1600 1604 3.987663 1584 1624 ## 23 1600 1582 -18.478543 1560 1603 ## 24 1600 1591 -8.571284 1570 1612 ## 25 1600 1588 -12.203208 1568 1607 ## 26 1600 1597 -3.424176 1579 1615 ## 27 1600 1612 11.548066 1592 1631 ## 28 1600 1607 6.941189 1587 1626 ## 29 1600 1604 4.179507 1585 1624 ## 30 1600 1591 -9.118204 1571 1611 ## 31 1600 1600 -0.295440 1580 1620 ## 32 1600 1622 21.771377 1602 1642 ## 33 1600 1611 11.025501 1592 1630 ## 34 1600 1602 2.438150 1580 1625 ## 35 1600 1598 -1.534284 1580 1617 ## 36 1600 1615 14.905999 1593 1636 ## 37 1600 1609 9.168151 1590 1628 ## 38 1600 1596 -4.446038 1576 1615 ## 39 1600 1610 9.609254 1590 1629 ## 40 1600 1595 -5.203361 1575 1615 ## 41 1600 1595 -4.886704 1575 1615 ## 42 1600 1610 10.297707 1590 1631 ## 43 1600 1599 -0.512888 1581 1618 ## 44 1600 1621 20.803611 1600 1641 ## 45 1600 1597 -2.897594 1579 1616 ## 46 1600 1595 -4.799805 1576 1615 ## 47 1600 1613 12.937178 1593 1633 ## 48 1600 1587 -12.582625 1569 1606 ## 49 1600 1603 2.981089 1584 1622 ## 50 1600 1596 -3.629258 1579 1614 ## 51 1600 1599 -1.109701 1582 1616 ## 52 1600 1594 -6.460380 1572 1615 ## 53 1600 1612 12.111165 1591 1633 ## 54 1600 1564 -36.049838 1545 1582 ## 55 1600 1596 -4.118078 1576 1616 ## 56 1600 1606 5.581449 1584 1628 ## 57 1600 1624 24.477880 1606 1643 ## 58 1600 1615 15.135746 1594 1636 ## 59 1600 1603 2.912773 1585 1620 ## 60 1600 1604 4.453340 1585 1624 ## 61 1600 1600 -0.009213 1578 1622 ## 62 1600 1621 20.549347 1602 1639 ## 63 1600 1603 3.376034 1582 1624 ## 64 1600 1608 7.536734 1589 1626 ## 65 1600 1586 -13.791598 1567 1605 ## 66 1600 1595 -4.943393 1578 1612 ## 67 1600 1601 0.747261 1582 1619 ## 68 1600 1616 16.253163 1599 1634 ## 69 1600 1594 -5.771740 1572 1617 ## 70 1600 1612 12.167261 1593 1632 ## 71 1600 1582 -17.519852 1563 1602 ## 72 1600 1604 3.628385 1584 1623 ## 73 1600 1578 -22.057391 1558 1598 ## 74 1600 1619 19.228394 1598 1641 ## 75 1600 1604 4.481026 1587 1622 ## 76 1600 1590 -10.069121 1572 1608 ## 77 1600 1597 -3.353455 1575 1619 ## 78 1600 1615 15.282562 1594 1636 ## 79 1600 1601 1.399270 1581 1621 ## 80 1600 1602 2.394851 1583 1622 ## 81 1600 1589 -10.521208 1571 1608 ## 82 1600 1601 0.503118 1581 1620 ## 83 1600 1585 -14.528167 1563 1608 ## 84 1600 1601 0.834998 1579 1623 ## 85 1600 1613 13.266104 1595 1631 ## 86 1600 1603 3.138234 1584 1623 ## 87 1600 1588 -11.712403 1569 1608 ## 88 1600 1603 3.448981 1584 1623 ## 89 1600 1600 -0.359498 1581 1618 ## 90 1600 1613 12.505675 1594 1631 ## 91 1600 1602 1.632606 1581 1622 ## 92 1600 1603 3.099873 1583 1623 ## 93 1600 1611 10.662820 1591 1631 ## 94 1600 1596 -3.683744 1575 1618 ## 95 1600 1601 0.563793 1580 1621 ## 96 1600 1616 15.533283 1595 1636 ## 97 1600 1592 -8.102881 1571 1613 ## 98 1600 1601 0.744401 1580 1621 ## 99 1600 1584 -15.807058 1563 1605 ## 100 1600 1616 15.907580 1596 1636 Observa-se que algumas das amostras, numa proporção igual ao nível de significância estabelecido quando do dimensionamento (5%), apresentam médias com valores que se afastam do valor médio populacional mais que o erro estabelecido (20 h).   Como já informado anteriormente, um teste de hipóteses é um método quantitativo e não se baseia, sobremaneira, em impressões pessoais ou outros achismos. Os cenários a seguir foram criados apenas para tentar estabelcer um paralelo entre a probabilidade de se obter médias amostrais muito destoantes da média populacional e uma “inclinação subjetiva” em se rejeitar uma afirmação. Considere que a sua amostra em particular é uma das que não se afasta tanto do valor que lhe afirmaram (a vida útil das lâmpadas é de 1.600 h). Nessa situação, talvez você não se “convencesse” de que a vida útil média fosse diferente daquilo que lhe informaram e, assim, não iria recusar a afirmação. Agora considere que a sua amostra em particular é uma das que se afasta muito do valor que lhe afirmaram. Nessa nova situação, certamente você iria “suspeitar” que a vida útil média é diferente daquilo que lhe informaram e assim, recusar a afirmação. Na primeira decisão, você não recusou uma afirmação que era, de fato, verdadeira; ao passo que na segunda, você rejeitou uma afirmação que era verdadeira (lembrando que você não sabia que a vida útil média é, de fato, 1.600 h). Como se vê no quadro abaixo, há dois tipos de erros envolvidos em um teste de hipóteses e suas consequências, muitas vezes, são bem diferentes. Erro do tipo I e Erro do tipo II. Um erro do tipo I ocorre quando o pesquisador rejeita uma hipótese nula quando é verdadeira. A probabilidade (limitada pelo pesquisador) de se incorrer em um erro do tipo I é chamada de nível de significância e é frequentemente denotada pela letra grega \\(\\alpha\\). Um erro do tipo II ocorre quando o pesquisador não rejeita uma hipótese nula que é falsa. A probabilidade de cometer um erro do tipo II, também chamada de poder do teste e é frequentemente denotada pela letra grega \\(\\beta\\). Erros envolvidos na rejeição ou não da hipótese nula Valor real do parâmetro Não rejeitar Rejeitar (desconhecido) H0 H0 H0 verdadeira Decisão correta Erro do tipo I probabilidade associada=(1 − α) probabilidade associada= α H0 falsa Erro do tipo II Decisão correta probabilidade associada=β probabilidade associada =(1 − β) No quadro acima identificam-se: \\(\\alpha\\): a probabilidade associada ao cometimento de um erro do tipo I: rejeitar a hipótese nula sendo ela verdadeira (arbitrado pelo pesquisador, é denominado nível de significância do teste); \\(\\beta\\): a probabilidade associada ao cometimento de um erro do tipo II: não rejeitar a hipótese nula sendo esta falsa; (1-\\(\\alpha\\)): o nível de confiança estabelecido para a decisão, a probabilidade associada em não se rejeitar a hipótese nula (\\(H_{0}\\)) quando ela é, de fato, verdadeira; e, (1-\\(\\beta\\)): o poder do teste, a probabilidade associada em não se aceitar a hipótese nula (\\(H_{0}\\)) quando ela é, de fato, falsa. Qual erro é o pior? Por exemplo, se alguém testa a presença de alguma doença em um paciente, decidindo incorretamente sobre a necessidade do tratamento (ou seja, decidindo que a pessoa está doente), pode submetê-lo ao desconforto pelo tratamento (efeitos colaterais) além de perda financeira pela despesa incorrida. Mas por outro lado, a falha em diagnosticar a presença da doença no paciente pode levá-lo à morte pela ausência de tratamento. Outro exemplo clássico a ser citado seria o de condenar uma pessoa inocente ou libertar um criminoso. Como não há uma regra clara sobre qual tipo de erro é o pior recomenda-se quando se usa dados para testar uma hipótese observar com muito cuidado as consequências que podem seguir os dois tipos de erros. Vários especialistas sugerem o uso de uma tabela como a abaixo para detalhar as consequências de um erro Tipo 1 e Tipo 2 em sua análise específica. Consequências da tomada de decisão face aos erro envolvidos H0 explicada Erro tipo 1: rejeitar H0 quando verdadeira Erro tipo II: não rejeitar H0 quando falsa O medicamento “A“ não alivia a Condição “B“ O medicamento “A“ não alivia a Condição “B“, mas não é eliminado como opção de tratamento O medicamento “A“ alivia a condição “B“, mas é eliminado como opção de tratamento Consequências Pacientes com Condição “B“ que recebem o Medicamento “A“ não obtêm alívio. Eles podem experimentar piora da condição e/ou efeitos colaterais, até e incluindo a morte. A empresa produtora do medicamento pode enfrentar processos judiciais Um tratamento viável permanece indisponível para pacientes com Condição “B“. Os custos de desenvolvimento são perdidos. O potencial lucro pela produção do medicamente “A“ pela empresa é eliminado. É desejável conduzir o teste de um modo a manter a probabilidade de ambos os tipos de erro em um mínimo. aumentar o tamanho amostral reduz a probabilidade associada ao cometimento de erro do tipo II (\\(\\beta\\)) e, consequentemente, aumenta o poder do teste (\\(1- \\beta\\)); aumentar o nível de significância (\\(\\alpha\\)) tem implicação direta na probabilidade associada ao cometimento de erro do tipo I todavia reduz a probabilidade associada ao cometimento de erro do tipo II (\\(\\beta\\)). "],["recomendações-gerais.html", "11.5 Recomendações gerais", " 11.5 Recomendações gerais o pesquisador deve delimitar o objeto de sua pesquisa; uma boa hipótese deve ser baseada em uma boa pergunta sobre o objeto do estudo; deve ser simples e específica; deve ser formulada na fase propositiva da pesquisa e não após a coleta de dados (post hoc); enunciar as hipóteses: as hipóteses são apresentadas de tal maneira que sejam mutuamente exclusivas (o que afirmado por uma deve ser contradito pela outra); as hipóteses são comumente denominadas por hipótese nula (\\(H_{0}\\)) e hipótese alternativa (\\(H_{1}\\)); a hipótese nula (\\(H_{0}\\)) que será testada sob um nível de significância (\\(\\alpha\\)) é, em geral, de concordância com o parâmetro que se estuda da população (conservadora) e baseada em conhecimento prévio; a hipótese alternativa (\\(H_{1}\\)) é contrária, oposta, antagônica à hipótese nula (novadora); e, estabelecer um nível apropriado para a significância \\(\\alpha\\) (em alguns campos do conhecimento níveis de significância muito reduzidos são impraticáveis). "],["efeito-do-limite-central.html", "11.6 Efeito do limite central", " 11.6 Efeito do limite central Seja \\(X_{1}, X_{2}, ...\\) uma sequência de variáveis aleatórias independentes e identicamente distribuídas, cada uma com média finita \\(\\mu=E(X_{i})\\). A Lei forte dos grandes números (teorema) demonstra que \\[ \\frac{X_{1} + X_{2} + \\dots, X_{n}}{n} \\to \\mu \\] quando \\(n \\to \\infty\\). Isto é, \\(P\\{lim_{\\to \\infty}(\\frac{X_{1} + X_{2} \\dots + X_{n}}{n})=\\mu\\}=1\\) 11.6.1 Erro global O erro global (\\(\\varepsilon= X -\\mu\\)) é um agregado de componentes. Uma medida (observação) obtida em um ensaio experimental específico pode estar sujeita a erros: analíticos; de amostragem (física, química, biológica, …); processuais (produzido por falhas no cumprimento das configurações exatas das condições experimentais); erros devidos à variação de matérias-primas; medição (diferentes operadores de equipamentos ou equipamentos descalibrados). Assim, \\(\\varepsilon\\) será uma função linear de componentes \\(\\varepsilon_{1}\\), \\(\\varepsilon_{2}, ...,\\varepsilon_{n}\\) de erros. Se cada erro individual for relativamente pequeno, será possível aproximar o erro global como uma função linear dos componentes de erros, onde \\(a\\) são constantes: \\[ \\varepsilon = a_{1}\\varepsilon_{1} + a_{2}\\varepsilon_{2} + ... + a_{n}\\varepsilon_{n} \\] O Teorema do limite central afirma que, sob condições quase sempre satisfeitas no mundo real da experimentação, a distribuição de tal função linear de erros tenderá à uma distribuição Normal quando o número de seus componentes torna-se grande, independentemente da distribuição original da população de onde suas amostras geradoras se originaram. Seja \\(X_{1},\\dots,X_{n}\\) uma sequência de variáveis aleatórias independentes e identicamente distribuídas, com média \\(\\mu\\) e variância \\(\\sigma^{2}\\). A distribuição assumirá um perfil \\[ \\frac{X_{1} + X_{2} \\dots + X_{n} - n \\mu}{\\sigma \\sqrt{n}} \\sim \\mathcal{N}(0,1) \\] quando \\(n \\to \\infty\\). Assim, para \\(-\\infty &lt; a &lt; \\infty\\), \\[ P \\{ \\frac{X_{1} + X_{2} \\dots + X_{n} - n \\mu}{\\sigma \\sqrt{n}} \\leq a\\}\\to \\mathcal{N}(0,1) \\] quando \\(n \\to \\infty\\). Denotando-se de um modo alternativo, podemos então definir a estatística Z e sua correspondente distribuição como \\[ Z = \\frac{ \\stackrel{-}{X} - \\mu }{ \\frac{\\sigma}{\\sqrt{n}} } = \\frac{\\sqrt{n}\\left(\\stackrel{-}{X}-\\mu \\right)}{\\sigma } \\sim \\mathcal{N}(0,1) \\] Ou seja, \\(Z\\) é uma variável aleatória que segue a distribuição Normal com média zero e desvio-padrão unitário (Normal padronizada). Em resumo: quando, como é habitual, um erro experimental é um agregado de vários erros de componentes, sua distribuição tende para a forma Normal, mesmo a distribuição dos componentes pode ser marcadamente não Normal; A média da amostra tende a ser distribuída Normalmente, mesmo que as observações individuais em que se baseia não o sejam. Consequentemente, métodos estatísticos que dependam, não diretamente da distribuição das observações individuais, mas na distribuição das médias tendem a ser insensíveis ou robustos à não normalidade. Procedimentos que comparam médias são geralmente robustos à não normalidade. "],["estruturas-das-hipóteses.html", "11.7 Estruturas das hipóteses", " 11.7 Estruturas das hipóteses 11.7.1 Interpretação gráfica dos níveis de significância/confiança O delineamento de um teste de hipóteses inclui regras de decisão para se rejeitar ou não a hipótese nula. Essas regras de decisão passam pela comparação dos valores calculados de uma estatística apropriada para o teste em curso com seus valores extremos, frequentemente obtidos em tabelas, os quais estão associados ao complemento de uma probabilidade (o nível de confiança) de ocorrência condizente ao nível de significância estabelecido na pesquisa. Essa comparação é por demais facilitada se visualizada no gráfico da densidade de probabilidade da distribuição da estatística do teste, onde regiões (baseadas no nível de significância estabelecido) podem ser estabelecidas: testes bilaterais ( hipótese alternativa do tipo: diferente de ): a região é fechada, delimitada à esquerda e à direita por valores críticos de estatística do teste; testes unilaterais à direita ( hipótese alternativa do tipo: maior que ): a região é fecfada à esquerda, delimitada por um valor crítico da estatística do teste e aberta à direita (ao \\(\\rightarrow \\infty\\)[; e, testes unilaterais à esquerda ( hipótese alternativa do tipo: menor que ): a região é fechada à direita, delimitada por um valor crítico da estatística do teste e aberta à esquerda ]$ -$). No gráfico de densidade de probabilidade da estatística do teste temos uma primeira região frequentemente denominada de região de não rejeição: um intervalo de valores dentro do qual, se o valor calculado para a estatística de teste estiver contido, a hipótese nula não será rejeitada. O intervalo de valores que delimitam a região de não rejeição é tal que a probabilidade dessa região é igual ao nível de confiança \\((1-\\alpha)\\). Se a estatística calculada para o teste estiver fora da faixa de valores delimitada na região de não rejeição a hipótese nula poderá ser rejeitada sob o nível de significância \\(\\alpha\\) estabelecido; ou seja, a probabilidade de se incorrer em um erro Tipo I: rejeitar a hipótese nula quando ela é verdadeira é igual a \\(\\alpha\\). Com a popularização dos programas estatísticos computacionais, a probabilidade exata associada ao valor calculado da estatística do teste passou ser neles apresentada de modo default, nominada pela expressão valor p ( p-Value ) que expressa uma probabilidade. Para melhor entender o valor-p ( p-value) suponha que o valor da estatística do teste seja igual a \\(\\zeta\\). O valor p é o quantil associado (a probabiliadde exata) a \\(\\zeta\\) na distribuição de probabilidade usada como referência. Se o valor p for menor que o nível de significância (\\(\\alpha\\)) estipulado pelo pesquisador, rejeita-se a hipótese nula sob esse nível de significância de cometimento de um erro do tipo I. 11.7.2 Teste de hipóteses Bilateral Nesse tipo de teste a hipótese aternativa é proposta como a dizer que o valor em teste é diferente daquele afirmado pela hipótese nula (conservadora): \\[ \\begin{cases} H_{0}: \\mu = \\mu_{0}\\\\ H_{1}: \\mu \\ne \\mu_{0}\\\\ \\end{cases} \\] em que \\(\\mu_{0}\\) é um valor conservador do parâmetro \\(\\mu\\) que se deseja testar. alfa=0.05 prob_desejada1=alfa/2 z_desejado1=round(qnorm(prob_desejada1),4) d_desejada1=dnorm(z_desejado1, 0, 1) prob_desejada2=1-alfa/2 z_desejado2=round(qnorm(prob_desejada2),4) d_desejada2=dnorm(z_desejado2, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores da estatística calculada para o teste&quot;) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P((-val. crítc), (val. crít.))=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; (-val. crític.))= P((val.crítc.); \\U221e)= \\u03b1/2 em vermelho &quot;)+ geom_segment(aes(x = z_desejado1, y = 0, xend = z_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = z_desejado2, y = 0, xend = z_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-0.1, y=d_desejada1, label=&quot;-(valor crítico)&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.3, y=d_desejada2, label=&quot;(valor crítico)&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.8: Regiões críticas, aquém e além das quais, a probabilidade associada aos valores amostrais observados é inferior a \\(\\frac{\\alpha}{2}\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 11.8 observa-se:   as regiões de rejeição da hipótese nula (subdivididas nos dois lados) sob a curva da função densidade de probabilidade da distribuição adequada ao teste com probabilidades iguais ao nível de significância \\(\\alpha\\) ; a região de não rejeição da hipótese nula (delimitada à esquerda e à direita) com probabilidade igual ao nível de confiança \\((1-\\alpha)\\); e, os valores críticos da estatística do teste. 11.7.3 Teste de hipóteses Unilateral à esquerda Nesse tipo de teste a hipótese aternativa é proposta como a dizer que o valor em teste não apenas é diferente, mas é menor do que aquele afirmado pela hipótese nula (conservadora): \\[ \\begin{cases} H_{0}: \\mu \\ge \\mu_{0}\\\\ H_{1}: \\mu &lt; \\mu_{0}\\\\ \\end{cases} \\] em que \\(\\mu_{0}\\) é um valor conservador do parâmetro \\(\\mu\\) que se deseja. alfa=0.05 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores da estatística calculada para o teste&quot;) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P( (-val. crít.),\\U221e,)=(1-\\u03b1) em cinza (nível de confiança) \\nP(-\\U221e; (-val. crític.))=\\u03b1 em vermelho &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado-0.1, y=d_desejada, label=&quot;-(valor crítico)&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado+1.3, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.9: Região crítica aquém da qual a probabilidade associada aos valores amostrais observados é inferior a \\(\\alpha\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 11.9 observa-se:   a região de rejeição da hipótese nula delimitada sob a curva da função densidade de probabilidade da distribuição adequada ao teste com probabilidade igual ao nível de significância \\(\\alpha\\) ; a região de não rejeição da hipótese nula (delimitada à esquerda) com probabilidade igual ao nível de confiança \\((1-\\alpha)\\); e, os valores críticos da estatística do teste. 11.7.4 Teste de hipóteses Unilateral à direita Nesse tipo de teste a hipótese aternativa é proposta como a dizer que o valor em teste não apenas é diferente, mas é maior do que aquele afirmado pela hipótese nula (conservadora): \\[ \\begin{cases} H_{0}: \\mu \\le \\mu_{0}\\\\ H_{1}: \\mu &gt; \\mu_{0}\\\\ \\end{cases} \\] em que \\(\\mu_{0}\\) é um valor conservador do parâmetro \\(\\mu\\) que se deseja testar . alfa=0.95 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores da estatística calculada para o teste&quot;) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-\\U221e, (val. crít.))=(1-\\u03b1) em cinza (nível de confiança) \\nP((val.crítc.); \\U221e)= \\u03b1 em vermelho &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado+0.3, y=d_desejada, label=&quot;(valor crítico)&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-2.5, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.10: Região crítica além da qual a probabilidade associada aos valores amostrais observados é inferior a \\(\\alpha\\), estabelecendo assim um intervalo com nível de confiança igual a \\((1-\\alpha)\\) Na Figura 11.10 observa-se:   a região de rejeição da hipótese nula delimitada sob a curva da função densidade de probabilidade da distribuição adequada ao teste com probabilidade igual ao nível de significância \\(\\alpha\\) ; a região de não rejeição da hipótese nula (delimitada à direita) com probabilidade igual ao nível de confiança \\((1-\\alpha)\\); e, os valores críticos da estatística do teste. "],["teste-de-hipóteses-para-uma-média-mu.html", "11.8 Teste de hipóteses para uma média \\(\\mu\\)", " 11.8 Teste de hipóteses para uma média \\(\\mu\\) 11.8.1 Cenários possíveis variância populacional (\\(\\sigma^2\\)) teoricamente conhecida; variância populacional (\\(\\sigma^2\\)) desconhecida, mas o tamanho da amostra (\\(n\\)) é grande: \\(n\\ge 30\\); e, variância populacional (\\(\\sigma\\)) desconhecida e as amostras de tamanho (\\(n\\)) reduzido: \\(n &lt; 30\\). Estatística do teste para a primeira situação: variância populacional conhecida \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim \\mathcal{N}(0,1) \\] em que: \\(\\stackrel{-}{X}\\) é a média observada na amostra; \\(\\mu\\) o valor (desconhecido) inferido à média populacional, a ser testado frente à média amostral observada; \\(\\sigma\\) é o desvio padrão populacional; e, \\(n\\) é o tamanho da amostra. Estatística do teste para a segunda situação: variância populacional desconhecida mas amostras grandes: \\(n\\ge30(40)\\): \\(S\\) pode ser tomado como estimativa de \\(\\sigma\\): \\[ Z = \\frac{\\stackrel{-}{X} - \\mu}{\\frac{S}{\\sqrt{n}}} \\sim \\mathcal{N}(0,1) \\] em que: \\(\\stackrel{-}{X}\\) é a média observada na amostra; \\(\\mu\\) o valor (desconhecido) inferido à média populacional a ser testado frente à média amostral observada; \\(S\\) é o desvio padrão amostral; e, \\(n\\) é o tamanho da amostra. Estatística do teste para a terceira situação: variância populacional desconhecida e amostras pequenas: \\(n&lt;30\\): \\[ T = \\frac{(\\stackrel{-}{X} - \\mu)}{ \\frac{S}{\\sqrt{n}} } \\sim t_{(n-1)} \\] em que: \\(\\stackrel{-}{X}\\) é a média observada na amostra; \\(\\mu\\) o valor (desconhecido) inferido à média populacional, a ser testado frente à média amostral; \\(S\\) é o desvio padrão amostral; e, \\(n\\) é o tamanho da amostra. # Definição do eixo x x &lt;- seq(-4, 4, length.out = 100) # Densidade da distribuição normal padrão y_norm &lt;- dnorm(x, mean = 0, sd = 1) # Lista com diferentes graus de liberdade df_list=c(1, 2, 4, 8, 20) # Lista com cores para as curvas da distribuição t colors=c(&quot;#097aeb&quot;, &quot;#a37602&quot;, &quot;#02a6f2&quot;, &quot;#9635a1&quot;, &quot;#16b533&quot;) # Criação do data frame com todas as curvas data=data.frame() for (i in seq_along(df_list)) { df = df_list[i] y_t = dt(x, df) df_data = data.frame(x, y_t, df) data = rbind(data, df_data) } # Plotagem do gráfico p = ggplot(data, aes(x = x)) + geom_line(aes(y = y_t, color = factor(df)), size = 1) + scale_color_manual(values = colors, name = &quot;Graus de liberdade&quot;)+ ggtitle(&quot;Distribuição t sob diferentes graus de liberdade \\ne sua aproximação à Normal padronizada)&quot;) + xlab(&quot;Valores assumidos&quot;) + ylab(&quot;Densidade&quot;) + theme_classic() + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = &quot;red&quot;, size=1.5, linetype=&#39;dashed&#39;) print(p) 11.8.2 Roteiro geral identificar o modelo de probabilidade do estimador do parâmetro da população que se estuda; identificar a estatística apropriada para o teste em razão das informações disponíveis acerca da população, do tamanho da amostra e sua independência: escore médio; proporção; estatísticas T, Z, F, ou \\(\\chi\\); determinar na curva de densidade de probabilidade do modelo da estatística de teste a(s) região(ões) crítica(s): faixa(s) de valores da estatística que nos levam à rejeição ou não da hipótese \\(H_{0}\\) em função do nível de significância previamente arbitrado pelo pesquisador \\(\\alpha\\); calcular a estatística do teste apropriada para o parâmetro que se pretende inferir com base na amostra extraída; concluir com base nos resultados analisados: se o valor da estatística do teste pertence à(s) região(ões) crítica(s) de sua distribuição teórica, rejeitar \\(H_{0}\\); caso contrário não há evidências estatisticamente significativas para rejeitá-la. 11.8.3 Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística Z (\\(Z \\sim \\mathcal{N}(0,1)\\)): Teste de hipóteses bilateral (tipo: diferente de): \\[\\begin{align*} P[\\left|Z_{calc}\\right| \\le {Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}|\\mu=\\mu_{0}] &amp; =(1-\\alpha)\\\\ P(-{Z}_{tab\\left(\\frac{\\alpha }{2}\\right)} \\le Z_{calc} \\le {Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}) &amp; = (1-\\alpha)\\\\ \\end{align*}\\] Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[\\begin{align*} P[Z_{calc} \\ge -{Z}_{tab\\left(\\alpha \\right)}|\\mu \\ge \\mu_{0}] &amp; =(1-\\alpha) \\\\ P(Z_{calc} \\ge -{Z}_{tab\\left(\\alpha \\right)}) &amp; =(1-\\alpha)\\\\ \\end{align*}\\] Teste de hipóteses unilateral à direita (tipo maior que): \\[\\begin{align*} P[Z_{calc} \\le {Z}_{tab\\left(\\alpha \\right)}|\\mu \\le \\mu_{0}] &amp; =(1-\\alpha)\\\\ P(Z_{calc} \\le {Z}_{tab\\left(\\alpha \\right)}) &amp; =(1-\\alpha)\\\\ \\end{align*}\\] 11.8.4 Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística T (\\(T\\sim t_{(n-1)}\\)): Teste de hipóteses bilateral (tipo: diferente de): \\[\\begin{align*} P[\\left|t_{calc}\\right| \\ge {t}_{tab\\left(\\frac{\\alpha }{2};n-1\\right)}|\\mu=\\mu_{0}] &amp; =(1-\\alpha)\\\\ P(-{t}_{tab\\left(\\frac{\\alpha }{2};n-1\\right)} \\le t_{calc} \\le {t}_{tab\\left(\\frac{\\alpha }{2};n-1\\right)}) &amp; =(1-\\alpha) \\end{align*}\\] Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[\\begin{align*} P[t_{calc} \\ge -{t}_{tab\\left(\\alpha \\right)}|\\mu \\ge \\mu_{0}] &amp; =(1-\\alpha)\\\\ P( t_{calc} \\ge -{t}_{tab\\left(\\alpha;n-1\\right)}) &amp; = (1-\\alpha) \\end{align*}\\] Teste de hipóteses unilateral à direita (tipo: maior que): \\[\\begin{align*} P[t_{calc} \\le {t}_{tab\\left(\\alpha \\right)}|\\mu \\le \\mu_{0}] &amp; =(1-\\alpha) \\\\ P( t_{calc} \\le {t}_{tab\\left(\\alpha;n-1\\right)} ) &amp; = (1-\\alpha) \\end{align*}\\] Exemplo: O tempo de vida útil de uma amostra de 100 lâmpadas fluorescentes produzidas por uma fábrica foi calculado resultando em uma vida útil média de 1570 h sob um desvio padrão de 120 h. Seja \\(\\mu\\) é o tempo de vida útil das lâmpadas produzidas pela empresa. Teste a hipótese de \\(\\mu=1600 h\\) contra a hipótese alternativa de \\(\\mu \\neq 1600 h\\) sob um nível de significância \\(\\alpha=0,05\\). O problema nos pede um teste bilateral (tipo: diferente de): \\[ \\begin{cases} H_{0}: \\mu = 1.600\\\\ H_{1}: \\mu \\ne 1.600\\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a vida útil média das lâmpadas a 1.600 h., fazendo então valer a hipótese alternativa que afirma ser a vida útil das lâmpadas diferente de 1.600 h. Pelo enunciado do problema a variância populacional \\(\\sigma^{2}\\) é desconhecida mas, como a amostra é de grande tamanho (n=100) podemos tomar \\(S\\) como uma estimativa de \\(\\sigma\\) e a estatística do teste fica definida como sendo: \\[ Z = \\frac{\\stackrel{-}{X} - \\mu_{0}}{\\frac{S}{\\sqrt{n}}} \\sim \\mathcal{N}(0,1) \\] Extraindo os dados do problema: \\(\\stackrel{-}{X}=1570h\\) é a média amostral; \\(\\mu_{0}=1600\\) o valor (desconhecido) inferido à média populacional a ser testado frente à média amostral; \\(S=120h\\) é o desvio padrão amostral; e, \\(n=100\\) é o tamanho da amostra. Calculando-se o valor da estatística do teste: \\[ z_{calc} = \\frac{1570 - 1600}{\\frac{120}{\\sqrt{100}} } =-2,50 \\] Da tabela da distribuição Normal reduzida obtemos o valor crítico bicaudal: \\(|{z}_{crit}|=1,96\\). Pelo cálculo, a estatística do teste é \\(z_{calc}=-2,50\\). alfa=0.05 prob_desejada1=alfa/2 z_desejado1=round(qnorm(prob_desejada1),4) d_desejada1=dnorm(z_desejado1, 0, 1) prob_desejada2=1-alfa/2 z_desejado2=round(qnorm(prob_desejada2),4) d_desejada2=dnorm(z_desejado2, 0, 1) z_calculado=-2.5 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de z&quot;, breaks = c(z_desejado1,z_desejado2)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-1,96, 1,96)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(-\\U221e; -1,96)= P(1,96; \\U221e)= \\u03b1/2 em vermelho (nível de significância/2=0,025) &quot;)+ geom_segment(aes(x = z_desejado1, y = 0, xend = z_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = z_desejado2, y = 0, xend = z_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-0.1, y=d_desejada1, label=&quot;valor crítico=-1,96&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.3, y=d_desejada2, label=&quot;valor crítico=1,96&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-2,5&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.11: Regiões de rejeição da hipótese nula para o teste bilateral (tipo: diferente de) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valores críticos da estatística do teste: \\(z_{crit} =\\pm 1,96\\). O valor calculado da estatística (\\(z_{calc}=-2,50\\)) situa-se na faixa de significância do teste, possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos na análise estatística realizada nos permitem rejeitar a hipótese de que a duração média populacional das lâmpadas seja igual a 1600h sob um nível de confiança de 95%. A vida útil média das lâmpadas é diferente de 1600h (Figura 11.11). Podemos ainda realizar testes de hipóteses unilaterais (\\(\\mu&lt;\\mu_{0}\\) ou \\(\\mu&gt;\\mu_{0}\\)). Teste unilateral à esquerda (tipo: menor que) \\[ \\begin{cases} H_{0}: \\mu \\ge 1.600 \\\\ H_{1}: \\mu &lt; 1.600 \\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a vida útil média das lâmpadas igual ou superior a 1.600 h., fazendo então valer a hipótese alternativa que afirma ser a vida útil das lâmpadas menor que 1.600 h. Da tabela da distribuição Normal reduzida obtemos o valor crítico monocaudal: \\({z}_{crit}=-1,64\\). Pelo cálculo, a estatística do teste é \\(z_{calc}=-2,50\\). alfa=0.05 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) z_calculado=-2.5 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores da estatística calculada para o teste&quot;) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P( -1,64,\\U221e,)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(-\\U221e; -1,64)=\\u03b1 em vermelho (nível de significância=0,05) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado-0.1, y=d_desejada, label=&quot;valor crítico=-1,64&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-2.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado+1, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-2,5&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.12: Região de rejeição da hipótese nula para o teste unilateral à esquerda (tipo: menor que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(z_{crit} = -1,64\\). O valor calculado da estatística (\\(z_{calc}=-2,50\\)) situa-se na faixa de significância do teste, possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos na análise estatística realizada nos permitem rejeitar a hipótese de que a duração média populacional das lâmpadas seja igual ou superior a 1600h sob um nível de confiança de 95%. A vida útil média é menor que 1600h (Figura 11.12). Teste unilateral à direita (tipo: maior que) \\[ \\begin{cases} H_{0}: \\mu \\le 1.600 \\\\ H_{1}: \\mu &gt; 1.600 \\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a vida útil média das lâmpadas igual ou inferior a 1.600 h., fazendo então valer a hipótese alternativa que afirma ser a vida útil das lâmpadas maior que 1.600 h. Da tabela da distribuição Normal reduzida obtemos o valor crítico monocaudal: \\({z}_{crit}=1,64\\). Pelo cálculo, a estatística do teste é \\(z_{calc}=-2,50\\). alfa=0.95 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) z_calculado=-2.5 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores da estatística calculada para o teste&quot;) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P( -1,96,\\U221e,)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(-\\U221e; -1,96)=\\u03b1 em vermelho (nível de significância=0,05) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado-0.1, y=d_desejada, label=&quot;valor crítico=-1,64&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado+1, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-2.5, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-2,5&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.13: Região de rejeição da hipótese nula para o teste unilateral à direita (tipo: maior que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(z_{crit} = 1,64\\). O valor calculado da estatística (\\(z_{calc}=-2,50\\)) situa-se na faixa de não significância do teste, não possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos na análise estatística realizada não nos permitem rejeitar a hipótese de que a duração média populacional das lâmpadas seja igual ou inferior a 1600h sob um nível de confiança de 95%. A vida útil média é maior que 1600h (Figura 11.12). Exemplo: De um universo Normal com parâmetros média e variância (\\(\\mu\\) e \\(\\sigma^{2}\\)) desconhecidos, retirou-se uma amostra aleatória composta por 9 observações que apresentou as seguintes sínteses numéricas: \\(\\stackrel{-}{X} = 4\\) e \\(S^{2} = 2,2\\). Proceda ao seguinte teste de hipóteses, a um nível de significância: \\(\\alpha=0,05\\), de que a média populacional é igual a 5. O problema nos pede um teste bilateral (tipo: diferente de): \\[ \\begin{cases} H_{0}: \\mu = 5\\\\ H_{1}: \\mu \\ne 5\\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a média igual a 5, fazendo então valer a hipótese alternativa que afirma ser a média diferente de 5. Pelo enunciado do problema a variância populacional \\(\\sigma^{2}\\) é desconhecida e a amostra é pequena (n=9). Nessa situação, a estatística do teste fica definida como sendo: \\[ T = \\frac{(\\stackrel{-}{X} - \\mu_{0})}{ \\frac{s}{\\sqrt{n}} } \\sim t_{(n-1)} \\] Extraindo os dados do problema: \\(\\stackrel{-}{x}=4\\) é a média amostral; \\(\\mu_{0}=5\\) o valor (desconhecido) inferido à média populacional, a ser testado frente à média amostral; \\(s = \\sqrt{2,2}=1,48\\) é o desvio padrão da amostra extraída; \\(n = 9\\) é o tamanho da amostra extraída; Calculando-se o valor da estatística do teste: \\[ t_{calc} = \\frac{(\\stackrel{-}{X} - \\mu_{0})}{ \\frac{s}{\\sqrt{n}} } = -2,02 \\] Da tabela ``t’’ de Student obtemos o valor crítico bicaudal: \\(|{t}_{tab\\left(\\frac{\\alpha }{2}\\right), (n-1)}|=2,306\\). Pelo cálculo a estatística do teste é \\(t_{calc}=-2,02\\). alfa=0.05 prob_desejada1=alfa/2 df=8 t_desejado1=round(qt(prob_desejada1,df ),df) d_desejada1=dt(t_desejado1,df) prob_desejada2=1-alfa/2 df=8 t_desejado2=round(qt(prob_desejada2, df),df) d_desejada2=dt(t_desejado2,df) t_calculado=-2 d_calculado=dt(t_calculado,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado1, t_desejado2)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-2,306, 2,306)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(-\\U221e; -2,306)= P(2,306; \\U221e)= \\u03b1/2 em vermelho (nível de significância/2=0,025) &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = t_desejado2, y = 0, xend = t_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-0.1, y=d_desejada1, label=&quot;valor crítico=-2,306&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.3, y=d_desejada2, label=&quot;valor crítico=2,306&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+2, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = t_calculado, y = 0, xend = t_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-2.02&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.14: Regiões de rejeição da hipótese nula para o teste bilateral (tipo: diferente de) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valores críticos da estatística do teste: \\(t_{crit} =\\pm 2,306\\). O valor calculado da estatística (\\(t_{calc}=-2,02\\)) situa-se na faixa de significância do teste, possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos na análise estatística realizada não nos permitem rejeitar a hipótese de que a média populacional seja igual a 5 sob um nível de confiança de 95% (Figura 11.14). # Dados do problema n=9 media_amostral=4 var_amostral=2.2 media_populacao=5 alfa=0.05 # Estatística de teste t=(media_amostral - media_populacao) / sqrt(var_amostral / n) # Graus de liberdade df=n - 1 # Valor-p à esquerda p_valor_1=pt(-abs(t), df, lower.tail = TRUE) # Valor-p à direita p_valor_2=pt(abs(t), df, lower.tail = FALSE) # p-valor p_valor=p_valor_1+p_valor_2 # Ou p_valor &lt;- 2 * pt(-abs(t), df) # Decisão e conclusão if (p_valor &lt; alfa) { cat(&quot;Os dados amostrais trazidos à análise nos permitem rejeitar, sob o nível de significância estabelecido de&quot;, alfa ,&quot;de se cometer um erro do tipo I, a hipótese nula (H0) que afirma ser a média populacional igual a&quot;, media_populacao,&quot;.A média populacional é diferente.&quot;) } else { cat(&quot;Os dados amostrais trazidos à análise não nos permitem rejeitar, sob o nível de confiança de&quot;, 1-alfa ,&quot;,a hipótese nula (H0). A média populacional é igual a&quot;, media_populacao,&quot;.&quot;) } ## Os dados amostrais trazidos à análise não nos permitem rejeitar, sob o nível de confiança de 0.95 ,a hipótese nula (H0). A média populacional é igual a 5 . &gt; Teste unilateral à esquerda (tipo: menor que) \\[ \\begin{cases} H_{0}: \\mu \\ge 5\\\\ H_{1}: \\mu &lt; 5\\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a média igual ou maior a 5, fazendo então valer a hipótese alternativa que afirma ser a média menor que 5. Da tabela ``t’’ de Student obtemos o valor crítico monocaudal: \\(|{t}_{tab_(\\alpha, (n-1))}|=-1,86\\). Pelo cálculo a estatística do teste é \\(t_{calc}=-2,02\\). alfa=0.05 prob_desejada=alfa df=8 t_desejado=round(qt(prob_desejada,df ),4) d_desejada=dt(t_desejado,df) t_calculado=-2 d_calculado=dt(t_calculado,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-1,86, \\U221e)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(-\\U221e; -1,86)= \\u03b1 em vermelho (nível de significância=0,05) &quot;)+ geom_segment(aes(x = t_desejado, y = 0, xend = t_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado-0.1, y=d_desejada, label=&quot;valor crítico=-1,86&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado+1.5, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = t_calculado, y = 0, xend = t_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-2.02&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.15: Região de rejeição da hipótese nula para o teste unilateral à esquerda (tipo: menor que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(t_{crit} = -1,86\\). O valor calculado da estatística (\\(t_{calc}=-2,02\\)) situa-se na faixa de significância do teste possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: sob um nível de confiança de confiança de 95%, face aos dados trazidos à análise podemos rejeitar a hipótese de que a média seja de no mínimo a 5 (Figura 11.15). Caso estabelecêssemos um nível de confiança \\((1-\\alpha) \\ge 0,9611277\\) (ou tivéssemos uma informação amostral \\(\\stackrel{-}{x} \\ge 4.080639\\)), a hipótese nula não seria rejeitada: a média populacional é maior ou igual a 5. # Dados do problema n=9 media_amostral=4 var_amostral=2.2 media_populacao=5 alfa=0.05 # Estatística de teste t=(media_amostral - media_populacao) / sqrt(var_amostral / n) # Graus de liberdade df=n - 1 # Valor-p à esquerda p_valor=pt(t, df) # Decisão e conclusão if (p_valor &lt; alfa) { cat(&quot;Os dados amostrais trazidos à análise nos permitem rejeitar, sob o nível de significância estabelecido de&quot;, alfa ,&quot;de se cometer um erro do tipo I, a hipótese nula (H0) que afirma ser a média populacional maior ou igual a &quot;, media_populacao,&quot;.A média populacional é menor.&quot;) } else { cat(&quot;Os dados amostrais trazidos à análise não nos permitem rejeitar, sob o nível de confiança de&quot;, 1-alfa ,&quot;,a hipótese nula (H0). A média populacional é maior ou igual a&quot;, media_populacao,&quot;.&quot;) } ## Os dados amostrais trazidos à análise nos permitem rejeitar, sob o nível de significância estabelecido de 0.05 de se cometer um erro do tipo I, a hipótese nula (H0) que afirma ser a média populacional maior ou igual a 5 .A média populacional é menor. Teste unilateral à direita (tipo: maior que) \\[ \\begin{cases} H_{0}: \\mu \\le 5\\\\ H_{1}: \\mu &gt; 5\\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a média igual ou menor a 5, fazendo então valer a hipótese alternativa que afirma ser a média maior que 5. Da tabela ``t’’ de Student obtemos o valor crítico monocaudal: \\(|{t}_{tab_(\\alpha, (n-1))}|=1,86\\). Pelo cálculo a estatística do teste é \\(t_{calc}=-2,02\\). alfa=0.95 prob_desejada=alfa df=8 t_desejado=round(qt(prob_desejada,df ),4) d_desejada=dt(t_desejado,df) t_calculado=-2 d_calculado=dt(t_calculado,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(-4, t_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-\\U221e; 1,86)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(1,86; \\U221e)= \\u03b1 em vermelho (nível de significância=0,05) &quot;)+ geom_segment(aes(x = t_desejado, y = 0, xend = t_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado-3, y=0.1, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = t_calculado, y = 0, xend = t_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-2.02&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.16: Região de rejeição da hipótese nula para o teste unilateral à direita (tipo: maior que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(t_{crit} = 1,86\\). O valor calculado da estatística (\\(t_{calc}=-2,02\\)) situa-se na faixa de não significância do teste, não possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: sob um nível de confiança de confiança de 95%, face aos dados trazidos à análise não podemos rejeitar a hipótese de que a média seja inferior a 5 (Figura 11.16). Caso estabelecêssemos um nível de confiança \\((1-\\alpha) \\ge 0,9611277\\) (ou tivéssemos uma informação amostral \\(\\stackrel{-}{x} \\ge 5.919361\\)), a hipótese nula seria rejeitada: a média populacional é maior que 5. # Dados do problema n=9 media_amostral=4 var_amostral=2.2 media_populacao=5 alfa=0.95 # Estatística de teste t=(media_amostral - media_populacao) / sqrt(var_amostral / n) # Graus de liberdade df=n - 1 # Valor-p à direita p_valor=pt(-t, df) # Decisão e conclusão if (p_valor &lt; alfa) { cat(&quot;Os dados amostrais trazidos à análise nos permitem rejeitar, sob o nível de significância estabelecido de&quot;, alfa ,&quot;de se cometer um erro do tipo I, a hipótese nula (H0) que afirma ser a média populacional menor ou igual a&quot;, media_populacao,&quot;.A média populacional é maior que&quot;,media_populacao,&quot;.&quot; ) } else { cat(&quot;Os dados amostrais trazidos à análise não nos permitem rejeitar, sob o nível de confiança de&quot;, 1-alfa ,&quot;,a hipótese nula (H0). A média populacional é menor ou igual a&quot;, media_populacao,&quot;.&quot;) } ## Os dados amostrais trazidos à análise não nos permitem rejeitar, sob o nível de confiança de 0.05 ,a hipótese nula (H0). A média populacional é menor ou igual a 5 . "],["teste-de-hipóteses-para-as-médias-mu_1mu_2-de-duas-populações-normais-independentes.html", "11.9 Teste de hipóteses para as médias (\\(\\mu_{1};\\mu_{2}\\)) de duas populações Normais independentes", " 11.9 Teste de hipóteses para as médias (\\(\\mu_{1};\\mu_{2}\\)) de duas populações Normais independentes Figure 11.17: Visão esquemática das amostras de duas populações Pelo Teorema Limite Central, para tamanhos amostrais \\(n\\) suficientemente grandes a média amostral \\(\\stackrel{-}{X}\\) tem distribuição aproximadamente Normal, com média \\(\\mu\\) e variância \\(\\frac{\\sigma^{2}}{n}\\), independente da distribuição da população, onde \\(\\mu\\) e \\(\\sigma^{2}\\) são a média e a variância populacionais. grandes: \\(n \\geq 30 (40)\\); e pequenas: \\(n &lt; 30\\). Situações possíveis: Variâncias populacionais conhecidas ou não conhecidas mas com amostras de grande tamanho; Variâncias populacionais desconhecidas: Variâncias populacionais admitidas iguais; ou, Variâncias populacionais quaisquer. Os valores assumidos pelas características de nosso interesse nas populações são tais que: \\[ X_{1} \\sim \\mathcal{N}(\\mu_{1}; \\sigma^{2}_{1}) \\] e \\[ X_{2} \\sim \\mathcal{N}(\\mu_{2}; \\sigma^{2}_{2}) \\] Ao se extrair duas amostras, os valores amostrais assumidos por essas características serão duas variáveis aleatórias tais que: \\[ \\stackrel{-}{X}_{1} \\sim \\mathcal{N} (\\mu_{1}\\frac{\\sigma^{2}_{1}}{n_{1}}) \\] e \\[ \\stackrel{-}{X}_{2} \\sim \\mathcal{N} (\\mu_{2};\\frac{\\sigma^{2}_{2}}{n_{2}}). \\] É de nosso particular interesse definir uma variável aleatória expressa como a diferença das variáveis \\(\\stackrel{-}{X}_{1}\\) e \\(\\stackrel{-}{X}_{2}\\). Segue-se assim (por serem independentes) que \\[ \\stackrel{-}{X}_{1}-\\stackrel{-}{X}_{2} \\sim \\mathcal{N} (\\mu_{1}-\\mu_{2}; \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}}) . \\] 11.9.1 As estruturas possíveis dos testes de hipóteses relacionados às suas médias serão: Teste bilateral (tipo: diferente de) \\[ \\begin{cases} H_{0}:(\\mu_{1} - \\mu_{2}) = \\Delta_{0} \\\\ H_{1}:(\\mu_{1} - \\mu_{2}) \\ne \\Delta_{0} \\\\ \\end{cases} \\] Teste unilateral à esquerda (tipo: menor que) \\[ \\begin{cases} H_{0}: (\\mu_{1} - \\mu_{2}) \\ge \\Delta_{0}\\\\ H_{1}: (\\mu_{1} - \\mu_{2}) &lt; \\Delta_{0}\\\\ \\end{cases} \\] Teste unilateral à direita (tipo: maior que) \\[ \\begin{cases} H_{0}: (\\mu_{1} - \\mu_{2}) \\le \\Delta_{0}\\\\ H_{1}: (\\mu_{1} - \\mu_{2}) &gt; \\Delta_{0}\\\\ \\end{cases} \\] Os valores assumidos pelas diferenças amostrais são tais que: \\[ \\frac{(\\stackrel{-}{X}_{1}-\\stackrel{-}{X}_{2}) - \\Delta_{0}}{\\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}}}} \\sim \\mathcal{N} (0,1) \\] para amostras Normais: \\(n_{1}\\) e \\(n_{2}\\) qualquer; amostras sob outras distribuições, desde que: \\(n_{1}\\) e \\(n_{2} \\ge 30(40)\\): \\({Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}\\) ou \\({Z}_{tab\\left(\\alpha \\right)}\\): valores da distribuição Normal padronizada para o nível de significância pretendido no teste (bilateral ou unilateral); e, \\(Z_{calc} = \\frac{(\\stackrel{-}{X}_{1} - \\stackrel{-}{X}_{2})-\\Delta_{0}}{\\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}}+\\frac{\\sigma^{2}_{2}}{n_{2}}}} \\sim \\mathcal{N}(0,1)\\) em que: \\(\\Delta_{0}\\) é o valor inferido à diferença das médias populacionais \\(\\mu_{1}\\) e \\(\\mu_{2}\\), usualmente 0 (igualdade); \\(\\sigma_{1}^{2}\\) é a variância da população 1; \\(\\sigma_{2}^{2}\\) é a variância da população 2; \\(\\stackrel{-}{X}_{1}, n_{1}\\) são a média e o tamanho da amostra 1; e, \\(\\stackrel{-}{X}_{2}, n_{2}\\) são a média e o tamanho da amostra 2. 11.9.2 Testes de hipóteses para as médias de duas populações com variâncias conhecidas (ou não conhecidas mas o tamanho das amostras é grande) Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística Z (\\(Z \\sim \\mathcal{N}(0,1)\\)): Teste de hipóteses bilateral (tipo: diferente de): \\[\\begin{align*} P[\\left|Z_{calc}\\right| \\le {Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}|\\mu_{1}=\\mu_{2}] &amp; =(1-\\alpha)\\\\ P(-{Z}_{tab\\left(\\frac{\\alpha }{2}\\right)} \\le Z_{calc} \\le {Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}) &amp; = (1-\\alpha)\\\\ \\end{align*}\\] Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[\\begin{align*} P[Z_{calc} \\ge -{Z}_{tab\\left(\\alpha \\right)}|\\mu_{1} \\ge \\mu_{2}] &amp; =(1-\\alpha) \\\\ P( Z_{calc} \\ge -{Z}_{tab\\left(\\alpha \\right)}) &amp; = (1-\\alpha) \\\\ \\end{align*}\\] Teste de hipóteses unilateral à direita (tipo maior que): \\[\\begin{align*} P[Z_{calc} \\le {Z}_{tab\\left(\\alpha \\right)}|\\mu_{1} \\le \\mu_{2}] &amp; =(1-\\alpha) \\\\ P( Z_{calc} \\le {Z}_{tab\\left(\\alpha \\right)}) &amp; = (1-\\alpha) \\\\ \\end{align*}\\] Nas figuras 11.8, 11.9 e 11.10 observam-se:   as regiões de rejeição da hipótese nula (subdivididas nos dois ou em apenas um dos lados) sob a curva da função densidade de probabilidade da distribuição adequada ao teste com probabilidades iguais ao nível de significância \\(\\alpha\\) ; a região de não rejeição da hipótese nula (delimitada à esquerda e à direita ou apenas em um dos lados) com probabilidade igual ao nível de confiança \\((1-\\alpha)\\); e, os valores críticos da estatística do teste. Exemplo: Duas máquinas são usadas para encher garrafas plásticas com um volume líquido de 16oz. Os volumes de enchimento podem ser admitidos como normais, tendo desvios padrão iguais a \\(\\sigma_{1}=0,020\\)oz e \\(\\sigma_{2}=0,025\\)oz. O departamento de engenharia da fábrica deseja saber a um nível de significância de \\(\\alpha=0,01\\) se ambas as máquinas enchem um mesmo volume e para isso coletou uma amostra de 10 garrafas enchidas por cada uma das máquinas cf. tabela abaixo: Enchimento de duas máquinas Máquina 01 Máquina 02 16,03 16,01 16,02 16,03 16,04 15,96 15,97 16,04 16,05 15,98 15,96 16,02 16,05 16,02 16,01 16,01 16,02 15,99 15,99 16,00 As variâncias populacionais \\(\\sigma_{1}^{2}\\) e \\(\\sigma_{2}^{2}\\) são conhecidas e as populações seguem uma distribuição Normal. A estatística do teste é: \\[ Z_{calc} = \\frac{(\\stackrel{-}{X}_{1} - \\stackrel{-}{X}_{2}) }{\\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}}+\\frac{\\sigma^{2}_{2}}{n_{2}}}} \\] tal que tal que Z (\\(Z \\sim \\mathcal{N}(0,1)\\)), em que: \\(\\mu_{1} , \\mu_{2}\\) são as médias das populações em teste; \\(\\sigma_{1}^{2}=0,020^{2}, \\sigma_{2}^{2}=0,025^{2}\\) são as variâncias das populações em teste; \\(\\stackrel{-}{x}_{1}=16,015, n_{1}=10\\) são a média e o tamanho da amostra 1; \\(\\stackrel{-}{x}_{2}=16,005, n_{2}=10\\) são a média e o tamanho da amostra 2; e, o nível de significância estabelecido para o teste é \\(\\alpha=0,01\\). O problema nos pede um teste bilateral (tipo: diferente de): \\[ \\begin{cases} H_{0}: (\\mu_{1} - \\mu_{2}) = 0 \\\\ H_{1}: (\\mu_{1} - \\mu_{2}) \\ne 0 \\\\ \\end{cases} \\] Se \\(z_{calc}\\) for tal que: \\[ -{z}_{tab\\left(\\frac{\\alpha }{2}\\right)} \\le z_{calc} \\le {z}_{tab\\left(\\frac{\\alpha }{2}\\right)} \\] não se rejeita a hipótese nula sob o nível de signficância estabelecido. Da tabela da distribuição Normal padronziada obtemos o valor crítico bicaudal: \\(|{Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}|=2,57\\). Pelo cálculo, a estatística do teste é \\(z_{calc}=0,98773\\). alfa=0.01 prob_desejada1=alfa/2 z_desejado1=round(qnorm(prob_desejada1),4) d_desejada1=dnorm(z_desejado1, 0, 1) prob_desejada2=1-alfa/2 z_desejado2=round(qnorm(prob_desejada2),4) d_desejada2=dnorm(z_desejado2, 0, 1) z_calculado=0.98773 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de z&quot;, breaks = c(z_desejado1,z_desejado2)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-2,57, 2,57)=(1-\\u03b1) em cinza (nível de confiança=0,99) \\nP(-\\U221e; -2,57)= P(2,57; \\U221e)= \\u03b1/2 em vermelho (nível de significância/2=0,005) &quot;)+ geom_segment(aes(x = z_desejado1, y = 0, xend = z_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = z_desejado2, y = 0, xend = z_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-0.1, y=d_desejada1, label=&quot;valor crítico=-2,57&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.3, y=d_desejada2, label=&quot;valor crítico=2,57&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-1.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+2, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=0,9877&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.18: Regiões de rejeição da hipótese nula para o teste bilateral (tipo: diferente de) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valores críticos da estatística do teste: \\(z_{crit} =\\pm 2,57\\). O valor calculado da estatística (\\(z_{calc}=0,987\\)) não nos possibilita a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos pela análise estatística de comparação de médias das duas amostras colhidas de garrafas de plástico enchidas por duas máquinas diferentes \\(1\\) e \\(2\\) não nos permitem rejeitar a hipótese de que suas médias sejam iguais sob um nível de confiança de 99% (Figura 11.18). Podemos ainda realizar testes de hipóteses para as diferenças entre as médias observadas (\\(\\mu_{1}&lt;\\mu_{2}\\) ou \\(\\mu_{1}&gt;\\mu_{2}\\)). As conclusões derivadas desses testes deverão indicar que as médias não diferem entre si ao nível de significância dos testes chegando assim, por outras vias (agora não se rejeitando a hipótese nula), à mesma conclusão do teste de igualdade das médias antes realizado. Teste unilateral à esquerda (tipo: menor que) Nessa situação postula-se que a diferença da média 1 para a média 2 é no mínimo 0 (o que equivale dizer que a média 1 é no mínimo igual à média 2): \\[ \\begin{cases} H_{0}: (\\mu_{1} - \\mu_{2}) \\ge 0 \\\\ H_{1}: (\\mu_{1} - \\mu_{2}) &lt; 0 \\end{cases} \\] Da tabela da distribuição Normal padronizada obtemos o valor crítico monocaudal: \\({Z}_{tab\\left(\\alpha \\right)}=-2,33\\). Pelo cálculo, a estatística do teste é \\(Z_{calc}=0,98773\\). alfa=0.01 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) z_calculado=0.98773 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de z&quot;, breaks = c(z_desejado)) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P( -2,33,\\U221e,)=(1-\\u03b1) em cinza (nível de confiança=0,99) \\nP(-\\U221e; -2,33)=\\u03b1 em vermelho (nível de significância=0,01) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado-0.1, y=d_desejada, label=&quot;valor crítico=-2,33&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado+1, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=0,98773&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.19: Regiões de rejeição da hipótese nula para o teste unilateral à esquerda (tipo: menor que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valor crítico da estatística do teste: \\(z_{crit}=-2,33\\). O valor calculado da estatística (\\(z_{calc}=0,98773\\)) não nos possibilita a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos pela análise estatística de comparação de médias das duas amostras colhidas de garrafas de plástico enchidas por duas máquinas diferentes \\(1\\) e \\(2\\) não nos permitem rejeitar a hipótese de que a média de enchimento da máquina 1 seja no mínimo igual à da máquina 2 sob um nível de confiança de 99% (Figura 11.19). Teste unilateral à direita (tipo: maior que) Nessa situação postula-se que a diferença da média 1 para a média 2 é no máximo 0 (o que equivale dizer que a média 1 é no máximo igual à média 2): \\[ \\begin{cases} H_{0}: (\\mu_{1} - \\mu_{2}) \\le 0 \\\\ H_{1}: (\\mu_{1} - \\mu_{2}) &gt; 0 \\\\ \\end{cases} \\] Da tabela da distribuição Normal padronizada obtemos o valor crítico monocaudal: \\({Z}_{tab\\left(\\alpha \\right)}=-2,33\\). Pelo cálculo, a estatística do teste é \\(Z_{calc}=0,98773\\). alfa=0.99 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) z_calculado=0.98773 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-4, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de z&quot;, breaks = c(z_desejado)) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(- \\U221e; 2,33)=(1-\\u03b1) em cinza (nível de confiança=0,99) \\nP(2,33 ; \\U221e)=\\u03b1 em vermelho (nível de significância=0,01) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado-0.1, y=d_desejada, label=&quot;valor crítico=-1,88&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-3, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=0,98773&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.20: Região de rejeição da hipótese nula para o teste unilateral à direita (tipo: maior que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(z_{crit} = 2,33\\). O valor calculado da estatística (\\(z_{calc}=0,98773\\)) não nos possibilita a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos pela análise estatística de comparação de médias das duas amostras colhidas de garrafas de plástico enchidas por duas máquinas diferentes \\(1\\) e \\(2\\) não nos permitem rejeitar a hipótese de que a média de enchimento da máquina 1 seja no máximo igual à da máquina 2 sob um nível de confiança de 99% (Figura 11.20). Pelo teste unilateral à esquerda concluiu-se que \\(\\mu_{1} \\ge \\mu_{2}\\); pelo teste unilateral à direita conclui-se que \\(\\mu_{1} \\le \\mu_{2}\\). Sob o nível de significânca estabelecido conclui-se que \\(\\mu_{1} = \\mu_{2}\\). 11.9.3 Testes de hipóteses para as médias de duas populações Normais independentes com variâncias desconhecidas mas iguais: teste “t’’ homocedástico (\\(\\sigma_{1}^{2}=\\sigma_{2}^{2}=?\\)) Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística t (\\(T \\sim t_{(n_{1} + n_{2} - 2)}\\)). Os valores assumidos pelas diferenças amostrais são tais que \\[ T = \\frac{(\\stackrel{-}{X}_{1} - \\stackrel{-}{X}_{2})-\\Delta_{0}} {S_{c} \\cdot \\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}} \\sim t_{(n_{1} + n_{2} - 2)} \\] em que: \\(\\Delta_{0}\\) usualmente é 0 (igualdade); \\(\\sigma_{1}^{2} = \\sigma_{2}^{2} = \\sigma^{2}\\) são as variâncias populacionais desconhecidas, mas admitidas iguais (homogêneas); \\(\\stackrel{-}{X}_{1}, S_{1}^{2}, n_{1}\\) são a média, a variância e o tamanho referentes à amostra 1; \\(\\stackrel{-}{X}_{2}, S_{2}^{2}, n_{2}\\) são a média, a variância e o tamanho referentes à amostra 2; e, \\(S_{c}^{2}\\) é a variância conjunta ou ponderada. Condições: amostras Normais (\\(n_{1}\\) e \\(n_{2}\\) qualquer); amostras sob outras distribuições (desde que \\(n_{1}\\) e \\(n_{2}\\) \\(\\ge 30\\)); a utilização da estatística ``t’’ para \\(n_{1}\\) e \\(n_{2} \\ge 30\\) apenas pressupõe que \\(S_{c}\\) e seja um estimador suficientemente bom para \\(\\sigma_{i}\\); e, \\({t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)}\\) ou \\({t}_{tab\\left(\\alpha ;{n}_{1}+{n}_{2}-2\\right)}\\): o quantil associado na distribuição ``t’’ de Student ao nível de significância pretendido no teste, com \\(({n}_{1}+{n}_{2}-2)\\) graus de liberdade. A variância conjunta (ou variância ponderada) \\(S_{c}^{2}\\) a ser utilizada no cálculo da estatística do teste é definida como: \\[ S_{c}^{2} = \\frac{\\left({n}_{1}-1\\right)\\cdot {S}_{1}^{2}+\\left({n}_{2}-1\\right)\\cdot {S}_{2}^{2}}{{n}_{1}+{n}_{2}-2} \\] Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística t (T \\(\\sim t_{(n_{1} + n_{2} - 2)}\\)) Teste de hipóteses bilateral (tipo: diferente de): \\[\\begin{align*} P[\\left|t_{calc}\\right| \\ge {t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)}|\\mu_{1}=\\mu_{2}] &amp; =(1-\\alpha) \\\\ P(- {t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)} \\le t_{calc} \\le {t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)}) &amp; =(1-\\alpha)\\\\ \\end{align*}\\] Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[\\begin{align*} P[t_{calc} \\ge -{t}_{tab\\left(\\alpha \\right)}|\\mu_{1} \\ge \\mu_{2}] &amp; = (1-\\alpha) \\\\ P( t_{calc} \\ge -{t}_{tab\\left(\\alpha;{n}_{1}+{n}_{2}-2\\right)} ) &amp; = (1-\\alpha) \\\\ \\end{align*}\\] Teste de hipóteses unilateral à direita (tipo: maior que): \\[\\begin{align*} P[t_{calc} \\le {t}_{tab\\left(\\alpha \\right)}|\\mu_{1} \\le \\mu_{2}] &amp; =(1-\\alpha) P( t_{calc} \\le {t}_{tab\\left(\\alpha;{n}_{1}+{n}_{2}-2\\right)}) &amp; = (1-\\alpha) \\end{align*}\\] Nas figuras 11.8, 11.9 e 11.10 observam-se:   as regiões de rejeição da hipótese nula (subdivididas nos dois ou em apenas um dos lados) sob a curva da função densidade de probabilidade da distribuição adequada ao teste com probabilidades iguais ao nível de significância \\(\\alpha\\) ; a região de não rejeição da hipótese nula (delimitada à esquerda e à direita ou apenas em um dos lados) com probabilidade igual ao nível de confiança \\((1-\\alpha)\\); e, os valores críticos da estatística do teste. 11.9.4 Teste de hipóteses para a razão de duas variâncias (\\(\\frac{\\sigma_{1}^{2}}{\\sigma_{2}^{2}}\\)) Para se verificar se a consideração de igualdade das variâncias é estatisticamente sustentável pode-se recorrer ao teste ``F’’ de sua razão. Estrutura do teste: \\[ \\begin{cases} H_{0}: (\\sigma_{1}^{2}-\\sigma_{2}^{2})=\\Delta \\\\ H_{1}: (\\sigma_{1}^{2} - \\sigma_{2}^{2}) \\ne \\Delta \\end{cases} \\] em que, usualmente, \\(\\Delta=0\\) (igualdade). A estatística do teste: \\(F_{calc}\\), será dada por: \\[ f_{calc} = \\left(\\frac{{S}_{1}^{2}}{{S}_{2}^{2}}\\right)\\cdot \\left(\\frac{{\\sigma }_{1}^{2}}{{\\sigma }_{2}^2}\\right ) \\sim F_{[(n_{1} -1), (n_{2} -1)]}, \\] lembrando que a pressuposição da igualdade estabeleciada na hipótese nula (\\(H_{0}\\)) leva a \\(\\left(\\frac{{\\sigma }_{1}^{2}}{{\\sigma}_{2}^2}\\right)=1\\). A Hipótese nula será rejeitada se: \\[ f_{calc} \\ge f_{[(n_{1} -1), (n_{2} -1), (1-\\frac{\\alpha}{2})]} \\] ou \\[ f_{calc} \\le f_{[(n_{1} -1), (n_{2} -1), (\\frac{\\alpha}{2})]} \\] em que \\({f}_{({n}_{1}-1),({n}_{2}-1)}\\) são os quantis de ordem \\(\\frac{\\alpha}{2}\\) e \\((1-\\frac{\\alpha}{2})\\), respectivamente os valores limite pelo lado esquerdo e pelo lado direito da curva da função densidade da Distribuição F (Ronald Fisher e George Waddel Snedecor) com graus de liberdade: \\((n_{1}-1)\\) são os graus de liberdade (GL) no numerador e \\((n_{2}-1)\\) são os graus de liberdade (GL) no denominador (em concordância com a razão adotada: \\(\\frac{S_{1}}{S_{2}}\\)). Como as tabelas costumam mostrar apenas \\(f_{[(n_{1} -1), (n_{2} -1), ( 1-\\frac{\\alpha}{2})]}\\) (o valor limite superior do intervalo), para a determinação do valor limite inferior (\\(f_{[(n_{1} -1), (n_{2} -1), (\\frac{\\alpha}{2})]}\\)) torna-se interessante relembrar a propriedade: \\[ {f}_{[({n}_{1}-1),({n}_{2}-1), (\\frac{\\alpha}{2})]} = \\frac{1}{ {f}_{[({n}_{2}-1),({n}_{1}-1), (1-\\frac{\\alpha}{2})]} }. \\] Considera eo exemplo: -vamos fazer o teste de hipóteses bilateral (pois estamos testando se são iguais ou não) a um nível de significância \\(\\alpha=0,05\\) (assim, teremos \\(\\frac{\\alpha}{2}=0,025\\) à esquerda e \\(\\frac{\\alpha}{2}=0,025\\) à direita; -admitamos que a razão das variâncias amostrais seja feita na ordem \\(\\frac{S_{1}^{2}}{S_{2}^{2}}\\) e que as mesmas tenham sido calculadas a partir das amostras \\(n_{1}=10\\) e \\(n_{2}=9\\); -assim, os graus de liberdade do numerador são: \\(GL_{numerador}=9\\) e (\\(GL_{denominador}=8\\); -na tabela para \\(\\alpha=0,025\\), temos o valor de 4,357; -esse é o valor \\(f_{[9; 8; 0,975]}\\) (o limitante superior da região crítica); -para achar o valor \\({f}_{[9; 8; 0,025]}\\) (o limitante inferior da região crítica) tomemos o valor de \\(f_{[8; 9; 0,975]}=4,102\\) e o seu inverso será \\({f}_{[9; 8; 0,025]}=\\frac{1}{4,102}=0,243\\). Regiões de rejeição da hipótese nula (Figura 11.21): # Parâmetros prob_desejada1 = 0.025 prob_desejada2 = 0.975 df1 = 9 df2 = 8 # Cálculo dos valores críticos f_desejado1 = round(qf(prob_desejada1, df1, df2), 4) f_desejado2 = round(qf(prob_desejada2, df1, df2), 4) # Densidades nos valores críticos d_desejada1 = df(f_desejado1, df1, df2) d_desejada2 = df(f_desejado2, df1, df2) # Gráfico plot = ggplot(data.frame(x = c(0, 6)), aes(x)) + # Curva da distribuição F stat_function(fun = function(x) df(x, df1, df2), geom = &quot;line&quot;, color = &quot;black&quot;) + # Região de rejeição à esquerda (Vermelha) stat_function(fun = function(x) df(x, df1, df2), geom = &quot;area&quot;, fill = &quot;red&quot;, alpha = 0.5, xlim = c(0, f_desejado1)) + # Região de não rejeição (Cinza) stat_function(fun = function(x) df(x, df1, df2), geom = &quot;area&quot;, fill = &quot;lightgrey&quot;, alpha = 0.5, xlim = c(f_desejado1, f_desejado2)) + # Região de rejeição à direita (Vermelha) stat_function(fun = function(x) df(x, df1, df2), geom = &quot;area&quot;, fill = &quot;red&quot;, alpha = 0.5, xlim = c(f_desejado2, 6)) + # Linhas verticais nos valores críticos annotate(&quot;segment&quot;, x = f_desejado1, y = 0, xend = f_desejado1, yend = d_desejada1, color = &quot;blue&quot;, linetype = &quot;dashed&quot;, size = 0.5) + annotate(&quot;segment&quot;, x = f_desejado2, y = 0, xend = f_desejado2, yend = d_desejada2, color = &quot;blue&quot;, linetype = &quot;dashed&quot;, size = 0.5) + # Rótulos nos valores críticos annotate(&quot;text&quot;, x = f_desejado1 + 0.2, y = 0.2, label = &quot;f crítico 1&quot;, angle = 90, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 4) + annotate(&quot;text&quot;, x = f_desejado2 - 0.2, y = 0.2, label = &quot;f crítico 2&quot;, angle = 90, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 4) + # Rótulos das zonas de rejeição e não rejeição annotate(&quot;text&quot;, x = f_desejado1 + 1, y = 0.4, label = &quot;Zona de não rejeição \\n(para f calculado)&quot;, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 3) + annotate(&quot;text&quot;, x = f_desejado2 + 1, y = 0.2, label = &quot;Zona de rejeição \\n(para f calculado)&quot;, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 3) + annotate(&quot;text&quot;, x = f_desejado1 - 1, y = 0.2, label = &quot;Zona de rejeição \\n(para f calculado)&quot;, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 3) + # Ajuste dos eixos scale_y_continuous(name = &quot;Densidade&quot;) + scale_x_continuous(name = &quot;Valores score (f)&quot;, limits = c(0, 6)) + # Títulos e tema labs(title = &quot;Curva da função densidade \\nDistribuição F&quot;, subtitle = &quot;P(f crítico 1, f crítico 2)=(1-\\u03b1) em cinza (nível de confiança) \\nP(0; f crítico 1)= P(f crítico 2; ∞)= \\u03b1/2 em vermelho (nível de significância/2)&quot;) + theme_bw() # Exibir gráfico print(plot) Figure 11.21: Regiões de rejeição da hipótese nula para o teste bilateral (tipo: diferente de) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valores críticos da estatística do teste: \\(f_{crit1}\\) e \\(f_{crit2}\\) para o nível de significância pretendido (\\(\\alpha\\) dividido em ambas as caudas) e (\\(df_{1}; df_{2}\\)) graus de liberdade. A curva não é simétrica e assim, os valores críticos são diferentes Uma regra prática permite reverter o teste bilateral em um teste unilateral à direita se tomarmos o maior valor (\\(f_{calc}\\) maior que 1, portanto) de \\(f_{calc}\\) dentre as possíveis razões: \\[ f_{calc} = (\\frac{{S}_{1}^{2}}{{S}_{2}^{2}})\\cdot (\\frac{{\\sigma }_{1}^{2}}{{\\sigma }_{2}^2}) \\sim F(_{(n_{1} -1), (n_{2} -1))} \\] ou \\[ f_{calc} = (\\frac{{S}_{2}^{2}}{{S}_{1}^{2}})\\cdot (\\frac{{\\sigma }_{2}^{2}}{{\\sigma }_{1}^2}) \\sim F(_{(n_{2} -1), (n_{1} -1))} \\] em que: \\({F}_{tab\\left(\\alpha ,{n}_{1}-1,{n}_{2}-1\\right)}\\) é o quantil de ordem \\(\\alpha\\) da Distribuição ``F’’ (Ronald Fisher e George Waddel Snedecor) com graus de liberdade \\((n_{1}-1)\\) no numerador e \\((n_{2}-1)\\) no denominador (em concordância com a razão utilizada: \\(\\frac{S_{1}}{S_{2}}\\)); ou, \\((n_{2}-1)\\) são os graus de liberdade (GL) no numerador e \\((n_{1}-1)\\) são os graus de liberdade (GL) no denominador (em concordância com a razão utilizada: \\(\\frac{S_{2}}{S_{1}}\\)). Região de rejeição da hipótese nula (Figura 11.22): # Parâmetros prob_desejada1 = 0.95 df1 = 9 df2 = 8 # Cálculo do valor crítico f_desejado1 = round(qf(prob_desejada1, df1, df2), 4) # Densidade no valor crítico d_desejada1 = df(f_desejado1, df1, df2) # Gráfico plot = ggplot(data.frame(x = c(0, 6)), aes(x)) + # Curva da distribuição F stat_function(fun = function(x) df(x, df1, df2), geom = &quot;line&quot;, color = &quot;black&quot;) + # Região de não rejeição à esquerda (Cinza) stat_function(fun = function(x) df(x, df1, df2), geom = &quot;area&quot;, fill = &quot;lightgray&quot;, alpha = 0.5, xlim = c(0, f_desejado1)) + # Região de rejeição à direita (Vermelha) stat_function(fun = function(x) df(x, df1, df2), geom = &quot;area&quot;, fill = &quot;red&quot;, alpha = 0.5, xlim = c(f_desejado1, 6)) + # Linhas verticais nos valores críticos annotate(&quot;segment&quot;, x = f_desejado1, y = 0, xend = f_desejado1, yend = d_desejada1, color = &quot;blue&quot;, linetype = &quot;dashed&quot;, size = 0.5) + # Rótulos nos valores críticos annotate(&quot;text&quot;, x = f_desejado1 + 0.2, y = 0.2, label = &quot;f crítico 1&quot;, angle = 90, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 4)+ # Rótulos das zonas de rejeição e não rejeição annotate(&quot;text&quot;, x = f_desejado1 + 1, y = 0.4, label = &quot;Zona de rejeição \\n(para f calculado)&quot;, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 3) + annotate(&quot;text&quot;, x = f_desejado1 - 1, y = 0.2, label = &quot;Zona de não rejeição \\n(para f calculado)&quot;, vjust = 0, hjust = 0, color = &quot;blue&quot;, size = 3) + # Ajuste dos eixos scale_y_continuous(name = &quot;Densidade&quot;) + scale_x_continuous(name = &quot;Valores score (f)&quot;, limits = c(0, 6)) + # Títulos e tema labs(title = &quot;Curva da função densidade \\nDistribuição F&quot;, subtitle = &quot;P(f crítico 1, f crítico 2)=(1-\\u03b1) em cinza (nível de confiança) \\nP(0; f crítico 1)= P(f crítico 2; ∞)= \\u03b1/2 em vermelho (nível de significância/2)&quot;) + theme_bw() # Exibir gráfico print(plot) Figure 11.22: Região de rejeição da hipótese nula para o teste uniletaral à direita (tipo: menor que): a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(f_{crit}\\) para o nível de significância pretendido (\\(\\alpha\\) em uma cauda) e (\\(df_{1}; df_{2}\\)) graus de liberdade. Exemplo: A Secretaria de Educação de um município deseja saber se o desempenho dos alunos de duas diferentes escolas municipais na disciplina de matemática pode ser considerado igual a um nível de significância de \\(\\alpha=0,05\\). Verifique antes de as variâncias são . Para tanto ministrou um mesmo teste a 10 alunos de cada uma delas e obteve os seguintes notas: Notas em matemática de duas escolas Escola 01 Escola 02 78 83 85 79 84 79 75 88 81 75 83 94 78 85 87 87 76 81 80 82 Teste de hipóteses para a igualdade das variâncias: \\[ \\begin{cases} H_{0}: (\\sigma_{1}^{2}-\\sigma_{2}^{2})=\\Delta \\\\ H_{1}: (\\sigma_{1}^{2} - \\sigma_{2}^{2}) \\ne \\Delta \\end{cases} \\] em que, usualmente, \\(\\Delta=0\\) (igualdade). Se \\(\\sigma_{1}^{2}=\\sigma_{2}^{2}\\), então \\(\\frac{\\sigma_{1}^{2}}{\\sigma_{2}^{2}}=1\\). \\[ F_{cal}=\\frac{{S}_{2}^{2}}{{S}_{1}^{2}}\\cdot \\frac{{\\sigma }_{2}^{2}}{{\\sigma }_{1}^2}=2,56 \\] \\[ F_{critico\\left(\\alpha ,{n}_{1}-1,{n}_{2}-1\\right)} = F_{tab\\left(5\\% ,9,9\\right)} = 3,18 \\] prob_desejada1=0.95 df1=9 df2=9 f_desejado1=round(qf(prob_desejada1,df1, df2), 4) d_desejada1=df(f_desejado1,df1, df2) f_calculado=2.56 d_calculado=df(f_calculado,df1, df2) f_test_3=ggplot(data.frame(x = c(0, 6)), aes(x)) + stat_function(fun = df, geom = &quot;area&quot;, fill = &quot;lightgrey&quot;, xlim = c(0,f_desejado1), colour=&quot;black&quot;, args = list( df1 = df1, df2 = df2 ))+ stat_function(fun = df, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(f_desejado1,6), colour=&quot;black&quot;, args = list( df1 = df1, df2 = df2 ))+ scale_y_continuous(name=&quot;Densidade&quot;) + #scale_x_continuous(name=&quot;Valores score (f)&quot;, breaks = c(f_desejado1, f_desejado2))+ scale_x_continuous(name=&quot;Valores score (f)&quot;)+ labs(title=&quot;Curva da função densidade \\nDistribuição F&quot;, subtitle = &quot;P(0; 3,18 1)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(3,18 ; \\U221e)= \\u03b1 em vermelho (nível de significância=0,05) &quot;)+ geom_segment(aes(x = f_desejado1, y = 0, xend = f_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=f_desejado1+0.1, y=d_desejada1, label=&quot;F crítico 1=3,18&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ annotate(geom=&quot;text&quot;, x=f_desejado1+1, y=d_desejada1, label=&quot;Zona de rejeição \\n(para F calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=f_desejado1-2, y=d_desejada1, label=&quot;Zona de não rejeição \\n(para F calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = f_calculado, y = 0, xend = f_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=f_calculado+0.1, y=d_calculado, label=&quot;f calculado=2,56&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ theme_bw() O valor calculado da estatística de teste (\\(F_{calc}=2,56\\)) situa-se na região não significante do teste, não permitindo a rejeição da hipótese nula de que as variâncias sejam iguais sob o nível de confiança estabelecido. Não se pode rejeitar a hipótese de que as variâncias sejam iguais a um nível de significância de 5% (Figura 11.23). Figure 11.23: O valor calculado da estatística de teste (\\(F_{calc}=2,56\\)) situa-se na região não significante do teste, não permitindo a rejeição da hipótese nula de que as variâncias são iguais sob o nível de confiança estabelecido. Estrutura do teste: \\[ \\begin{cases} H_{0}: (\\mu_{1} - \\mu_{2}) = 0 \\\\ H_{1}: (\\mu_{1} - \\mu_{2}) \\ne 0 \\\\ \\end{cases} \\] Variâncias populacionais desconhecidas mas estatisticamente iguais. Nada se sabe sobre a distribuição da população e amostras de reduzido tamanho. \\[ S_{c}^{2} = \\frac{\\left({n}_{1}-1\\right)\\cdot {S}_{1}^{2}+\\left({n}_{2}-1\\right)\\cdot {S}_{2}^{2}}{{n}_{1}+{n}_{2}-2} \\] é a variância conjunta ponderada, em que: \\(\\mu_{1} , \\mu_{2}\\) são as médias das populações em teste; \\(\\sigma_{1}^{2}=\\sigma_{2}^{2}=\\sigma^{2}\\) são as variâncias das populações em teste, desconhecidas e estatisticamente iguais; \\(\\stackrel{-}{x}_{1}=80, S_{1}^{2}= 3,366^{2} , n_{1}=10\\) são a média, a variância e o tamanho referentes à amostra 1; \\(\\stackrel{-}{x}_{2}=84, S_{2}^{2}= 5,395^{2} , n_{2}=10\\) são a média, a variância e o tamanho referentes à amostra 2; \\({t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)}\\): o quantil associado na distribuição ``t’’ de Student ao nível de significância pretendido no teste, com \\(({n}_{1}+{n}_{2}-2)\\) graus de liberdade. \\[\\begin{align*} S_{c}^{2} &amp; = 20,2180\\\\ S_{c} &amp; = 4,4964 \\end{align*}\\] Estatística do teste: \\[ T_{calc} = \\frac{(\\stackrel{-}{X}_{1} - \\stackrel{-}{X}_{2})} {S_{c} \\cdot \\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}} \\] \\[ t_{cal}= -1,9892 \\] Teste bilateral: \\[ {t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)} &lt; t_{calc} &lt; {t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)} \\] \\[ |{t}_{tab\\left(\\frac{\\alpha }{2};{n}_{1}+{n}_{2}-2\\right)}|=|{t}_{tab\\left(2.5\\%;18\\right)}|=2,101 \\] alfa=0.05 prob_desejada1=alfa/2 df=8 t_desejado1=round(qt(prob_desejada1,df ),df) d_desejada1=dt(t_desejado1,df) prob_desejada2=1-alfa/2 df=8 t_desejado2=round(qt(prob_desejada2, df),df) d_desejada2=dt(t_desejado2,df) t_calculado=-1.9892 d_calculado=dt(t_calculado,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado1, t_desejado2)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-2,101, 2,101)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(-\\U221e; -2,101)= P(2,101; \\U221e)= \\u03b1/2 em vermelho (nível de significância/2=0,025) &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = t_desejado2, y = 0, xend = t_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-0.1, y=d_desejada1, label=&quot;valor crítico=-2,101&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.3, y=d_desejada2, label=&quot;valor crítico=2,101&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+2, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = t_calculado, y = 0, xend = t_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-1,9892&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.24: Regiões de rejeição da hipótese nula para o teste bilateral (tipo: diferente de) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valores críticos da estatística do teste: \\(t_{crit} =\\pm 2,101\\). O valor calculado da estatística (\\(t_{calc}=-1,9892\\)) situa-se na faixa de não significância do teste, impossibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos pela análise estatística de comparação de médias das duas amostras colhidas das notas de testes de matemáticas realizados em duas escolas diferentes (escola 1 e escola 2) não nos permitem rejeitar a hipótese de que suas médias sejam iguais a um nível de confiança de 5% (Figura 11.24). 11.9.5 Teste de hipóteses para as médias de duas populações Normais independentes com variâncias desconhecidas e desiguais: teste “``t’’ heterocedástico (\\(\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}=?\\)) Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística t (\\(T \\sim t_{\\nu}\\)). Os valores assumidos pelas diferenças amostrais são tais que \\[ T_{calc} = \\frac{(\\stackrel{-}{X}_{1} - \\stackrel{-}{X}_{2})-\\Delta_{0}}{ \\sqrt{\\frac{S_{1}^{2}}{n_{1}}+\\frac{S_{2}^{2}}{n_{2}}}} \\sim t_{\\nu} \\] em que: \\(\\Delta_{0}\\) usualmente é 0 (igualdade); \\(\\stackrel{-}{X}_{1}, S_{1}^{2}, n_{1}\\) são a média, a variância e o tamanho referentes à amostra 1; \\(\\stackrel{-}{X}_{2}, S_{2}^{2}, n_{2}\\) são a média, a variância e o tamanho referentes à amostra 2; e, a aproximação dos graus de liberdade (\\(\\nu\\)) é dada por uma combinação linear de variâncias de amostras independentes (Welch-Satterhwaite, 1946) \\[ \\nu=\\frac{{\\left(\\frac{{S}_{1}^{2}}{{n}_{1}}+\\frac{{S}_{2}^{2}}{{n}_{2}}\\right)}^{2}}{\\frac{{\\left(\\frac{{S}_{1}^{2}}{{n}_{1}}\\right)}^{2}}{{n}_{1}-1}+\\frac{{\\left(\\frac{{S}_{2}^{2}}{{n}_{2}}\\right)}^{2}}{{n}_{2}-1}} \\] Condições: amostras Normais (\\(n_{1}\\) e \\(n_{2}\\) qualquer); amostras sob outras distribuições (desde que \\(n_{1}\\) e \\(n_{2}\\) \\(\\ge 30\\)); \\({t}_{tab\\left(\\frac{\\alpha }{2};\\nu\\right)}\\) ou \\({t}_{tab\\left(\\alpha ;\\nu\\right)}\\): o quantil associado na distribuição ``t’’ de Student ao nível de significância pretendido no teste, com \\(\\nu\\) graus de liberdade. Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística t (T \\(\\sim t_{\\nu}\\)) Teste de hipóteses bilateral (tipo: diferente de): \\[\\begin{align*} P[\\left|t_{calc}\\right| \\ge {t}_{tab\\left(\\frac{\\alpha }{2};\\nu \\right)} |\\mu_{1} = \\mu_{2} ] &amp; = (1-\\alpha) \\\\ P( - {t}_{tab\\left(\\frac{\\alpha }{2};\\nu \\right)} \\le t_{calc} \\le {t}_{tab\\left(\\frac{\\alpha }{2};\\nu \\right)} ) &amp; = (1-\\alpha) \\\\ \\end{align*}\\] Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[\\begin{align*} P[t_{calc} \\ge {t}_{tab \\left(\\alpha ;\\nu \\right)} |\\mu_{1} \\ge \\mu_{2}] &amp; = (1-\\alpha) \\\\ P(t_{calc} \\ge {t}_{tab \\left(\\alpha ;\\nu \\right)}) &amp; = (1-\\alpha) \\\\ \\end{align*}\\] Teste de hipóteses unilateral à direita (tipo: maior que): \\[\\begin{align*} P[t_{calc} \\le {t}_{tab \\left(\\alpha ;\\nu \\right)}|\\mu_{1} \\le \\mu_{2}] &amp; = (1-\\alpha) \\\\ P( t_{calc} \\le {t}_{tab \\left(\\alpha ;\\nu \\right)} ) &amp; = (1-\\alpha) \\\\ \\end{align*}\\] Exemplo: a Secretaria de Educação de um município deseja saber se o desempenho dos alunos de duas diferentes escolas municipais na disciplina de matemática pode ser considerado igual a um nível de significância de \\(\\alpha=0,05\\) (verifique antes se as variâncias podem ser admitidas como iguais). Para tanto ministrou um mesmo teste a 10 alunos de cada uma delas e obteve os seguintes notas: Desempenho dos alunos de duas escolas Escola 01 Escola 02 68 94 85 79 51 100 75 88 50 75 83 94 81 70 87 87 100 20 80 82 Estrutura do teste: \\[ \\begin{cases} H_{0}: (\\mu_{1} - \\mu_{2}) = 0 \\\\ H_{1}: (\\mu_{1} - \\mu_{2}) \\ne 0 \\end{cases} \\] Teste de hipóteses bilateral (tipo: diferente de): \\[ P (- {t}_{tab\\left(\\frac{\\alpha }{2};\\nu \\right)} \\le t_{calc} \\le {t}_{tab\\left(\\frac{\\alpha }{2};\\nu \\right)}) = (1-\\alpha) \\] As variâncias populacionais não são conhecidas e o tamanho das amostras é reduzido. Teste de hipóteses para a igualdade das variâncias: \\[ \\begin{cases} H_{0}: (\\sigma_{1}^{2}-\\sigma_{2}^{2})=\\Delta \\\\ H_{1}: (\\sigma_{1}^{2} - \\sigma_{2}^{2}) \\ne \\Delta \\end{cases} \\] usualmente \\(\\Delta=0\\) (igualdade)} Se \\(\\sigma_{1}^{2}=\\sigma_{2}^{2}\\), então \\(\\frac{\\sigma_{1}^{2}}{\\sigma_{2}^{2}}=1\\). O maior valor de \\(F_{calc}\\) é dado por: \\[ F_{cal}=\\left(\\frac{{S}_{1}^{2}}{{S}_{2}^{2}}\\right)\\cdot \\left(\\frac{{\\sigma }_{1}^{2}}{{\\sigma }_{2}^2}\\right)=22,056 \\] e o valor crítico é \\[ {F}_{tab\\left(\\alpha ,{n}_{1}-1,{n}_{2}-1\\right)} = {F}_{tab\\left(5\\% ,9,9\\right)} = 3,18 \\] prob_desejada1=0.95 df1=9 df2=9 f_desejado1=round(qf(prob_desejada1,df1, df2), 4) d_desejada1=df(f_desejado1,df1, df2) f_calculado=22.056 d_calculada=df(f_calculado,df1, df2) f_test_4=ggplot(data.frame(x = c(0, 25)), aes(x)) + stat_function(fun = df, geom = &quot;area&quot;, fill = &quot;lightgrey&quot;, xlim = c(0,f_desejado1), colour=&quot;black&quot;, args = list( df1 = df1, df2 = df2 ))+ stat_function(fun = df, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(f_desejado1,25), colour=&quot;black&quot;, args = list( df1 = df1, df2 = df2 ))+ scale_y_continuous(name=&quot;Densidade&quot;) + #scale_x_continuous(name=&quot;Valores score (f)&quot;, breaks = c(f_desejado1, f_desejado2))+ scale_x_continuous(name=&quot;Valores score (f)&quot;)+ labs(title=&quot;Curva da função densidade \\nDistribuição F&quot;, subtitle = &quot;P(0; 22,056)=(1-\\u03b1) em cinza (nível de confiança) \\nP(22,056 ; \\U221e)= \\u03b1 em vermelho (nível de significância) &quot;)+ geom_segment(aes(x = f_desejado1, y = 0, xend = f_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=f_desejado1+0.1, y=d_desejada1, label=&quot;F crítico&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ geom_segment(aes(x = f_calculado, y = 0, xend = f_calculado, yend = d_calculada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=f_calculado+0.1, y=d_desejada1, label=&quot;F calculado&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ annotate(geom=&quot;text&quot;, x=f_desejado1+5, y=d_desejada1, label=&quot;Zona de rejeição \\n(para F calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=f_desejado1-2.5, y=d_desejada1, label=&quot;Zona de não rejeição \\n(para F calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.25: O valor calculado da estatística de teste (\\(F_{calc}=3,18\\)) situa-se na região significante do teste, permitindo a rejeição da hipótese nula de que as variâncias sejam iguais sob o nível de confiança estabelecido. Conclusão: não se pode aceitar a hipótese de que as variâncias sejam iguais a um nível de significância de 5% (cf. figura 11.25). Estatística do teste: \\(T \\sim t_{(\\nu)}\\) considerando que as variãncias populacionais não podem ser, estatisticamente, admitidas como iguais: \\[ T_{calc} = \\frac{(\\stackrel{-}{X}_{1} - \\stackrel{-}{X}_{2})-\\Delta_{0}} { \\sqrt{\\frac{S^{2}_{1}}{n_{1}}+\\frac{S^{2}_{2}}{n_{2}}}} \\] em que: \\(\\mu_{1} , \\mu_{2}\\) são as médias das populações em teste; \\(\\stackrel{-}{X}_{1}=70,90, S_{1}^{2}= 25,339^{2} , n_{1}=10\\) são a média, a variância e o tamanho amostral 1; \\(\\stackrel{-}{X}_{2}=84, S_{2}^{2}= 5,395^{2} , n_{2}=10\\) são a média, a variância e o tamanho amostral 2; \\({t}_{tab \\left(\\frac{\\alpha }{2};\\nu \\right)}\\) ou \\({t}_{tab \\left(\\alpha ;\\nu \\right)}\\): o quantil associado na distribuição ``t’’ de Student ao nível de significância pretendido no teste, com graus de liberdade \\((\\nu)\\). A aproximação dos graus de liberdade (\\(\\nu\\)) é dada por uma combinação linear das variâncias de amostras independentes (equação de Welch-Satterhwaite, 1946): \\[ \\nu=\\frac{{\\left(\\frac{{S}_{1}^{2}}{{n}_{1}}+\\frac{{S}_{2}^{2}}{{n}_{2}}\\right)}^{2}}{\\frac{{\\left(\\frac{{S}_{1}^{2}}{{n}_{1}}\\right)}^{2}}{{n}_{1}-1}+\\frac{{\\left(\\frac{{S}_{2}^{2}}{{n}_{2}}\\right)}^{2}}{{n}_{2}-1}}=10 \\] (aproximar o resultado para o inteiro superior mais próximo). Cálculo da estatística do teste: \\[ T_{calc} = \\frac{(\\stackrel{-}{X}_{1} - \\stackrel{-}{X}_{2})-\\Delta_{0}} { \\sqrt{\\frac{S^{2}_{1}}{n_{1}}+\\frac{S^{2}_{2}}{n_{2}}}}=-1,599 \\] Da tabela `t’’ de Student obtemos o valor crítico bicaudal da estatística: \\[ |{t}_{tab \\left(\\frac{\\alpha }{2};\\nu \\right)}| = |{t}_{tab \\left(\\frac{0,025}{2};10 \\right)}| = 2,22 \\] alfa=0.05 prob_desejada1=alfa/2 df=8 t_desejado1=round(qt(prob_desejada1,df ),df) d_desejada1=dt(t_desejado1,df) prob_desejada2=1-alfa/2 df=8 t_desejado2=round(qt(prob_desejada2, df),df) d_desejada2=dt(t_desejado2,df) t_calculado=-1.599 d_calculado=dt(t_calculado,df) ggplot(NULL, aes(c(-4,4))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(-4, t_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(t_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(0, t_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado2,4), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado1, t_desejado2)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-2,22, 2,22)=(1-\\u03b1) em cinza (nível de confiança=0,95) \\nP(-\\U221e; -2,22)= P(2,22; \\U221e)= \\u03b1/2 em vermelho (nível de significância/2=0,025) &quot;)+ geom_segment(aes(x = t_desejado1, y = 0, xend = t_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = t_desejado2, y = 0, xend = t_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-0.1, y=d_desejada1, label=&quot;valor crítico=-2,101&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.3, y=d_desejada2, label=&quot;valor crítico=2,101&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado2+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado1+2, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = t_calculado, y = 0, xend = t_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-1,599&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.26: Regiões de rejeição da hipótese nula para o teste bilateral (tipo: diferente de) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valores críticos da estatística do teste: \\(t_{crit} =\\pm 2,22\\). O valor calculado da estatística (\\(t_{calc}=-1,599\\)) não se situa na faixa de significância do teste, não nos permitindo a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos pela análise estatística de comparação de médias das duas amostras colhidas das notas de testes de matemáticas realizados em duas escolas diferentes (1 e 2) não nos permitem rejeitar a hipótese de que suas médias sejam iguais a um nível de confiança de 5% (cf. figura 11.26. "],["teste-de-hipóteses-para-uma-proporção-pi.html", "11.10 Teste de hipóteses para uma proporção \\(\\pi\\)", " 11.10 Teste de hipóteses para uma proporção \\(\\pi\\) A aproximação de uma população sob distribuição Binomial pela distribuição Normal pode ser realizada desde que atendidas às seguintes condições: a amostra é colhida de modo aleatório, os ensaios são independentes e com probabilidade de ``sucesso’’ constante; se a amostra é colhida sem reposição, o tamanho da população deve ser ao menos 10 (20) vezes o tamanho da amostra (\\(N \\ge 10,20 \\cdot n\\)); tamanho de amostra deve ser de ao menos 30 (\\(n \\ge 30\\)); a proporção populacional não extrema (próxima a 0 ou 1); o número de ``sucessos’’ deve ser de ao menos 5 (\\(n \\cdot \\pi_{0} \\ge 5\\)); e, o número de ``fracassos’’ deve ser de ao menos 5 (\\(n \\cdot (1-\\pi_{0}) \\ge 5\\)). 11.10.1 Estruturas possíveis para as hipóteses Teste bilateral (tipo: diferente de) \\[ \\begin{cases} H_{0}: \\pi = \\pi_{0}\\\\ H_{1}: \\pi \\ne \\pi_{0}\\\\ \\end{cases} \\] Teste unilateral à esquerda (tipo: menor que) \\[ \\begin{cases} H_{0}: \\pi \\ge \\pi_{0}\\\\ H_{1}: \\pi &lt; \\pi_{0}\\\\ \\end{cases} \\] Teste unilateral à direita (tipo: maior que) \\[ \\begin{cases} H_{0}: \\pi \\le \\pi_{0}\\\\ H_{1}: \\pi &gt; \\pi_{0}\\\\ \\end{cases} \\] Estatística do teste: \\[ Z=\\frac{p-\\pi_{0} }{\\sqrt{\\frac{\\pi_{0} \\left(1-\\pi_{0}) \\right)}{n}}} \\sim \\mathcal{N}(0,1) \\] em que: \\(p\\) é a proporção observada na amostra, uma estimativa da proporção populacional \\(\\pi\\); \\(\\pi_{0}\\) o valor (desconhecido) inferido à proporção populacional, a ser testado frente à proporção amostral; e, \\(n\\): é o tamanho da amostra. 11.10.2 Probabilidade dos intervalos de confiança para os testes de hipóteses com o uso da estatística Z (\\(Z \\sim \\mathcal{N}(0,1)\\)): Teste de hipóteses bilateral (tipo: diferente de): \\[\\begin{align*} P[\\left|Z_{calc}\\right| \\le {Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}|\\pi= \\pi_{0}] &amp; =(1-\\alpha) \\\\ P( -{Z}_{tab\\left(\\frac{\\alpha }{2}\\right)} \\le Z_{calc} \\le {Z}_{tab\\left(\\frac{\\alpha }{2}\\right)} ) &amp; =(1-\\alpha)\\\\ \\end{align*}\\] Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[\\begin{align*} P[Z_{calc} \\ge {Z}_{tab\\left(\\alpha \\right)}|\\pi \\ge \\pi_{0}] &amp; =(1-\\alpha)\\\\ P( Z_{calc} \\ge {Z}_{tab\\left(\\alpha \\right)}) &amp; = (1-\\alpha)\\\\ \\end{align*}\\] Teste de hipóteses unilateral à direita (tipo maior que): \\[\\begin{align*} P[Z_{calc} \\le {Z}_{tab\\left(\\alpha \\right)}|\\pi \\le \\pi_{0}] &amp; =(1-\\alpha)\\\\ P( Z_{calc} \\le {Z}_{tab\\left(\\alpha \\right)}) &amp; = (1-\\alpha)\\\\ \\end{align*}\\] Nas figuras 11.8, 11.9 e 11.10 observam-se:   as regiões de rejeição da hipótese nula (subdivididas nos dois ou em apenas um dos lados) sob a curva da função densidade de probabilidade da distribuição adequada ao teste com probabilidades iguais ao nível de significância \\(\\alpha\\) ; a região de não rejeição da hipótese nula (delimitada à esquerda e à direita ou apenas em um dos lados) com probabilidade igual ao nível de confiança \\((1-\\alpha)\\); e, os valores críticos da estatística do teste. Exemplo: Um relatório de uma companhia afirma que 40% de toda a água obtida a partir de poços artesianos no nordeste é salobra. Há muita controvérsia sobre essa informação, alguns dizem que a proporção é maior, outros que é menor. Para dirimir essa dúvida, 400 poços foram sorteados e observou-se em 120 deles que a água era salobra. Qual seria a conclusão a um nível de significância de 3%? O problema nos pede um teste bilateral (tipo: diferente de): \\[ \\begin{cases} H_{0}: \\pi = 0,40\\\\ H_{1}: \\pi \\ne 0,40\\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a proporção dos poços com água salobra é de 40%, fazendo então valer a hipótese alternativa que afirma ser diferente de 40%. Verificação das condições: nada se afirmou sobre o tamanho da população para se verificar: \\(N ge 10n\\)); tamanho de amostra \\(n \\ge 30\\): nossa amostra é de 400 poços; proporção populacional não extrema (próxima a 0 ou 1): a afirmação é de que \\(\\pi=0,40\\); e, \\((n \\cdot \\pi)\\) e \\((n \\cdot (1-\\pi)\\) são maiores que 5 (160 e 240, respectivamente). Assim, a estatística do teste fica definida como sendo: \\[ Z=\\frac{p-\\pi_{0} }{\\sqrt{\\frac{\\pi_{0} \\left(1-\\pi_{0}) \\right)}{n}}} \\sim \\mathcal{N}(0,1) \\] em que: \\(p=0,30\\) é a proporção amostral, uma estimativa da proporção populaciona \\(\\pi\\); \\(\\pi_{0}=0,40\\) é o valor (desconhecido) inferido à proporção populacional, a ser testado frente à proporção amostral; e, \\(n=400\\): é o tamanho da amostra. Da tabela da distribuição Normal padronizada obtemos o valor crítico bicaudal: \\(|{Z}_{tab\\left(\\frac{\\alpha }{2}\\right)}|=2,17\\). Pelo cálculo, a estatística do teste é \\(z_{calc}=-4,082\\). alfa=0.03 prob_desejada1=alfa/2 z_desejado1=round(qnorm(prob_desejada1),4) d_desejada1=dnorm(z_desejado1, 0, 1) prob_desejada2=1-alfa/2 z_desejado2=round(qnorm(prob_desejada2),4) d_desejada2=dnorm(z_desejado2, 0, 1) z_calculado=-4.082 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-5,5))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-5, z_desejado1), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado1,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado2), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado2,5), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de z&quot;, breaks = c(z_desejado1,z_desejado2)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-2,17, 2,17)=(1-\\u03b1) em cinza (nível de confiança=0,97) \\nP(-\\U221e; -2,17)= P(2,17; \\U221e)= \\u03b1/2 em vermelho (nível de significância/2=0,015) &quot;)+ geom_segment(aes(x = z_desejado1, y = 0, xend = z_desejado1, yend = d_desejada1), color=&quot;blue&quot;, lty=2, lwd=0.3)+ geom_segment(aes(x = z_desejado2, y = 0, xend = z_desejado2, yend = d_desejada2), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-0.1, y=d_desejada1, label=&quot;valor crítico=-2,17&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.3, y=d_desejada2, label=&quot;valor crítico=2,17&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1-1.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado2+0.5, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1/2&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado1+1.3, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-4,082&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.27: Regiões de rejeição da hipótese nula para o teste bilateral (tipo: diferente de) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valores críticos da estatística do teste: \\(z_{crit} =\\pm 2,17\\). O valor calculado da estatística (\\(z_{calc}=-4,082\\)) situa-se na faixa de significância do teste, possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos na análise estatística realizada nos permitem rejeitar a hipótese de que a proporção de poços com água salobra é de 40% sob um nível de confiança de 97%. A proporção de poços com água salobra no Nordeste é diferente de 40% (Figura 11.25). Teste unilateral à esquerda (tipo: menor que) \\[ \\begin{cases} H_{0}: \\pi \\ge 0,40\\\\ H_{1}: \\pi &lt; 0,40\\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a proporção igual ou maior a 40%, fazendo então valer a hipótese alternativa que afirma ser a proporção menor que 40%. Da tabela obtemos o valor crítico monocaudal: \\(Z_{tab\\left(\\alpha\\right)}=-1,88\\). Pelo cálculo, a estatística do teste é \\(Z_{calc}=-4,082\\). alfa=0.03 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) z_calculado=-4.082 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-5,5))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(-5, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,0), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(0, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(z_desejado,5), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de z&quot;, breaks = c(z_desejado)) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P( -1,88,\\U221e,)=(1-\\u03b1) em cinza (nível de confiança=0,97) \\nP(-\\U221e; -1,88)=\\u03b1 em vermelho (nível de significância=0,03) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado-0.1, y=d_desejada, label=&quot;valor crítico=-1,88&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-2, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado+1, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-4,082&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.28: Regiões de rejeição da hipótese nula para o teste unilateral à esquerda (tipo: menor que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelos valor crítico da estatística do teste: \\(z_{crit} = -1,88\\). O valor calculado da estatística (\\(z_{calc}=-4,082\\)) situa-se na faixa de significância do teste, o que nos permite a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos na análise estatística realizada nos permitem rejeitar a hipótese de que a proporção de poços com água salobra é de 40% sob um nível de confiança de 97%. A proporção de poços com água salobra no Nordeste é menor que de 40% (Figura 11.26. Teste unilateral à direita (tipo: maior que) \\[ \\begin{cases} H_{0}: \\pi \\le 0,40\\\\ H_{1}: \\pi &gt; 0,40\\\\ \\end{cases} \\] Iremos verificar se a informação amostral obtida nos permite rejeitar a hipótese nula que afirma ser a proporção igual ou meor a 40%, fazendo então valer a hipótese alternativa que afirma ser a proporção maior que 40%. Da tabela obtemos o valor crítico monocaudal: \\(Z_{tab\\left(\\alpha\\right)}=1,88\\). Pelo cálculo, a estatística do teste é \\(Z_{calc}=-4,082\\). alfa=0.97 prob_desejada=alfa z_desejado=round(qnorm(prob_desejada),4) d_desejada=dnorm(z_desejado, 0, 1) z_calculado=-4.082 d_calculado=dnorm(z_calculado, 0, 1) ggplot(NULL, aes(c(-5,5))) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;lightgrey&quot;, xlim = c(-5, z_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dnorm, fill = &quot;red&quot;, xlim = c(z_desejado,5), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de z&quot;, breaks = c(z_desejado)) + labs(title= &quot;Região crítica sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P( -\\U221e; 1,88)=(1-\\u03b1) em cinza (nível de confiança=0,97) \\nP(1,88; \\U221e)=\\u03b1 em vermelho (nível de significância=0,03) &quot;)+ geom_segment(aes(x = z_desejado, y = 0, xend = z_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_desejado-0.1, y=d_desejada, label=&quot;valor crítico=-1,88&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado+1, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=z_desejado-2.5, y=0.2, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = z_calculado, y = 0, xend = z_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=z_calculado-0.1, y=d_calculado, label=&quot;valor da estatística do teste=-4,082&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.29: Região de rejeição da hipótese nula para o teste unilateral à direita (tipo: maior que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(z_{crit} = 1,88\\). O valor calculado da estatística (\\(z_{calc}=-4,082\\)) situa-se na faixa de não significância do teste, não possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: Os resultados obtidos na análise estatística realizada não nos permitem rejeitar a hipótese de que a proporção de poços com água salobra seja menor ou igual a 40% sob um nível de confiança de 97%. (cf. Figura 11.27). "],["testes-não-paramétricos.html", "11.11 Testes não paramétricos", " 11.11 Testes não paramétricos Um teste não paramétrico (às vezes chamado de teste livre de distribuição) não assume nada sobre a distribuição subjacente (por exemplo, que os dados vêm de uma distribuição Normal ). Isso não equivale a dizer que não se saiba nada sobre a população de origem. Geralmente significa que se sabe que os dados populacionais não são de uma distribuição Normal . Tipos de testes não paramétricos Teste de sinal; Teste de Sinal de Wilcoxon; Teste de Friedman; Teste de Mann-Whitney; Teste de Kruskal Wallis; e, Teste qui-quadrado. Há um conjunto importante de testes de hipóteses que possibilita a análise de frequências que ocorrem nas classes de um fator. Esses testes de hipóteses são muitas vezes referenciados como testes qui-quadrado porque a estatística do teste possui, de modo assintótico, distribuição qui-quadrado. Embora esses testes se enquadrem em categorias distintas, compartilham algumas características comuns: Em cada situação considera-se a amostra aleatória, gerada por um ou mais experimentos multinomiais, independentes, de uma ou mais populações multinomiais. Obviamente, a população Bernoulli e a população binomial são casos particulares. A amostra aleatória é formada pelas frequências observadas nas classes, definidas pela classificação de cada uma das unidades de observação de acordo com um ou mais critérios de interesse. Em todas as situações, a estatística do teste envolve a comparação entre frequências observadas e frequências esperadas, obtidas sob a hipótese de nulidade. Na essência, o teste qui-quadrado verifica hipóteses sobre as probabilidades e utiliza a discrepância existente entre as frequências observadas e as frequências esperadas para concluir sobre elas. Basicamente, dispõe-se de observações (contidas na amostra) sobre uma ou mais populações e busca-se determinar de qual população multinomial essa amostra veio. A hipótese de nulidade especifica a população de interesse. Se as probabilidades não forem completamente especificadas, algumas probabilidades (e, consequentemente, frequências esperadas) deverão ser estimadas pelos dados, reduzindo os graus de liberdade da distribuição limite. Como mencionado, a distribuição limite da estatística do teste é a distribuição qui-quadrado. Uma regra usualmente exigida para uma boa aproximação da distribuição qui-quadrado é que a frequência esperada seja maior ou igual a 5. Evidentemente, quanto maiores forem as frequências esperadas, melhor será a aproximação. Testes paramétricos exigem que a variável seja numérica e várias hipóteses relativas aos parâmetros sejam satisfeitas, tais como que os dados tenham uma distribuição Normal (ou a sigam assintoticamente) ou ainda, em alguns casos que, suas variâncias sejam homogêneas (homocedasticidade) e as amostras tenham um certo tamanho ou frequência observada mínimos. Testes não paramétricos não assumem nenhum tipo de distribuição e são menos exigentes, podendo também trabalhar com variáveis não numéricas. Como regra geral, opta-se por testes não paramétricos quando: os valores observados forem extraídos de populações que não possuem uma aproximação com a distribuição Normal; as populações de origem não possuem homogeneidade de variâncias (heterocedasticidade); e, as variáveis em estudo não apresentem medidas intervalares que possibilitem o cálculo de estatísticas tais como a média e desvios. 11.11.1 Teste Qui-quadrado para verificação da independência (homogeneidade) O Teste Qui-quadrado de homogeneidade (ou independência) é um teste estatístico aplicado a dados categóricos para avaliar quão provável é que qualquer diferença observada nas proporções observadas entre os vários níveis de uma variável categórica em populações diferentes (ou níveis de uma segunda variável categórica) seja simples decorrência do acaso; ou seja, o teste Qui-quadrado é geralmente usado verificar quão homogêneas são entre si as frequências observadas não havendo, portanto, diferença estatisticamente significativa entre as populações (ou variáveis). Diferenças entre o teste Qui-quadrado de homogeneidade e de independência: Teste Qui-quadrado de homogeneidade: selecionamos uma amostra de elementos de cada uma das populações e distribuímos os elementos de cada uma dessas amostras segundo as categorias da variável estudada; e, Teste Qui-quadrado de independência: distribuímos uma amostra de n elementos de apenas uma população segundo as categorias da primeira variável categórica A e as da segunda variável categórica B. Esse tipo de investigação equivale à realização de Teste de Hipóteses onde a hipótese nula que pressupõe que exista homogeneidade (independência) na distribuição das contagens observadas em cada uma das categorias da variável nas populações amostradas (ou níveis da outra variável, no teste Qui-quadrado de Independência) será confrontada com a hipótese alternativa, de que não são homogêneas (dependência) e as flutuações não são podem ser atribuídas ao acaso. Desse modo o foco será buscar evidência estatística robusta o suficiente que confirmem que as frequências observadas entre as diferentes populações (ou níveis da outra variável, no teste Qui-quadrado de Independência) podem ser consideradas homogêneas (independentes) sob um dado nível de significância \\(\\alpha\\). Consideremos para isso a tabela genérica para a realização do Teste Qui-quadrado onde em cada célula (habitualmente chamada de casela) temos uma frequência (uma quantidade) observada na Tabela a seguir. Tabela (r × s) de frequências observadas Variável categórica B1 B2 … Bs Total A1 n(1, 1) n(1, 2) … n(1, s) n(1, .) A2 n(2, 1) n(2, 2) … n(2, s) n(2, .) … … … … … … Ar n(r, 1) n(r, 2) … n(r, s) n(r, .) Totais n(., 1) n(., 2) … n(., s) n(., .) Notação utilizada na tabela: \\(r\\) é o número de linhas da tabela; \\(s\\) é o número de colunas da tabela; \\(i\\) indexa a i-ésima linha da tabela; \\(j\\) indexa a j-ésima coluna da tabela; \\(n_{i,j}\\) indica o elemento localizado na casela situada na i-ésima linha e j-ésima coluna; \\(n_{(1,.)}\\) indica o último elemento da primeira linha; \\(n_{(.,1)}\\) indica o último elemento da primeira coluna;e, \\(n_{(.,.)}\\) indica o último elemento simultaneamente das linhas e colunas da tabela. Quantas observações devemos ter em cada casela da tabela acima para que as proporções observadas de \\(A\\) e \\(B\\) sejam consideradas estatisticamente homogêneas (independentes)? Se \\(A\\) e \\(B\\) forem independentes então \\(P(A_{i} \\cap B_{j})= P(A_{i}) \\times P(B_{j})\\). O número esperado de observações com as características (\\(A_{i}\\) e \\(B_{j}\\)) entre as \\(n_{.,.}\\) observações - sob a hipótese de homogeneidade (independência) da distribuição das contagens observadas entre das variáveis (ou da variável nas populações) - em cada casela deverá ser: \\[\\begin{align*} E_{(i,j)} &amp; = n_{(.,.)} \\times p_{(i,j)} \\\\ &amp; = n_{(.,.)} \\times p_{(i,.)} \\times p_{(.,j)} \\\\ &amp; = n_{(.,.)} \\times \\frac{n_{(i,.)}}{n_{(.,.)}} \\times \\frac{n_{(.,j)}}{n_{(.,.)}}\\\\ \\end{align*}\\] Assim, o valor esperado - sob a hipótese de homogeneidade (independência) da distribuição das contagens observadas entre as variáveis (ou da variável nas populações) \\(A\\) e \\(B\\) - em cada célula deverá ser: \\[ E_{(i,j)} = \\frac{n_{(i,.)} \\times n_{(.,j)}}{n_{(.,.)}} \\] Em que: \\(E_{(i,j)}\\) é o valor esperado na casela \\((i,j)\\); \\(n_{(i,.)}\\) é o total observado na linha \\(i\\); \\(n_{(.,j)}\\) é o total observado na coluna \\(j\\); e, \\(n_{(.,.)}\\) é o total geral observado. Para a aplicação do teste \\(\\chi{2}\\) exige-se que: preferencialmente as amostras sejam grandes (\\(n \\ge 30\\)); no máximo 20% das caselas tenham uma frequência esperada menor que 5; e, em nenhuma casela a frequência esperada pode ser menor que 1. A estatística (\\(X\\)) do Teste Qui-quadrado de homogeneidade (independência) baseia-se na diferença (dsitância) entre as contagens observados e as contagens esperadas sob a suposição de homogeneidade (independência) pode ser definida da seguinte maneira: \\[ X=\\sum_{i=1}^r\\sum_{j=1}^s \\frac{(O_{(i,j)} - E_{(i,j)})^2}{E_{(i,j)}} \\sim \\chi^{2}_{((r-1)\\times(s-1))} \\] e sua correspondente distribuição: \\[ X\\sim \\chi^{2}_{((r-1)\\times(s-1))} \\] A hipótese nula postula que não há associação: as variáveis são independentes. A flutuação observada nas contagens é devida apenas a fatores puramente aleatórios. A hipótese alternativa a contradiz, afirmando existir algum fator não aleatório (alguma forma de associação) que resulta na distribuição não homogênea entre as contagens observadas: há dependência entre as variáveis. \\[ \\begin{cases} H_{0}: \\text{ as variáveis são independentes (a flutuação nas contagens é aleatória}) \\\\ H_{1}: \\text{ as variáveis não são independentes (há alguma associação}) \\end{cases} \\] A distribuição de referência que permite julgar se um determinado valor da estatística \\(X\\) pode ser considerado grande o suficiente para rejeitar \\(H_{0}\\) em favor de \\(H_{1}\\) é a chamada distribuição Qui-quadrado: \\(\\chi^{2}\\). Formulação do teste: teste de hipóteses unilateral à direita (tipo: maior que): \\[\\begin{align*} P[X_{calc} \\le {\\chi^{2}}_{tab \\left(\\alpha ;(r-1)\\times(s-1) \\right)} | IND]&amp; =(1-\\alpha)\\\\ P(X_{calc} \\le \\chi^{2}_{tab \\left(\\alpha ;(r-1)\\times(s-1) \\right)})&amp;=(1-\\alpha) \\end{align*}\\] A região de não rejeição da hipótese nula pode ser vista na Figura 11.30. prob_desejada=0.95 r=4 s=3 df=(r-1)*(s-1) q_desejado=round(qchisq(prob_desejada,df), 4) d_desejada=dchisq(q_desejado,df) ggplot(data.frame(x = c(0, 30)), aes(x)) + stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;lightgrey&quot;, xlim = c(0,q_desejado), colour=&quot;black&quot;, args=list(df=df) )+ stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(q_desejado,30), colour=&quot;black&quot;, args = list(df = df))+ scale_y_continuous(name=&quot;Densidade&quot;) + #scale_x_continuous(name=&quot;Valores score (f)&quot;, breaks = c(f_desejado1, f_desejado2))+ scale_x_continuous(name=&quot;Valores score (X)&quot;)+ labs(title=&quot;Curva da função densidade \\nDistribuição Qui-quadrado&quot;, subtitle = &quot;P(0; x crítico)=(1-\\u03b1) em cinza (nível de confiança) \\nP(x crítico ; \\U221e)= \\u03b1 em vermelho (nível de significância) &quot;)+ geom_segment(aes(x = q_desejado, y = 0, xend = q_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=q_desejado+0.5, y=d_desejada, label=&quot;x crítico&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ annotate(geom=&quot;text&quot;, x=q_desejado+5, y=d_desejada, label=&quot;Zona de rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=q_desejado-5, y=d_desejada, label=&quot;Zona de não rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.30: Região de rejeição da hipótese nula para o teste uniletaral à direita (tipo: menor que): a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(x_{crit}\\) para o nível de significância pretendido (\\(\\alpha\\) em uma cauda) e (\\(df\\)) graus de liberdade. Exemplo: verifique a independência (homogeneidade) nas contagens da intenção de voto de quatro candidatos distintos em amostras de três diferentes bairros, partindo das informações consolidadas na tabela abaixo. Pesquisa sobre intenção de votos nos bairros “A”, “B” e “C” Candidato Bairros Total 2-4 “A” “B” “C” Candidato “A” 70 44 86 200 Candidato “B” 50 30 45 125 Candidato “C” 10 6 34 50 Candidato “D” 20 20 85 125 Totais 150 100 250 500 estrutura das hipóteses para o teste a um nível de significância: 0,05 \\[ \\begin{cases} H_{0}: \\text{as contagens são homogêneas} \\\\ H_{1}: \\text{as contagens não são homogêneas} \\end{cases} \\] Equivale dizer que há independência entre a escolha de um ou outro candidato e o bairro em questão (não há relação entre um determinado bairro e um determinado candidato) Estatística do teste e sua distribuição: \\[ X=\\sum_{i=1}^r\\sum_{j=1}^s \\frac{(O_{(i,j)} - E_{(i,j)})^2}{E_{(i,j)}} \\sim \\chi^{2}_{((r-1)\\times(s-1))} \\] Cálculo da frequência esperada em cada casela (\\(E_{(i,j)}\\)): \\[ E_{(i,j)} = \\frac{n_{(i,.)} \\times n_{(.,j)}}{n_{(.,.)}} \\] \\[ \\frac{\\text{soma da linha i} \\times \\text{soma da coluna j}}{\\text{total de observações}} \\] As frequências esperadas em cada casela (\\(i,j\\)) serão calculadas pela fórmula acima seguir e estão apresentadas na tabela a segui, em conjunto com as frequências observadas. Pesquisa sobre intenção de voto nos bairros “A”, “B” e “C”: frequências observadas (e entre parênteses e negrito as frequências esperadas) Candidato Bairros Total 2-4 “A” “B” “C” Candidato “A” 70 (60) 44 (40) 86 (100) 200 Candidato “B” 50 (37,5) 30 (25) 45 (62,5) 125 Candidato “C” 10 (15) 6 (10) 34 (25) 50 Candidato “D” 20 (37,5) 20 (25) 85 (62,5) 125 Totais 150 100 250 500 Nenhuma casela teve frequência esperada menor que 1 nem tampouco observou-se casela com frequência inferior a 5. Cálculo da estatística do teste: \\[ X=\\sum_{i=1}^4\\sum_{j=1}^3 \\frac{(O_{(i,j)} - E_{(i,j)})^2}{E_{(i,j)}} = 37,88 \\] Da tabela \\(\\chi^{2}\\) para o total de graus de liberdade \\(((r-1)\\times(s-1))=(4-1)\\times(3-1)=6\\) obtemos o valor crítico da estatística do teste (\\(\\chi^{2}_{crit(6)}=12,60\\)). prob_desejada=0.95 r=4 s=3 df=(r-1)*(s-1) q_desejado=round(qchisq(prob_desejada,df), 4) d_desejada=dchisq(q_desejado,df) q_calculado=37.88 d_calculado=dchisq(q_calculado,df) ggplot(data.frame(x = c(0, 50)), aes(x)) + stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;lightgrey&quot;, xlim = c(0,q_desejado), colour=&quot;black&quot;, args=list(df=df) )+ stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(q_desejado,40), colour=&quot;black&quot;, args = list(df = df))+ scale_y_continuous(name=&quot;Densidade&quot;) + #scale_x_continuous(name=&quot;Valores score (f)&quot;, breaks = c(f_desejado1, f_desejado2))+ scale_x_continuous(name=&quot;Valores score (X)&quot;)+ labs(title=&quot;Curva da função densidade \\nDistribuição Qui-quadrado&quot;, subtitle = &quot;P(0; 12,60)=(1-\\u03b1) em cinza (nível de confiança) \\nP(12,60 ; \\U221e)= \\u03b1 em vermelho (nível de significância) &quot;)+ geom_segment(aes(x = q_desejado, y = 0, xend = q_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=q_desejado+0.5, y=d_desejada, label=&quot;x crítico=12,60&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ annotate(geom=&quot;text&quot;, x=q_desejado+5, y=d_desejada, label=&quot;Zona de rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=q_desejado-5, y=d_desejada, label=&quot;Zona de não rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = q_calculado, y = 0, xend = q_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=q_calculado+0.5, y=d_calculado, label=&quot;x calculado=37,88&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ theme_bw() Figure 11.31: Região de rejeição da hipótese nula para o teste uniletaral à direita (tipo: menor que): a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(x_{crit}=12,60\\) para o nível de significância pretendido (\\(\\alpha=0,05\\) em uma cauda) e (\\(df=6\\)) graus de liberdade. Conclusão: face aos dados trazidos à análise rejeitamos a proposição de que a preferência por um determinado candidato não esteja de algum modo associada ao bairro pesquisado sob um nível de significância de 5% (a probabilidade de cometimento de um erro tipo I. Há alguma relação entre a preferência por um ou outro candidato e os bairros (Figura 11.31). . 11.11.2 Correção de continuidade em tabelas 2x2 Em tabelas de dimensão 2x2, especialmente quando as amostras não forem muito grandes, recomenda-se aplicar a chamada correção de continuidade de Yates, que consiste em reduzir 0,5 unidade nas diferenças absolutas entre as frequências observadas e esperadas: \\[ X=\\sum_{i=1}^r\\sum_{j=1}^s \\frac{(|O_{(i,j)} - E_{(i,j)}|-0,5)^2}{E_{(i,j)}} \\] Ou seja, em cada casela, depois de calculada a diferença entre a frequência observada e a frequência esperada, tomamos o módulo dessa operação (isto é, despreza-se o sinal \\(\\pm\\) ) e reduz-se esse valor em 0,5 unidade para, em seguida, elevamos ao quadrado e então dividir-se pela frequência esperada da célula. 11.11.3 Coeficiente de contingência de Pearson (modificado: \\(C^{*})\\) } Como vimos, a aplicação do teste qui-quadrado permite verificar se existe associação entre duas variáveis, com base em um conjunto de observações. A intensidade dessa associação pode ser quantificada por coeficientes que têm por objetivo medir a força da associação entre duas variáveis categorizadas. Um deles é o chamado coeficiente de contingência de Pearson modificado (uma correção em razão da dimensão da tabela). Um coeficiente de associação, aplicado a uma tabela de contingência, produz um valor numérico que descreve se os dados se aproximam mais de uma situação de independência (\\(C^{*}=0\\)) ou de uma situação de associação ou dependência perfeita (\\(C^{*}=1\\)). \\[ C^{*} = \\sqrt{ \\frac{k \\times X^{2}}{(k-1)\\times (n + X^{2}) } } \\] em que: \\(k\\) é o menor valor entre o número de linhas (l) e de colunas (c) da tabela; \\(n\\) é o número de elementos da tabela; e, \\(X{2}\\): valor calculado da estatística do teste qui-quadrado. Exemplo: no exercício resolvido anteriormente (\\(X^{2}=37,88\\) e uma tabela \\(3 \\times 4\\) com 500 observaçoes) teremos o seguinte valor para o coeficiente de contingência modificado (\\(C^{*}\\):) \\[\\begin{align*} C^{*} &amp; = \\sqrt{ \\frac{k \\times X^{2}}{(k-1)\\times (n + X^{2}) } }\\\\ &amp; = \\sqrt{ \\frac{3 \\times 37,88}{(3-1)\\times (500 + 37,88) } }\\\\ &amp; = \\sqrt{ \\frac{113,64}{(2)\\times (537,88) } }\\\\ &amp; = \\sqrt{0,105637}\\\\ &amp; = 0,325 \\end{align*}\\] 11.11.4 Teste Qui-quadrado para verificação da qualidade do ajuste a uma distribuição teórica de probabilidade O teste de ajuste de qui-quadrado é um teste não paramétrico usado para descobrir como o valor observado de um dado fenômeno é significativamente diferente do valor esperado. No teste de ajuste do qui-quadrado, o termo qualidade de ajuste ( goodness-of-fit ) é usado para comparar a distribuição da amostra observada com uma distribuição teórica de probabilidade esperada. O teste de ajuste do qui-quadrado determina quão bem a distribuição teórica (como Normal, binomial ou Poisson) se encaixa na distribuição empírica. No teste de ajuste do qui-quadrado, os dados da amostra são divididos em intervalos. Em seguida, os números de pontos que se enquadram no intervalo são comparados, com o número esperado de pontos em cada intervalo. Considere-se a seguinte tabela com as observações agrupadas em classes. Dados observados agrupados em classes ID Classes Frequência observada (fobsi) Frequência teórica esperada (fespi) \\(\\frac{(f_{obs_i} - f_{esp_i})^{2}}{f_{esp_i}}\\) 1 liminf ⊢ limsup fobs1 fesp1 ….. 2 liminf ⊢ limsup fobs2 fesp2 ….. … … … …. …. k liminf ⊢ limsup fobsk fespk Totais - \\(\\sum_{i=1}^{k}f_{obs_i}\\) - \\(X_{calc}= \\sum_{i=1}^{k} \\frac{(f_{obs_i} - f_{esp_i})^{2}}{f_{esp_i}}\\)  A frequência esperada em cada classe, sob a suposição de que os dados seguem uma distribuição Normal: \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\) é dada por: \\[\\begin{align*} f_{esp_{i}} &amp; = P[ lim_{inf_{i}} \\le X \\le lim_{sup_{i}} ]\\times \\sum_{i=1}^kf_{obs_{i}}\\\\ &amp; = P[ \\frac{(lim_{inf_{i}}-\\mu)}{\\sigma} \\le Z \\le \\frac{(lim_{sup_{i}}-\\mu)}{\\sigma} ]\\times \\sum_{i=1}^kf_{obs_{i}}\\\\ \\end{align*}\\] Há de se considerar duas situações: \\(\\mu\\) e \\(\\sigma\\) conhecidos, ou estimados a partir dos dados da amostra. Caso sejam conhecidos, demonstra-se que \\(X_{calc} \\sim \\chi^{2}_{(k-1)}\\); na outra situação, se forem estimados a partir da amostra (usando-se \\(\\stackrel{-}{x}\\) e \\(s\\)) então, igualmente, tem-se que \\(X_{calc} \\sim \\chi^{2}_{(k-1-2)}\\), apenas com a perda de dois graus de liberdade pelas estimações feitas. A estatística do teste qui-quadrado de qualidade de ajuste baseia-se na distância entre as frequências observadas e as frequências esperados sob a distribuição de probabilidade considerada e pode então ser definida, bem como o teste de hipóteses, da seguinte maneira: \\[ X_{calc}= \\sum_{i=1}^k \\frac{(f_{obs_i} - f_{esp_i})^2}{f_{esp_i}} \\] Demonstra-se que para uma amostra grande e com classes com frequências esperadas (\\(f_{esp_i}\\ge 5\\)) que \\(X_{calc} \\sim \\chi^{2} (k-1)\\) e o correspondente teste de hipóteses assume a estrutura seguinte: \\[ \\begin{cases} H_{0}: \\text{X segue o modelo teórico proposto} \\\\ H_{1}: \\text{X não segue o modelo proposto} \\end{cases} \\] Formulação do teste: Teste de hipóteses unilateral à direita (tipo: maior que): \\[\\begin{align*} P[X_{calc} \\le {\\chi^{2}}_{tab \\left(\\alpha ;(k-1) \\right)} | X \\sim \\mathcal{N}] &amp; =(1-\\alpha) \\\\ P(X_{calc} \\le \\chi^{2}_{tab \\left(\\alpha ;(k-1) \\right)}) &amp; =(1-\\alpha) \\end{align*}\\] prob_desejada=0.95 r=4 s=3 df=(r-1)*(s-1) q_desejado=round(qchisq(prob_desejada,df), 4) d_desejada=dchisq(q_desejado,df) ggplot(data.frame(x = c(0, 30)), aes(x)) + stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;lightgrey&quot;, xlim = c(0,q_desejado), colour=&quot;black&quot;, args=list(df=df) )+ stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(q_desejado,30), colour=&quot;black&quot;, args = list(df = df))+ scale_y_continuous(name=&quot;Densidade&quot;) + #scale_x_continuous(name=&quot;Valores score (f)&quot;, breaks = c(f_desejado1, f_desejado2))+ scale_x_continuous(name=&quot;Valores score (X)&quot;)+ labs(title=&quot;Curva da função densidade \\nDistribuição Qui-quadrado&quot;, subtitle = &quot;P(0; x crítico)=(1-\\u03b1) em cinza (nível de confiança) \\nP(x crítico ; \\U221e)= \\u03b1 em vermelho (nível de significância) &quot;)+ geom_segment(aes(x = q_desejado, y = 0, xend = q_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=q_desejado+0.5, y=d_desejada, label=&quot;x crítico&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ annotate(geom=&quot;text&quot;, x=q_desejado+5, y=d_desejada, label=&quot;Zona de rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=q_desejado-8, y=d_desejada, label=&quot;Zona de não rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.32: Região de rejeição da hipótese nula para o teste uniletaral à direita (tipo: menor que): a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(x_{crit}\\) para o nível de significância pretendido (\\(\\alpha\\) em uma cauda) e (\\(df\\)) graus de liberdade. Exemplo: deseja-se verificar a afirmação de que a porcentagem de cinzas (material estranh ao produtoo) contidas em café torrado e moído produzido por certa empresa de torrefação segue uma distribuição Normal. Os dados abaixo representam a quantidade percentual desse material encontradas em 250 amostras analisadas em laboratório. Faça um teste qui-quadrado de adequação das frequências observadas a essa distribuição com um nível de significância \\(\\alpha=0.04\\). Análise da presença de cinzas em café torrado e moído ID Cinzas de material Frequência observada (k) estranho (%) (fobsi) 1 9, 50 ⊢ 10, 50 2 2 10, 50 ⊢ 11, 50 5 3 11, 50 ⊢ 12, 50 16 4 12, 50 ⊢ 13, 50 42 5 13, 50dash14, 50 69 6 14, 50 ⊢ 15, 50 51 7 15, 50 ⊢ 16, 50 32 8 16, 50 ⊢ 17, 50 23 9 17, 50 ⊢ 18, 50 9 10 18, 50 ⊢ 19, 50 1 Totais 250 Análise do problema: verificar se as frequências observadas nas classes diferem das que seriam esperadas se a distribuição dessa variável seguisse uma distribuição Normal com parâmetros \\(\\mu\\) e \\(\\sigma\\) (não informados pelo enunciado do problema). Essa omissão nos força a utilizar a média e o desvio padrão amostrais (\\(\\stackrel{-}{x}\\) e \\(S\\)) como suas estimativas. Isso irá nos impor a perda adicional de mais dois graus de liberdade na estatística do teste: \\(\\chi^{2}_{(k-1-2)}\\). Para dados agrupados em classes a média e a variância são calculados por: \\[ \\sum_{i=1}^k \\frac{\\stackrel{-}{x_{i}} \\cdot f_{obs_{i}}}{n} = 14,512 \\] e \\[ S^{2} = \\frac{\\sum_{i=1}^k (\\stackrel{-}{x_{i}} -\\stackrel{-}{x})^{2} \\times f_{obs_{i}}}{n-1} = 2,701 \\] Na sequência, calculam-se as frequências esperadas para cada classe sob a premissa de Normalidade. Abaixo mostramos o cálculo para a primeira classe: \\[\\begin{align*} f_{esp_{i}} &amp; = P[ lim_{inf_{i}} \\le X \\le lim_{sup_{i}} ].\\sum_{i=1}^{k}f_{obs_{i}} \\\\ &amp; = P[ 9,50 \\le X \\le 10,50 ] \\times 250\\\\ &amp; = P[ \\frac{(lim_{inf_{i}}-\\mu)}{\\sigma} \\le Z \\le \\frac{(lim_{sup_{i}}-\\mu)}{\\sigma} ] \\times \\sum_{i=1}^kf_{obs_{i}}\\\\ &amp; = P[ \\frac{(9,50-14,512)}{\\sqrt{2,701}} \\le Z \\le \\frac{(10,50-14,512)}{\\sqrt{2,701}} ]\\times 250\\\\ &amp; = P[ \\frac{(9,50-14,512)}{\\sqrt{2,701}} \\le Z \\le \\frac{(10,50-14,512)}{\\sqrt{2,701}} ]\\times 250\\\\ &amp; = P[-3,0496 \\le Z \\le -2,4412 ]\\times 250\\\\ &amp; = (0,4989-0,4927) \\times 250\\\\ &amp; = (0,0062) \\times 250\\\\ &amp; = 1,55\\\\ \\end{align*}\\] Análise da presença de cinzas em café torrado e moído ID Cinzas de material Frequência Frequência \\(\\frac{(f_{obs_{i}} - f_{esp_i})^2}{f_{esp_i}}\\) (k) estranho (%) observada (fobsi) teórica esperada (fespi) 1 9, 50 ⊢ 10, 50 2 1,543559 2 10, 50 ⊢ 11, 50 5 6,525845 3 11, 50 ⊢ 12, 50 16 19,25203 4 12, 50 ⊢ 13, 50 42 39,648 5 13, 50 ⊢ 14, 50 69 57,01595 6 14, 50 ⊢ 15, 50 51 57,26207 7 15, 50 ⊢ 16, 50 32 40,16374 8 16, 50 ⊢ 17, 50 23 19,67134 9 17, 50 ⊢ 18, 50 9 6,725776 10 18, 50 ⊢ 19, 50 1 1,604656 Totais 250 -   As frequências esperadas para as classes 1 e 10 são menores que 5 (\\(f_{esp_i}\\ge 5\\)) impondo que essas duas classes sejam agrupadas às classes imediatamente adjacentes. Análise da presença de cinzas em café torrado e moído ID Cinzas de material Frequência Frequência \\(\\frac{(f_{obs_{i}} - f_{esp_i})^2}{f_{esp_i}}\\) (k) estranho (%) observada (fobsi) teórica esperada (fespi) 1-2 9, 50 ⊢ 11, 50 7 8,069404 0,141724 3 11, 50 ⊢ 12, 50 16 19,25203 0,549329 4 12, 50 ⊢ 13, 50 42 39,648 0,139525 5 13, 50 ⊢ 14, 50 69 57,01595 2,518900 6 14, 50 ⊢ 15, 50 51 57,26207 0,684808 7 15, 50 ⊢ 16, 50 32 40,16374 1,659374 8 16, 50 ⊢ 17, 50 23 19,67134 0,563255 9-10 17, 50 ⊢ 19, 50 10 8,330432 0,334611 Totais 250 - 6,591525 Estrutura do teste: teste de hipóteses unilateral à direita (tipo: maior que): \\[ \\begin{cases} H_{0}: X \\sim \\mathcal{N} (\\stackrel{-}{x}, S) \\\\ H_{1}: \\text{X não segue o modelo proposto} \\end{cases} \\] A hipótese nula postula que a variável X segue a distribuição Normal (\\(X \\sim \\mathcal{N}(\\stackrel{-}{x}, S)\\)) Estatística do teste: \\[ x_{calc}= \\sum_{i=1}^k \\frac{(f_{obs_{i}} - f_{esp_i})^2}{f_{esp_i}}=6,59 \\] Valor crítico da estatística de teste \\(\\chi^{2}_{(\\alpha), (k-1-2)}\\): \\[ \\chi^{2}_{(0,04), (8-1-2)}=11,64 \\] prob_desejada=0.96 df=5 q_desejado=round(qchisq(prob_desejada,df), 4) d_desejada=dchisq(q_desejado,df) q_calculado=round(6.59, 4) d_calculada=dchisq(q_calculado,df) ggplot(data.frame(x = c(0, 30)), aes(x)) + stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;lightgrey&quot;, xlim = c(0,q_desejado), colour=&quot;black&quot;, args=list(df=df) )+ stat_function(fun = dchisq, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(q_desejado,30), colour=&quot;black&quot;, args = list(df = df))+ scale_y_continuous(name=&quot;Densidade&quot;) + #scale_x_continuous(name=&quot;Valores score (f)&quot;, breaks = c(f_desejado1, f_desejado2))+ scale_x_continuous(name=&quot;Valores score (X)&quot;)+ labs(title=&quot;Curva da função densidade \\nDistribuição Qui-quadrado&quot;, subtitle = &quot;P(0; 11,64)=0,96 em cinza (nível de confiança) \\nP(11,64 ; \\U221e)= 0,04 em vermelho (nível de significância) &quot;)+ geom_segment(aes(x = q_desejado, y = 0, xend = q_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=q_desejado+0.5, y=d_desejada, label=&quot;x crítico=11,64&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ annotate(geom=&quot;text&quot;, x=q_desejado+5, y=d_desejada, label=&quot;Zona de rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=q_desejado-8, y=d_desejada, label=&quot;Zona de não rejeição \\n(para x calculado)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = q_calculado, y = 0, xend = q_calculado, yend = d_calculada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=q_calculado+0.5, y=d_calculada, label=&quot;x calculado=6,59&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=4)+ theme_bw() Figure 11.33: Região de rejeição da hipótese nula para o teste uniletaral à direita (tipo: menor que): a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(x_{crit}=11,64\\) para o nível de significância pretendido (\\(\\alpha\\) em uma cauda) e (\\(df\\)) graus de liberdade. Conclusão: O resultado do teste de hipóteses realizado com as amostras trazidas à análise não nos permite rejeitar a afirmação de que os seus valores procedem de uma distribuição Normal (\\(X \\sim \\mathcal{N}(\\stackrel{-}{x}=14,512, S=1,6435)\\)) a um nível de significância de 4% (Figura 11.33). 11.11.5 Teste de significância para as médias de duas populações dependentes O Teste ``t’’ emparelhado é usado quando dados das duas amostras são colhidas de um mesmo indivíduo (ensaio clínico) ou em uma mesma unidade experimental (experimento agronômico) havendo, portanto, dependência entre os valores observados. As possívies estruturas dos testes de hipóteses para duas médias dependentes (amostras emparelhadas) são: Teste de hipóteses bilateral (tipo: diferente de): \\[ \\begin{cases} H_{0}: \\mu_{\\text{dif}} = \\Delta_{0} \\\\ H_{1}: \\mu_{\\text{dif}} \\ne \\Delta_{0} \\end{cases} \\] Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[ \\begin{cases} H_{0}: \\mu_{\\text{dif}} \\ge \\Delta_{0} \\\\ H_{1}: \\mu_{\\text{dif}} &lt; \\Delta_{0} \\end{cases} \\] Teste de hipóteses unilateral à direita (tipo: maior que): \\[ \\begin{cases} H_{0}: \\mu_{\\text{dif}} \\le \\Delta_{0} \\\\ H_{1}: \\mu_{\\text{dif}} &gt; \\Delta_{0} \\end{cases} \\] em que: \\(\\Delta_{0}\\) é, usualmente, 0 (as médias são iguais); e, \\(\\mu_{\\text{dif}} = \\mu_{1} - \\mu_{2}\\) é a diferença entre os pares de observaçõe.; Estatística do teste para amostras Normais (\\(n_{1}\\) e \\(n_{2}\\) quaisquer) ou amostras de outras distribuições, mas desde que \\(n_{1}\\) e \\(n_{2}\\) $ $: \\(t_{cal} = \\frac{\\sqrt{n}\\cdot \\left({\\stackrel{-}{x}}_{dif}-{\\Delta }_{0}\\right)}{{S}_{dif}}\\) \\(\\stackrel{-}{x}_{dif}\\): valor médio das diferenças entre as observações (amostra) \\(S_{dif}\\): desvio padrão das diferenças entre as observações (amostra) \\({t}_{tab\\left(\\frac{\\alpha }{2}; n-1 \\right)}\\) ou \\({t}_{tab\\left(\\alpha ; n-1\\right)}\\): o quantil associado na distribuição ``t’’ de Student ao nível de significância pretendido no teste, com \\((n-1)\\) graus de liberdade. Formulação dos testes com a estatística T (\\(T \\sim t_{(n-1)}\\)): Teste de hipóteses bilateral (tipo: diferente de): \\[\\begin{align*} P[\\left|t_{calc}\\right| \\ge {t}_{tab\\left(\\frac{\\alpha }{2}; n-1 \\right)}|\\mu_{\\text{dif}}=0] &amp; =(1-\\alpha)\\\\ P ( - {t}_{tab\\left(\\frac{\\alpha }{2}; n-1 \\right)} \\le t_{calc} \\le {t}_{tab\\left(\\frac{\\alpha }{2}; n-1 \\right)}) &amp; = (1-\\alpha)\\\\ \\end{align*}\\] As regiões de rejeição (regiões críticas) da hipótese nula podem ser vistas na Figura 11.14. Teste de hipóteses unilateral à esquerda (tipo: menor que): \\[\\begin{align*} P[t_{calc} \\ge {t}_{tab\\left(\\alpha ; n-1\\right)} |\\mu_{\\text{dif}}=0] &amp; =(1-\\alpha)\\\\ P(t_{calc} \\ge {t}_{tab\\left(\\alpha ; n-1\\right)}) &amp; = (1-\\alpha) \\\\ \\end{align*}\\] A região de rejeição (região crítica) da hipótese nula pode ser vista na Figura 11.15. Teste de hipóteses unilateral à direita (tipo: maior que): \\[\\begin{align*} P[t_{calc} \\le {t}_{tab\\left(\\alpha ; n-1\\right)}|\\mu_{\\text{dif}}=0] &amp; = (1-\\alpha)\\\\ P( t_{calc} \\le {t}_{tab\\left(\\alpha ; n-1\\right)}) &amp; = (1-\\alpha)\\\\ \\end{align*}\\] A região de rejeição (região crítica) da hipótese nula pode ser vista na Figura 11.16. Exemplo: Uma empresa precisa tomar a decisão de adquirir uma nova máquinas de usinagem. Contudo, o fornecedor apresentou dois modelos (A e B) de preços diferentes. Para tomar a decisão, convocou 5 de seus funcionários mais experientes e os despachou para a fábrica, que os treinou a executar a mesma tarefa em ambas as máquinas. A tabela abaixo apresenta os tempos gastos pelos funcionários em ambas as máquinas (cf. tabela \\(\\ref{tab7}\\)). No nível de significância de 10% podemos afirmar que a tarefa realizada na máquina \\(A\\) demora mais que na máquina \\(B\\)? Tempo necessário para usinagem de uma mesma peça em duas máquinas diferentes, por 5 operadores diferentes Funcionário Máquina A (h) Máquina B (h) A 80 75 B 72 70 C 65 60 D 78 72 E 85 78 O enunciado do problema deixa bastante claro que as medidas, os tempos gastos para a realização da tarefa nas máquinas A e B foram tomados no mesmo grupo de funcionários, de tal sorte que não nos é possível afirmar que há independência. O Teste ``t’’ é usado quando dados das duas amostras são colhidas de um mesmo sujeito, havendo, portanto dependência entre as amostras. A tabela a seguir apresenta as diferenças de tempo de usinagem entre as máquinas, para cada operador. Diferenças nos tempos de usinagem Funcionário Diferença: A-B (h) A 5 B 2 C 5 D 6 E 7 Média 5,00 Desvio padrão 1,8708 Estrutura do teste: teste de hipóteses unilateral à direita (tipo: maior que): \\[ \\begin{cases} H_{0}: \\mu_{\\text{dif}} (\\mu_{A} - \\mu_{B}) \\le 0 \\\\ H_{1}: \\mu_{\\text{dif}} (\\mu_{A} - \\mu_{B}) &gt; 0 \\end{cases} \\] A hipótese nula afirma que o tempo médio \\(\\mu_{A}\\) é igual ou menor que o tempo médio \\(\\mu_{B}\\); já a hipótese alternativa, contrariamente, afirma que o tempo médio \\(\\mu_{A}\\) é maior que o tempo médio \\(\\mu_{B}\\). Estatística do teste: \\[ t_{cal} = \\frac{\\sqrt{n} \\times \\left({\\stackrel{-}{x}}_{dif}\\right)}{{S}_{dif}} \\] \\[ t_{calc} &gt; {t}_{tab\\left(\\alpha ; (n-1)\\right)} \\] em que: \\(n=5\\); \\({t}_{tab\\left(0,10 ; (5-1) \\right)} = 1,533\\) é o quantil associado na distribuição ``t’’ de Student no nível de significância pretendido no teste e com \\((n-1)\\) graus de liberdade (valor crítico monocaudal); \\(t_{cal} = \\frac{\\sqrt{n}\\cdot \\left({\\stackrel{-}{x}}_{dif}\\right)}{{S}_{dif}} = 5,97\\); \\(\\stackrel{-}{x}_{dif} = 5,00\\) é o valor médio das diferenças entre as observações amostrais; \\(S_{dif} = 1,87\\): desvio padrão das diferenças entre as observações amostrais. alfa=0.90 prob_desejada=alfa df=4 t_desejado=round(qt(prob_desejada,df ),4) d_desejada=dt(t_desejado,df) t_calculado=5.97 d_calculado=dt(t_calculado,df) ggplot(NULL, aes(c(-7,7))) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;lightgrey&quot;, xlim = c(-7, t_desejado), colour=&quot;black&quot;) + geom_area(stat = &quot;function&quot;, fun = dt, args=list(df), fill = &quot;red&quot;, xlim = c(t_desejado,7), colour=&quot;black&quot;) + scale_y_continuous(name=&quot;Densidade&quot;) + scale_x_continuous(name=&quot;Valores de t&quot;, breaks = c(t_desejado)) + labs(title= &quot;Regiões críticas sob a curva da função densidade da \\ndistribuição apropriada ao teste&quot;, subtitle = &quot;P(-\\U221e; 1,53)=(1-\\u03b1) em cinza (nível de confiança=0,90) \\nP(1,53; \\U221e)= \\u03b1 em vermelho (nível de significância=0,10) &quot;)+ geom_segment(aes(x = t_desejado, y = 0, xend = t_desejado, yend = d_desejada), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_desejado-0.1, y=d_desejada, label=&quot;Valor crítico da estatística do teste=1,53&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado-3, y=0.1, label=&quot;Região de não rejeição da hipótese nula \\nprobabilidade=\\u03b1&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ annotate(geom=&quot;text&quot;, x=t_desejado+1, y=0.1, label=&quot;Região de rejeição da hipótese nula \\nprobabilidade= (1-\\u03b1)&quot;, angle=0, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ geom_segment(aes(x = t_calculado, y = 0, xend = t_calculado, yend = d_calculado), color=&quot;blue&quot;, lty=2, lwd=0.3)+ annotate(geom=&quot;text&quot;, x=t_calculado-0.1, y=d_calculado, label=&quot;Valor da estatística do teste=5,97&quot;, angle=90, vjust=0, hjust=0, color=&quot;blue&quot;,size=3)+ theme_bw() Figure 11.34: Região de rejeição da hipótese nula para o teste unilateral à direita (tipo: maior que) realizado: a região de não rejeição da hipótese nula (região de não significância do teste) está delimitada pelo valor crítico da estatística do teste: \\(t_{crit} = 1,53\\). O valor calculado da estatística (\\(t_{calc}=5,97\\)) situa-se na faixa de significância do teste, não possibilitando a rejeição da hipótese nula sob aquele nível de confiança Conclusão: O resultado do teste de hipóteses realizado com as amostras trazidas à análise não nos permite suportar a afirmação de que o tempo médio para a realização da tarefa na máquina \\(A\\) seja menor ou igual ao tempo médio gasto na máquina \\(B\\) a um nível de significância de 10%. O tempo médio na máquina \\(A\\) é maior (Figura 11.34). "],["fluxograma-auxiliar-para-escolha-da-estatística-do-teste-de-hipóteses.html", "11.12 Fluxograma auxiliar para escolha da estatística do teste de hipóteses", " 11.12 Fluxograma auxiliar para escolha da estatística do teste de hipóteses Figure 11.35: Fluxograma auxiliar para escolha da estatística do teste de hipóteses "],["tabelas-1.html", "11.13 Tabelas", " 11.13 Tabelas Figure 11.36: Tabela Normal padronizada Figure 11.37: Tabela da distribuição t de Student Figure 11.38: Tabela da distribuição F de Fisher-Snedecor (5%) Figure 11.39: Tabela da distribuição F de Fisher-Snedecor (2,5%) Figure 11.40: Tabela da distribuição Qui-quadrado Figure 11.41: Alfabeto grego .small-equation80 { font-size: 80%; } .small-equation70 { font-size: 70%; } .small-equation60 { font-size: 60%; } .small-equation40 { font-size: 40%; } blockquote { background-color: #c0c0c0; /* Fundo cinza claro */ border-left: 4px solid #21130d; /* Barra lateral */ padding: 10px; margin: 10px 0; border-radius: 4px; /* Bordas arredondadas */ color: #21130d; /* Cor do texto */ } "],["introdução-à-correlação-linear-de-pearson-e-regressão-linear-simples.html", "Capítulo 12 Introdução à Correlação Linear de Pearson e Regressão Linear Simples", " Capítulo 12 Introdução à Correlação Linear de Pearson e Regressão Linear Simples “Essentially, all models are wrong,but some are useful […]” (George Edward Pelham Box, 1919 - 2013) "],["contexto-histórico.html", "12.1 Contexto histórico", " 12.1 Contexto histórico Sir Francis Galton (1822-1911), antropólogo e meteorologista inglês, propôs no artigo escrito em conjunto com J. D. Hamilton Dickson (Family Likeness in Stature) apresentado à Royal Society of London em 21 de janeiro de 1886, expressar por uma função uma relação que observou entre estaturas de pais e seus filhos e descendentes. Nesse artigo, Galton verificou que, embora houvesse uma tendência de que pais mais altos tivessem filhos altos (e pais mais baixos, filhos mais baixos), a estatura média de crianças nascidas de pais com dada altura tendia a regredir à altura média da população como um todo. Nas palavras de Galton isso seria uma regressão à mediocridade: pais mais altos que a estatura média têm filhos mais baixos que eles “Each peculiarity in a man is shared by his kinsman but, on the average, in a less degree[…]” A Lei da Regressão de Galton foi referendada por Karl Pearson (On the Laws of Inheritance, 1903) poucos anos depois, quando analisou os dados de milhares de registros de estatura, tamanho do antebraço e da palma. Em latim o prefixo co remete ao significado colaboração , união ou até simultaneidade. Correlação significa, portanto, uma relação mútua entre dois termos, uma correspondência. Em Correlations and their Measurement, chiefly from Anthropometric Data, apresentado à Royal Society of London em dezembro de 1888, ele observou aquilo que viria a conceituar como co-relação ou correlação de estrutura. Galton afirmou ao analisar o tamanho do braço com o da perna de um indivíduo que que dois órgãos são ditos serem correlacionados quando a variação de um é acompanhada, na média, pela variação para mais ou menos do outro: se a correlação fosse alta, uma pessoa com um braço longo teria também uma perna longa; se a correlação fosse moderada, o comprimento da perna não seria tão longo e, se não houvesse correlação, o comprimento de sua perna seria o comprimento médio desse membro na população. “…Assim, ele naturalmente atingiu uma linha de regressão reta com variabilidade constante para todas as matrizes de um caractere para um dado caractere de um segundo. Talvez fosse melhor para o progresso do cálculo correlacional que este simples caso especial fosse exposto primeiro: é tão facilmente compreendido pelo iniciante[…]” Houve um momento que Johann Carl Friedrich Gauss considerou sua descoberta (1795) da regressão estatística como “trivial”. O método dos mínimos quadrados parecia tão óbvio para Carl Friedrich Gauss que ele imaginou não ter sido o primeiro a usá-lo. Ele não declarou publicamente sua descoberta até alguns anos depois ( Theoria motus corporum coelestium in sectionibus conicis solem ambientium, 1809), quando seu contemporâneo Adrien-Marie Legendre ( Nouvelles méthodes pour la détermination des orbites des comètes, 1805) publicou o método. Quando Gauss sugeriu que ele o havia usado antes deu-se partida a uma das mais famosas disputas de antecedência na história da ciência. Gauss acabaria recebendo a maior parte do crédito como fundador da regressão, mas não sem uma briga. "],["conceitos-1.html", "12.2 Conceitos", " 12.2 Conceitos Em estatística, a expressão correlação faz referência à relação existente entre variáveis, digamos X e Y que pode assumir diferentes padrões: linear ou não linear (quadrática, cúbica, exponencial …). A correlação existente entre valores de uma mesma variável, digamos X (em diferentes momentos de tempo \\((X_{t_i}, X_{t_j})\\) ou espaço \\((X_{s_i}, X_{s_j})\\) é denominada autocorrelação. 12.2.1 Correlação linear versus regressão a análise de correlação tem como principal objetivo medir a força ou o grau de associação linear entre as duas variáveis. na análise de regressão linear o objetivo primário é expressar matematicamente uma relação linear entre duas variáveis de modo a possibilitar obter estimativas de uma para um valor não amostrado da outra, contruir intervalos de confiança para essas estimativas e testar variadas hipóteses. 12.2.2 Correlação versus causação Embora a análise de regressão lide com o comportamento de uma variável em relação a outra(s), isso não implica necessariamente em causação. É preciso levar em conta que uma relação estatística por si só não implica logicamente uma causação. Para atribuir uma relação de causação deve-se lançar mão de considerações a priori ou teóricas. Considerem a correlação existente entre a altura dos alunos de 6 a 17 anos e as notas médias anuais obtidas em matemática. Naturalmente não é o incremento que os alunos sofrem em suas alturas na fase de crescimento que causa a melhora nas notas; mas sim processos biológicos e comportamentais que resultam em melhorias na capacidade cognitiva. "],["diagrama-de-dispersão.html", "12.3 Diagrama de dispersão", " 12.3 Diagrama de dispersão Descrito pela primeira vez por Francis Galton ( Regression Towards Mediocrity in Hereditary Stature , 1886), os diagramas de dispersão ( scatterplot ) ou gráficos de dispersão são representações de dados de duas (tipicamente) ou mais variáveis que são organizadas em um gráfico. O gráfico de dispersão utiliza coordenadas cartesianas para exibir valores de um conjunto de dados. Os dados são exibidos como uma coleção de pontos, cada um com o valor de uma variável determinando a posição no eixo horizontal e o valor da outra variável determinando a posição no eixo vertical (em caso de duas variáveis). Considerem as simulações da dispersão de alguns valores de duas variáveis \\(X\\) e \\(Y\\). Vemos que em alguns casos nos parece ser razoável tentar exprimir qualquer tipo de relação entre os valores de \\(X\\) e \\(Y\\); todavia, há situações onde claramente vemos alguma forma de relação. Essas formas bem poderiam ser expressas, aproximadamente, por diferentes funções como: lineares (retas) ou não lineares (curvas). Vemos também que essas formas de associação entre os valores de \\(X\\) e \\(Y\\) podem ser diretas ou inversamente proporcionais (“positiva” ou “negativa”). Estamos particularmente interessados em quantificar o grau da relação dos valores de \\(X\\) e \\(Y\\) nos padrões lineares. (SIMULADOR 1) "],["coeficiente-de-correlação-linear-de-pearson.html", "12.4 Coeficiente de correlação linear de Pearson", " 12.4 Coeficiente de correlação linear de Pearson O mais importante aspecto da correlação linear é a medida de sua intensidade, expressa pelo coeficiente de correlação linear (ou coeficiente de correlação produto momento de Pearson). A notação adotada para o coeficiente de correlação linear de Pearson depende dos dados analisados: se são dados amostrais ou populacionais: - população: pela letra grega \\(\\rho\\) (“rô”) - amostra: pela letra latina r Cálculo do coeficiente de correlação amostral \\(r\\): \\[r = \\frac{\\sum _{i=1}^{n}{x}_{i} \\cdot {y}_{i} - \\frac{\\sum _{i=1}^{n}{x}_{i}\\sum _{i=1}^{n}{y}_{i}}{n}}{\\sqrt{\\left(\\sum _{i=1}^{n}{x}_{i}^{2}-\\frac{{\\left(\\sum _{i=1}^{n}{x}_{i}\\right)}^{2}}{n}\\right)\\cdot \\left[\\sum_{i=1}^{n}{y}_{i}^{2}-\\frac{{\\left(\\sum _{i=1}^{n}{y}_{i}\\right)}^{2}}{n}\\right]}}\\] em que \\(x_{i}\\): é o iésimo valor observado da variável X, \\(y_{i}\\): é o iésimo valor observado da variável Y, \\(n\\) é o número de pares de valores observados. Ou, simplificadamente: \\[ r = \\frac{{s}_{xy}}{\\sqrt{{s}_{xx}\\cdot {s}_{yy}}} \\] em que \\(S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i}\\) - \\(\\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n}\\), \\(S_{xx} = \\sum _{i=1}^{n} x_{i}^{2}\\) - \\(\\frac{(\\sum _{i=1}^{n} x_{i})^{2}}{n}\\), \\({S}_{yy}=\\sum _{i=1}^{n}y_{i}^{2}\\) - \\(\\frac{(\\sum _{i=1}^{n} y_{i})^{2}}{n}\\) e \\(n\\) é o número de pares de valores observados. o coeficiente de correlação linear de Pearson tem uma faixa limitada de variação: \\(-1 \\le r \\le 1\\), é simétrico; isto é, a correlação linear observada entre \\(X\\) e \\(Y\\) é a mesma que a medida entre as variáveis \\(Y\\) e \\(X\\), é apenas uma medida da associação linear entre duas variáveis e, portanto, não tem sentido usá-lo na quantificação de relações que não o sejam, a possibilidade de uma correlação linear negativa virá do resultado do numerador (\\(S_{xy}\\)), pois no denominador temos duas somas de quadrados, o coeficiente de correlação mede apenas a intensidade das relações lineares entre \\(x\\) e \\(y\\) e não estabelece per si nenhuma relação de causação. se \\(r&gt;0\\) dizemos que há uma relação linear positiva entre as variáveis estudadas: para um incremento na primeira variável observa-se também um incremento na segunda; se \\(r&lt;0\\) a relação linear é negativa: um incremento em uma das variáveis é acompanhado por um decremento na outra; e, se \\(r=0\\), então não há uma relação linear entre as variáveis consideradas. O cálculo do coeficiente de correlação linear de Pearson assemelha-se a uma análise de variância \\[ y - \\stackrel{-}{y} = (\\hat{y} - \\stackrel{-}{y}) + (y - \\hat{y}) \\] Elevando-se ao quadrado ambos os termos, para todos os valores observados, teremos: \\[ \\sum _{i=1}^{n} ({y_{i}} - \\stackrel{-}{y_{i}})^{2} = \\sum _{i=1}^{n} (\\hat{y_{i}} - \\stackrel{-}{y_{i}})^{2} + \\sum _{i=1}^{n} (y_{i} - \\hat{y_{i}})^{2} \\] A quantidade à esquerda mede a variação total dos y (Soma de quadrados total); à direita temos a Soma de quadrados da regressão e a Soma de quadrados dos resíduos e, \\[ r=\\sqrt{\\frac{\\sum _{i=1}^{n} (\\hat{y_{i}} - \\stackrel{-}{y_{i}})^{2}}{\\sum _{i=1}^{n} ({y_{i}} - \\stackrel{-}{y_{i}})^{2}}} \\] A definição acima de \\(r\\) nos diz que \\(100.r^{2}\\) é a percentagem da variação total dos \\(y\\) que está sendo explicada por sua regressão linear com \\(x\\). Exemplo 1: Um jornal deseja verificar a eficácia de seus anúncios na venda de carros usados e para isso realizou um levantamento de todos os seus anúncios e informações dos resultados obtidos pelas empresas que o contrataram e dele extraiu uma pequena amostra. A tabela a seguir mostra o número de anúncios e o correspondente número de veículos vendidos por 6 companhias que usaram apenas este jornal como veículo de propaganda. Existe alguma relação linear entre as variáveis? Construa o diagrama de dispersão e calcule o coeficiente de correlação linear. Quadro de dados da quantidade de carros vendidos por 6 empresas distintas pela quantidade de anúncios feitos Companhia Anúncios feitos (X) Carros vendidos (Y) A 74 139 B 45 108 C 48 98 D 36 76 E 27 62 F 16 57 Quadro para cálculo do coeficiente de correlação linear (\\(r\\)) Companhia Anúncios (X) Carros vendidos (Y) xi * yi xi2 yi2 A 74 139 10286 5476 19321 B 45 108 4860 2025 11664 C 48 98 4704 2304 9604 D 36 76 2736 1296 5776 E 27 62 1674 729 3844 F 16 57 912 256 3249 Totais 246 540 25172 12086 53458 Sendo \\(n= 6\\) temos: \\[ S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i} - \\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n} = 25172 - \\frac{246 \\cdot 540}{6} = 3032\\\\ S_{xx} = \\sum _{i=1}^{n} x_{i}^{2} - \\frac{(\\sum _{i=1}^{n} x_{i})^{2}}{n} = 12086 - \\frac{246^2}{6} = 2000\\\\ {S}_{yy} = \\sum _{i=1}^{n}y_{i}^{2} - \\frac{(\\sum _{i=1}^{n} y_{i})^{2}}{n}= 53458 - \\frac{540^2}{6} = 4858 \\] Portanto: \\[ r = \\frac{{s}_{xy}}{\\sqrt{{s}_{xx}\\cdot {s}_{yy}}} = \\frac{3032}{\\sqrt{2000 \\cdot 4858}} = 0,9727 \\] "],["teste-de-hipóteses-para-a-correlação-linear-na-população.html", "12.5 Teste de hipóteses para a correlação linear na população", " 12.5 Teste de hipóteses para a correlação linear na população O coeficiente de correlação populacional \\(\\rho\\) sempre é estimado a partir do coeficiente de correlação amostral \\(r\\). Para se realizar inferências concernentes a \\(\\rho\\) a partir de \\(r\\) temos que ter o conhecimento da distribuição amostral dos coeficientes de correlação linear \\(r\\). Para se testar a existência de correlação na população um teste de hipóteses na estrutura seguinte (bilateral) pode ser proposto: \\[ \\begin{cases} H_{0}:\\rho = 0 \\hspace{0.1cm} \\text{, ie. a correlação linear entre X e Y é nula} \\\\ H_{1}:\\rho \\ne 0 \\hspace{0.1cm} \\text{, ie. a correlação linear entre X e Y não é nula} \\\\ \\end{cases} \\] Lembrando que um teste de hipóteses guarda uma certa semelhança a um julgamento: caso não haja indício algum que comprove a culpa do acusado ele é declarado inocente. Seguindo essa analogia, o indício ou evidência que nos permitirá rejeitar a hipótese nula virá de uma evidência amostral. A quantificação da relevância da evidência amostral virá de uma estatística calculada (\\({t}_{calc}\\)) a partir do coeficiente de correlação amostral \\(r\\) e o tamanho amostral \\(n\\), que será comparado a um valor limite tabelado (\\(t_{tab}\\)) da correspondente distribuição da variável aleatória \\(T\\): A estatística do teste é: \\[ {t}_{calc}=\\frac{r\\cdot\\sqrt{n-2}}{\\sqrt{1-{r}^{2}}}\\\\ T \\sim t_{(n-2)} \\] Rejeita-se a hipótese nula (\\(H_{0}\\)) se o valor da estatística for tão extremo que se verifique: \\[ t_{calc} \\le {t}_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]}\\\\ \\text{ou}\\\\ t_{calc} \\ge {t}_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]} \\] em que \\(t_{tab}\\) é o quantil associado na distribuição “t” de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido (\\(\\alpha\\)) com \\((n-2)\\) graus de liberdade. O número de graus de liberdade irá determinar qual curva da família dessa distribuição será utilizada, por essa razão, as tabelas apresentam-se individualizadas por nível de significância e graus de liberdade. As curvas da família “t” possuem simetria em relação a um eixo vertical central. O valor tabelado dessa estatística acha-se associado à área sob ela pois é uma função densidade de probabilidade: a totalidade da área sob essa curva é igual a 1 (probabilidade de 100%) Assim, se consultarmos em uma tabela o valor “t” para um nível de significância \\(\\alpha\\) qualquer, correspodente assim a um nível de confiança de (\\(1-\\alpha\\)), qualquer veremos que ele será igual, em módulo, ao valor “t” no outro extremo dessa curva. Por essa razão muitas tabelas apresentam valores dessa estatística sob os títulos de monocaudal ou bicaudal pois estão apresentando os valores para um determinado nível de significância (\\(\\alpha\\)): área sob a curva, situado apenas em um lado (ou subdividido nos dois ramos da curva nas tabelas chamadas “bilaterais”). O teste de hipótese que iremos realizar é um teste bilateral; assim, o gráfico apropriado para se decidir pela rejeição ou não da hipótese nula assume a forma mostrada nessa simulação. (SIMULADOR 2 COM t) 12.5.1 Outros testes de hipóteses sobre a correlação linear na população Outros tipos de testes só podem ser realizados através da estatística \\(\\zeta\\) (zeta) de Fisher. A transformação \\(Z\\) proposta por Fisher produz uma estatística que possui distribuição aproximadamente Normal. Para essa situação a estatística a ser utilizada é dada por: \\[ \\zeta = \\frac{1}{2}.ln\\frac{(1+r)}{(1-r)} \\] que possui uma distribuição aproximadamente Normal, com média e desvio padrão: \\[ \\mu_{\\zeta} = \\frac{1}{2}.ln\\frac{(1+\\rho_{0})}{(1-(\\rho_{0})} \\text{ e } \\sigma_{\\zeta} = \\frac{1}{\\sqrt{n-3}}. \\] Transformando-se \\(\\zeta\\) em unidades padrão (pela subtração de \\(\\mu_{\\zeta}\\) e divisão por \\(\\sigma_{\\zeta}\\)), chega-se à estatística tabelada \\(z = (Z - \\mu_{\\zeta}) . \\sqrt{n-3}\\). Exemplo 2: Faça o teste de hipóteses para a correlação linear \\(\\rho\\) a partir da correlação amostral \\(r\\) calculada no exercício dos anúncios de veículos, sob um nível de significância (\\(\\alpha\\)) de 0,05. No exercício referido obtivemos um valor para a correlação linear de Pearson de \\(r=0,9727\\). A partir desse valor podemos calcular o valor de nossa estatística \\({t}_{calc}\\) para o teste: \\[ {t}_{calc}=\\frac{r\\cdot\\sqrt{n-2}}{\\sqrt{1-{r}^{2}}} = 8,38 \\] Rejeitaremos a hipótese nula (\\(H_{0}\\)) se: \\[ t_{calc} \\le {t}_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]}\\\\ \\text{ou}\\\\ t_{calc} \\ge {t}_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]} \\] Da tabela extraímos o valor de nossa estatística de comparação a um nível de significância \\(\\alpha=5\\%\\) e, para um tamanho amostral \\(n=6\\), temos como graus de liberdade \\(n-2=4\\) (\\(t_{tab}=2,776\\)). Vê-se que o valor calculado da estatística “t” encontra-se além dos limites estabelecidos pela estatística de comparação (\\(t_{tab}\\)) para um nível de significância de \\(\\alpha=5\\%\\) (SIMULADOR 2 COM t) "],["regressão-linear-simples.html", "12.6 Regressão linear simples", " 12.6 Regressão linear simples 12.6.1 Introdução Considerem a proposição de John Maynard Keynes para a relação entre o consumo e a renda, onde ele postulava haver uma relação positiva entre ambos: uma mudança em uma das variáveis iria alterar a outra. Seu modelo funcional para essa relação, com \\(Y\\) sendo as despesas de consumo e \\(X\\) a renda, é: \\[ Y = \\alpha + \\beta \\cdot X \\] Esse modelo admite que a verdadeira relação entre \\(Y\\) e \\(X\\) seja uma linha reta e que a observação \\(Y\\) para cada nível de \\(X\\) seja uma variável aleatória. Assim, o valor esperado de \\(Y\\) para cada valor de \\(X\\) é: \\[ Y_i = E(Y | X_i) = \\alpha + \\beta \\cdot X_i \\] Nesse modelo, \\(\\alpha\\) e \\(\\beta\\) são parâmetros desconhecidos da relação estabelecida entre as duas populações: \\(\\alpha\\): intercepto (um consumo mínimo é observado mesmo nas situações em que a renda é nula, em razão de programas de assistência governamental). \\(\\beta\\): inclinação (a propensão média do crescimento do consumo com o incremento da renda). É um modelo puramente teórico, de limitada aplicabilidade prática, pois pretende exprimir por uma relação exata (determinística) o consumo e a renda, quando se sabe que grande parte das relações entre duas variáveis não são exatas. Entretanto, ao se fixar um único valor para a variável explicativa, observa-se que há flutuações nos valores observados da variável explicada. Essa inexatidão, esse desvio do valor observado \\(Y_i\\) em relação ao seu valor esperado, pode ser expresso da seguinte maneira: \\[ \\varepsilon_i = Y_i - E(Y | X_i) \\] em que \\(E(Y | X_i)\\) é denominado componente sistemático ou determinístico, representando o gasto médio de todas as famílias com um mesmo nível de renda, e \\(\\varepsilon_i\\) é denominado termo de erro ou distúrbio estocástico. O termo de erro pode ser admitido como um substituto para todas as demais variáveis omitidas ou negligenciadas no modelo e que podem afetar \\(Y\\). Um modelo de regressão pode ser linear nas variáveis ou nos parâmetros. Uma função \\(Y = f(X)\\) é dita linear em \\(X\\) se \\(X\\) tiver um expoente igual a 1 e não estiver multiplicado ou dividido por outra variável. a função \\(Y = \\alpha + \\beta \\cdot X\\) é dita linear em \\(\\beta\\) se \\(\\beta\\) tiver um expoente de 1 e não estiver multiplicado ou dividido por qualquer outro parâmetro. A função \\(E(Y | X) = \\alpha + \\beta \\cdot X^2\\) não é linear em \\(X\\), pois \\(X\\) está elevado ao quadrado. mas é linear nos parâmetros, pois, para \\(X = 3\\), temos \\(E(Y | X = 3) = \\alpha + 9 \\cdot \\beta\\). Das duas interpretações de linearidade, a linearidade nos parâmetros é a relevante para a formulação da teoria da regressão (a linearidade nas variáveis pode ou não ocorrer). No contexto deste curso, o modelo será linear tanto nos parâmetros quanto na variável. Admitindo-se que \\(E(Y | X_i)\\) seja linear em \\(X_i\\), podemos reescrever o modelo original na forma que incorpora o erro aleatório: \\[ Y_i = E(Y | X_i) = \\alpha + \\beta \\cdot X_i \\\\ Y_i = E(Y | X_i) + \\varepsilon_i \\\\ Y_i = \\alpha + \\beta \\cdot X_i + \\varepsilon_i \\] em que \\(\\alpha\\) é o intercepto da reta, representando o valor esperado da variável \\(Y\\) quando \\(X = 0\\), \\(\\beta\\) é a inclinação da reta, representando a variação esperada de \\(Y\\) para um aumento unitário em \\(X_i\\), (\\(\\alpha + \\beta \\cdot X_i\\)) é a parte explicada pelo modelo e \\(\\varepsilon_i\\) é o termo de erro ou distúrbio estocástico. Nessa função: \\(Y\\): variável dependente (também chamada de explicada, prevista, regressando, resposta, endógena, saída, controlada) — aqui, representando o consumo. \\(X\\): variável independente (também chamada de explicativa, previsora, regressor, estímulo, exógena, entrada, controle) — aqui, representando a renda. Se o termo de erro \\(\\varepsilon_i\\) representa todas aquelas variáveis omitidas no modelo (mas que, coletivamente, afetam \\(Y\\)), por que não formular um modelo de regressão com o máximo de variáveis possíveis? Embasamento teórico vago: A teoria existente suporta com certeza apenas algumas variáveis; o termo de erro \\(\\varepsilon_i\\) serve como um substituto para todas as variáveis excluídas no modelo. Princípio da parcimônia: Um modelo mais simples que explique bem a relação é preferível. Forma funcional equivocada: Em gráficos de dispersão, é mais fácil inferir a relação entre duas variáveis do que com muitas. Limitação na quantidade de observações: Muitas variáveis exigem mais observações para garantir a precisão do modelo. Sendo inviável, e muitas vezes impossível, construir um modelo populacional, focamos o estudo em uma parte dessa população: uma amostra. Um modelo funcional estimado com base em uma amostra apresenta estimativas dos parâmetros da função que descreve a população de origem (os quais são desconhecidos). Por isso, adota-se uma notação diferente para a função de regressão amostral em sua forma estocástica: \\[ \\hat{Y} = a + b \\cdot X \\] em que \\(\\hat{Y}\\) é um estimador de \\(E(Y | X)\\), \\(a\\) é uma estimativa do parâmetro \\(\\alpha\\) e \\(b\\) é uma estimativa do parâmetro \\(\\beta\\). Para um determinado valor de \\(X = x_i\\), temos uma observação amostral \\(Y = y_i\\) que pode ser expressa pela função de regressão amostral como: \\[ y_i = \\hat{y}_i + e_i \\\\ y_i = a + b \\cdot x_i + e_i \\] em que \\(\\hat{y}_i\\) é o valor estimado de \\(Y_i\\) para um determinado \\(X_i\\), \\(e_i\\) é o erro amostral, que representa a diferença entre o valor observado \\(y_i\\) e o valor estimado \\(\\hat{y}_i\\). Mas, como estimar \\(a\\) e \\(b\\)? 12.6.2 Método dos mínimos quadrados Na literatura estatística há vários métodos de estimação dos parâmetros de um modelo de regressão linear, dentre os quais: Método dos momentos (creditado a Karl Pearson-1895, Ronald Aylmer Fisher-1925, Neyman e Egon Pearson-1928, publicado por Lars Peter Hansen-1982); Método da máxima verossimilhaça (creditado a Johann Carl Friedrich Gauss, Pierre-Simon Laplace, Thorvald N. Thiele e Francis Ysidro Edgeworth, popularizado por Ronald Aylmer Fisher, 1912-1922); e, Método dos mínimos quadrados (creditado a Johann Carl Friedrich Gauss-1795, publicado por Adrien-Marie Legendre-1805, Friedrich Robert Helmert-1872 ). 12.6.2.1 Contexto histórico Desde tempos remotos as pessoas têm se interessado pelo problema de escolher o melhor valor único (médio) para resumir as informações fornecidas por várias observações, cada uma sujeita a erro. O problema de se estimar as constantes na equação da linha reta que melhor se ajusta a três ou mais pontos não colineares no plano (x, y) cujas coordenadas são pares de valores associados de duas variáveis relacionadas: \\(X\\) e \\(Y\\) remonta a Galileu Galilei (1632). Credita-se Johann Carl Friedrich Gauss como o desenvolvedor das bases fundamentais do Método dos mínimos quadrados, em 1795, quando Gauss tinha apenas dezoito anos. Mas o Método dos mínimos quadrados foi publicado pela primeira vez por por Adrien-Marie Legendre (1752-1833) em 1805: Nouvelles méthodes pour la détermination des orbites des comètes. Alguns demonstradores: Robert Adrain (1775-1843) em 1808: Research concerning the probabilities of the errors which happen in making observations Johann Carl Friedrich Gauss (1777-1855) em 1809: Theoria motus corporum coelestium Pierre-Simon Laplace (1749-1827) em 1810: Theorie analytique des Probabilite - Johann Carl Friedrich Gauss (1777-1855) em 1823: Theoria combinationis observationum erroribus obnoxiae James Ivory (1765-1842) em 1825: On the Method of the Least Squares. Para o modelo \\(y_{i}= a + b.x_{i}\\) na simulação mostrada: problema: determinar as constantes \\(a\\) e \\(b\\) da equação de uma linha reta que melhor se ajusta a três ou mais pontos não colineares solução: minimizar a soma dos quadrados dos resíduos como mostrado na simulação. \\[ \\sum _{i=1}^{n}{e}_{i}^{2} \\rightarrow 0 \\] A grande vantagem do método dos mínimos quadrados é que ele é um método puramente geométrico, e não faz nenhuma suposição sobre a distribuição dos dados ou dos erros (resíduos). Em outras palavras, ele é aplicado sem se preocupar com a natureza probabilística dos erros (resíduos). O objetivo é apenas ajustar a melhor reta possível para um conjunto de pontos de dados (SIMULADOR 3) Matematicamente, a partir da igualdade: \\[ \\sum _{i=1}^{n} [ y_{i} - \\hat{y} ]^{2} = \\sum _{i=1}^{n}{\\left[yi-\\left(a{x}_{i}+b\\right)\\right]}^{2} \\] a solução passar por derivar-se em relação a: \\(a|b \\text{ fixo}\\), e em relação b: \\(b|a \\text{ fixo}\\), igualando-se a zero: \\[ \\frac{\\delta }{\\delta a}\\sum _{i=1}^{n}{\\left[yi-\\left(a{x}_{i}+b\\right)\\right]}^{2}= 2 \\cdot \\sum _{i=1}^{n}\\left({y}_{i}-a{x}_{i}-b\\right)\\left(-{x}_{i}\\right)=0 \\\\ \\frac{\\delta }{\\delta b}\\sum _{i=1}^{n}{\\left[yi-\\left(a{x}_{i}+b\\right)\\right]}^{2}= 2\\cdot \\sum _{i=1}^{n}\\left({y}_{i}-a{x}_{i}-b\\right)\\left(-1\\right)=0 \\] Após algumas manipulações algébricas obtemos as seguintes expressões para as estimativas: \\(a\\) e \\(b\\): \\[ b\\cdot n+a\\cdot \\sum _{i=1}^{n}{x}_{i}=\\sum _{i=1}^{n}{y}_{i} \\] \\[ b\\cdot \\sum _{i=1}^{n}{x}_{i}+a\\cdot \\sum _{i=1}^{n}{x}_{i}^{2}=\\sum _{i=1}^{n}{x}_{i}\\cdot {y}_{i} \\] chegando-se ao estimador para b: \\[ b=\\frac{n\\cdot \\left(\\sum _{i=1}^{n}{x}_{i}{y}_{i}\\right)-\\sum _{i=1}^{n}{x}_{i}\\sum _{i=1}^{n}{y}_{i}}{n\\cdot \\sum _{i=1}^{n}{x}_{i}^{2}-{\\left(\\sum _{i=1}^{n}{x}_{i}\\right)}^{2}} \\] e ao estimador para a: \\[ a=\\frac{\\left(\\sum _{i=1}^{n}{x}_{i}^{2}\\right)\\cdot \\left(\\sum _{i=1}^{n}{y}_{i}\\right)-\\left(\\sum _{i=1}^{m}{x}_{i}{y}_{i}\\right)\\cdot \\left(\\sum _{i=1}^{n}{x}_{i}\\right)}{n\\cdot \\left(\\sum _{i=1}^{n}{x}_{\\stackrel{.}{i}}^{2}\\right)-{\\left(\\sum _{i=1}^{n}{x}_{i}\\right)}^{2}} \\] Se definirmos \\(S_{xy}\\) e \\(S_{xx}\\) como sendo: \\[ S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i} - \\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n} \\] e \\[ S_{xx} = \\sum _{i=1}^{n} x_{i}^{2} - \\frac{(\\sum _{i=1}^{n} x_{i})^{2}}{n} \\] então podemos escrever: \\[ b = \\frac{S_{xy}}{S_ {xx}}\\\\ \\text{e} \\\\ a = \\stackrel{-}{y} - b\\cdot\\stackrel{-}{x} \\] Uma vez que \\[ \\stackrel{-}{y}=\\frac{\\sum _{i=1}^{n}{y}_{i}}{n}\\\\ \\text{e}\\\\ \\stackrel{-}{x}=\\frac{\\sum _{i=1}^{n}{x}_{i}}{n} \\] o estimador \\(a\\) pode ser reescrito na forma: \\[ a = \\frac{\\sum _{i=1}^{n}{y}_{i} - b . \\sum _{i=1}^{n}{x}_{i}}{n} \\] Exemplo 3: Um jornal deseja verificar a eficácia de seus anúncios na venda de carros usados e para isso realizou um levantamento de todos os seus anúncios e informações dos resultados obtidos pelas empresas que o contrataram e dele extraiu uma pequena amostra. A tabela abaixo mostra o número de anúncios e o correspondente número de veículos vendidos por 6 companhias que usaram apenas este jornal como veículo de propaganda. Obtenha a equação de regressão linear simples e estime o número de carros vendidos para um volume de 70 anúncios? Quadro de dados da quantidade de carros vendidos por 6 empresas distintas em função da quantidade de anúncios feitos Companhia Anúncios feitos (X) Carros vendidos (Y) A 74 139 B 45 108 C 48 98 D 36 76 E 27 62 F 16 57 Quadro para cálculo das estimativas a e b dos parâmetros do modelo Companhia Anúncios (x) Carros vendidos (y) xi.yi xi2 yi2 A 74 139 10286 5476 19321 B 45 108 4860 2025 11664 C 48 98 4704 2304 9604 D 36 76 2736 1296 5776 E 27 62 1674 729 3844 F 16 57 912 256 3249 Totais 246 540 25172 12086 53458 Valor médio 41 90 Sendo \\(n= 6\\), \\(\\stackrel{-}{y}= 90\\) e \\(\\stackrel{-}{x} = 41\\): \\[ S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i} - \\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n} = 25172 - \\frac{246 \\cdot 540}{6} = 3032 \\\\ S_{xx} = \\sum _{i=1}^{n} x_{i}^{2} - \\frac{(\\sum _{i=1}^{n} x_{i})^{2}}{n} = 12086 - \\frac{246^2}{6} = 2000 \\\\ {S}_{yy} = \\sum _{i=1}^{n}y_{i}^{2} - \\frac{(\\sum _{i=1}^{n} y_{i})^{2}}{n}= 53458 - \\frac{540^2}{6} = 4858 \\] As estimativas dos parâmetros do modelo serão: \\[ b = \\frac{S_{xy}}{S_ {xx}} = \\frac{3032}{2000} = 1,5160 \\] e \\[ a = \\stackrel{-}{y} - b\\cdot\\stackrel{-}{x} = 90 - 1,5160 \\cdot 41 = 27,844 \\] e o modelo toma a seguinte forma \\(\\hat{y} = 27,844 + 1,5160 \\cdot x\\). Para um volume de anúncios de 70 veiculações teremos, em média, 134 carros vendidos. "],["modelo-de-regressão-linear-sob-erros-normais.html", "12.7 Modelo de regressão linear sob erros Normais", " 12.7 Modelo de regressão linear sob erros Normais Embora o método dos mínimos quadrados forneça estimativas para \\(a\\) e \\(b\\), ele não nos diz nada sobre a incerteza dessas estimativas. Não podemos fazer inferências estatísticas tais como construir intervalos de confiança ou realizar testes de hipóteses, a menos que façamos suposições adicionais sobre os erros do modelo. Para realizar inferências estatísticas, introduzimos um modelo de regressão linear com erro normal, que assume: os erros ( \\(\\varepsilon_{i}\\)) são variáveis aleatórias Normalmente distribuídas com média zero e variância constante (\\(\\sigma^{2}\\)): \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) os erros são independentes entre si a relação entre \\(Y_i\\) e \\(X_i\\) é linear, descrita pela equação \\(Y_{i} = \\alpha + \\beta X_{i} + \\varepsilon_{i}\\) 12.7.1 Propriedades dos Estimadores sob Erro Normal Demonstra-se que, para um modelo \\(Y_{i}=\\alpha+\\beta\\cdot X_{i}+\\varepsilon_{i}\\) que: \\(b\\) é um estimador não tendencioso do parâmetro \\(\\beta\\) com: \\[ E\\left(b\\right)=\\beta \\\\ \\text{e} \\\\ Var\\left(b\\right)=\\frac{{\\sigma }^{2}}{{S}_{xx}} \\] \\(a\\) é um estimador não tendencioso do parâmetro \\(\\alpha\\) com: \\[ E\\left(a\\right)=\\alpha\\\\ \\text{e} \\\\ Var\\left(a\\right)={\\sigma }^{2}\\cdot \\left(\\frac{1}{n}+\\frac{{\\stackrel{-}{X}}^{2}}{{S}_{xx}}\\right) \\] \\(\\hat{\\sigma^{2}}\\) é um estimador não tendencioso de \\(\\sigma^{2}\\): \\[ \\hat{\\sigma^{2}} = \\text{QMR} = \\frac{S_{yy} -b \\cdot S{xy}}{n-2} \\] Assim as variâncias dos estimadores \\(a\\) e \\(b\\) serão, \\[ s_{b} = \\sqrt{\\frac{{\\hat{\\sigma}}^{2}}{{S}_{xx}}} = \\sqrt{\\frac{\\text{QMRES}}{S_{xx}}} \\] \\[ s_{a} = \\sqrt{{\\hat{\\sigma} }^{2}\\cdot \\left(\\frac{1}{n}+\\frac{{\\stackrel{-}{x}}^{2}}{{S}_{xx}}\\right)} = \\sqrt{\\text{QMRES} \\cdot \\left(\\frac{1}{n}+\\frac{{\\stackrel{-}{x}}^{2}}{{S}_{xx}}\\right)} \\] lembrando que: \\[ S_{yy} = \\sum (Y_i - \\bar{Y})^2 \\\\ S_{xy} = \\sum (X_i - \\bar{X})(Y_i - \\bar{Y}), \\] e \\(n - 2\\) representa os graus de liberdade, já que dois parâmetros (\\(\\alpha\\) e \\(\\beta\\)) são estimados. 12.7.2 Implicações da Normalidade A normalidade dos resíduos \\(\\varepsilon_i\\) garante que os estimadores \\(a\\) e \\(b\\) também sejam Normalmente distribuídos, o que é fundamental para realizar testes de hipóteses e construir intervalos de confiança nos modelos de regressão linear. Isso permite o uso de distribuições de referência, como as distribuições \\(t\\) e \\(F\\), especialmente em amostras pequenas, onde a variância dos estimadores não pode ser assumida como conhecida com precisão. Na estimação de um modelo de regressão linear simples com erro Normal (na forma \\(Y=\\beta_{0}+\\beta_{1}X+ \\varepsilon\\)) muitas premissas preliminarmente como válidas deverão ser efetivamente verificadas a posteriori, na chamada etapa de diagnóstico do modelo, de modo a que a condução de inferências com esse modelo sejam dotada de razoável segurança. Essas premissas podem ser classificadas em quatro categorias: linearidade da relação entre a variável preditora \\(X\\) e a variável resposta \\(Y\\): o valor esperado da variável resposta é uma função linear da variável preditora Normalidade: \\(\\varepsilon_{i} \\sim N (0,\\sigma_{i}^{2})\\) independência estatística dos resíduos: \\(Cov(\\varepsilon_{i},\\varepsilon_{j})=E(\\varepsilon_{i},\\varepsilon_{j})=0, i \\neq j\\) e, em particular, nenhuma correlação entre erros de observações sucessivas no caso de dados provenientes de uma série; e, homogeneidade da variância dos resíduos (homocedasticidade): \\(Var(\\varepsilon_{i})=E(\\varepsilon{i}^{2})=\\sigma_{i}^{2}\\) quando analisada frente aos valores estimados pelo modelo (\\(\\hat{Y}\\)), a variável peditora (\\(X\\)) ou o tempo de coleta nos casos de dados provenientes de uma série Se qualquer uma dessas premissas for violada então uma conclusão científica baseada em resultados advindos desse modelo de regressão poderá estar seriamente comprometida. As violações desses pressupostos não podem ser detectadas pelas estatísticas de resumo do modelo que usualmente se dipõe logo após sua estimação: estatísticas \\(t\\), \\(F\\) dos testes de significância ou então o coeficiente de determinação \\(R^{2}\\). Assim, é sobretudo fundamental examinar mais aprofundadamente o modelo de modo a se assegurar com razoável confiança de sua adequação aos dados antes de se avançar com seu uso. A esse exame denominamos diagnóstico do modelo. 12.7.3 Linearidade na relação entre a variável preditora \\(X\\) e a variável resposta \\(Y\\): A violação da linearidade é extremamente graves pois um modelo ajustado a dados não lineares leva a previsões equivocadas não somente para valores situados além das fronteiras amostrais (como se usualmente observa) mas também para valores próximos ao seu centro. Uma técnica gráfica para se verificar a linearidade da relação é através de dois gráficos: valores observados em relação aos valores estimados; ou/e, resíduos contra valores estimados (ou valores observados). Os padrões desejados nos gráficos acima deve assemelhar-se a: pontos dispersos de modo aproximadamente simétrico em torno de uma linha diagonal; e, pontos dispersos de modo aproximadamente simétrico em torno de uma linha horizontal, com uma variância aproximadamente homogênea. Relações não lineares devem ser tratadas por meio da aplicação de uma transformação não linear adequada ao padrão da relação na variável resposta ou no variável preditora. Para dados estritamente positivos com uma relação não linear a transformação com a função logaritmo pode ser uma opção. Se uma a transformação com o uso da função logaritmo é aplicada apenas à variável resposta isso equivalente a assumir que ela cresce (ou decai) exponencialmente como uma função da variável preditora. Outra possibilidade a considerar é adicionar outra variável preditora na forma de uma função não linear como, por exepmplo, nos padrões de dispersão que mostrem uma curva parabólica onde pode fazer sentido regredir \\(Y\\) em função de \\(X\\) e \\(X^{2}\\). Finalmente, a relação não linear observada pode decorrer da omissão de outra(s) variáveis importantes que explicam ou corrigem o padrão não linear quando então modelos de regressão linear múltipla devem ser estudados. 12.7.4 Homogeneidade da variância de \\(\\varepsilon\\) (homocedasticidade): A violação da homogeneidade de variância dos resíduos (heterocedasticidade) resulta numa estimação imprecisa do verdadeiro desvio padrão dos erros das estimativas e acarreta em intervalos de confiança irreais: são mais amplos ou mais estreitos do que deveriam ser, e resultam em elevada imprecisão nas inferências feitas com estatísticas baseadas na variância (\\(t\\), \\(F\\)). Com variância constante (homocedasticidade) temos que \\(Var(\\varepsilon|X_{i})=\\sigma^{2}\\); todavia o que se observa em muitas situações é que a variância está relacionada de algum modo funcional com a média (\\(\\sigma^{2}=\\mathcal{f}(X)\\)) e, assim: \\[ \\begin{aligned} Var(\\varepsilon_{i}|X_{i})=\\sigma^{2}_{i} \\\\ E(\\varepsilon_{i}^{2})=\\sigma^{2}_{i} \\end{aligned} \\] Na presença de heterocedasticidade nos resíduos, os estimadores de mínimos quadrados continuam sendo não viesados e consistentes, mas perdem eficiência. Equivale a dizer que haverá um outro estimador para os parâmetros do modelo que terá uma variância menor e menos tendencioso: \\[ \\begin{aligned} Var(b^{*}) &lt; Var(b) \\end{aligned} \\] Uma técnica gráfica para se verificar a homocedasticidade dos resíduos é através dos gráficos: resíduos contra valores estimados; ou, resíduos contra a variável preditora Os padrões desejados nos gráficos acima deve assemelhar-se a pontos dispersos de modo aproximadamente simétrico em torno de um eixo horizontal e que não exibam, sistematicamente, nenhum padrão de crescimento ou decaimento na amplitude visual de sua dispersão como nas imagens abaixo: A heterocedasticidade pode ser um subproduto de uma violação significativa das premissas de linearidade e/ou independência, caso em que todas essas violações podem ser conjuntamente corrigidas com a aplicação de uma transformação de potência na variável dependente que terá como objetivos: linearizar o ajuste tanto quanto possível; e/ou, estabilizar a variância dos resíduos. Algum cuidado e discernimento é requerido pois esses dois objetivos podem conflitar entre si. Geralmente opta-se em estabilizar a variância dos resíduos primeiramente para, só então analisar linearização das relações. As transformações sugeridas pela família Box-Cox (1964) em função dos valor que maximizam a verissimilhança perfilada são: se \\(\\lambda\\)=-2 \\(\\rightarrow\\) \\(\\frac{1}{Y^{2}}\\) se \\(\\lambda\\)=-1 \\(\\rightarrow\\) \\(\\frac{1}{Y}\\) se \\(\\lambda\\)=-0,5 \\(\\rightarrow\\) \\(\\frac{1}{\\sqrt{Y}}\\) se \\(\\lambda\\)=0 \\(\\rightarrow\\) log(Y) se \\(\\lambda\\)=0,50 \\(\\rightarrow\\) \\(\\sqrt{Y}\\) se \\(\\lambda\\)=1 \\(\\rightarrow\\) Y se \\(\\lambda\\)=2 \\(\\rightarrow\\) \\(Y^{2}\\) Gráficos dos valores absolutos dos resíduos (ou do quadrado dos resíduos pois os sinais dos resíduos não são significativos para o propósito desse exame) contra a variável preditora \\(X\\) ou em relação aos valores ajustados também são úteis para o diagnóstico da heterocedasticidade da variância dos resíduos. Esses gráficos são recomendados quando não há muitas observações no conjunto de dados pois a plotagem dos resíduos absolutos ou seus quadrados coloca as informações sobre a alteração das suas magnitudes acima da linha horizontal do zero o que facilita a inspeção visual de possíveis alterações de sua magnitude em relação a outra variável adotada no gráfico. 12.7.4.1 Testes para verificação da a homogeneidade da variância: teste de Park: teste de Bartlett: teste de Levene: teste de Brown-Forsythe: teste de Breuch-Pagan: 12.7.5 Inconsistência de observações (outliers) Outliers são observações extremas afastadas das demais observações que formam a amostra e sua identificação deve ser feita já na análise descritiva que antecede todo estudo estatístico. Essas observações podem ser resultado dos mais variados erros de medição (observadores diferentes, equipamentos descalibrados, instrumentos de medição diversos) quando então, nessa hipótese e confirmado o erro de registro, devem ser descartados com discernimento. Todavia na maior parte dos experimentos a identificação desse tipo de erro na etapa descritiva não é possível e, nessas situações, a análise dos residuos gerados pelo modelo na estimação de cada observação é a principal ferramenta. A principal razão para sua identificação é que esses pontos extremos podem ter grande repercussão e exercer grande influência nas estimativas do modelo. Uma observação é influente se uma uma pequena modificação em seu valor ou sua exclusão do modelo produz alterações significativas nas estimativas dos parâmetros. Uma técnica gráfica para se verificar a presença observações outliers é através dos gráficos: resíduos contra valores estimados; e/ou, resíduos contra a variável preditora A plotagem de resíduos estudentizados é particularmente útil para distinguir as observações cujos resíduos distem muitos desvios padrão da média zero. Os padrões desejados nos gráficos acima deve assemelhar-se a pontos dispersos de modo aproximadamente simétrico em torno do eixo horizontal zero, que não exibam, sistematicamente, nenhum padrão de crescimento ou decaimento na amplitude visual de sua dispersão. Uma regra comum para amostras grandes (n&gt;30) é considerar resíduos estudentizados com afstamentos em valor absoluto de quatro ou mais desvios padrão serem outliers. 12.7.6 Pontos influentes com capacidade de alavanca (leverage): Os elementos \\(h_{ii}\\) da diagonal da matriz de projeção (H) tem importante papel no diagnóstico de pontos influentes. Há diferentes opiniões sobre os valores críticos para essa medida: \\(h_{ii}&gt;2\\frac{p}{n}\\) (Hoaglin, D. C. and Welsch, R. E, 1978. The hat matrix in regression and ANOVA) \\(h_{ii}&gt; 3\\frac{p}{n}\\) onde p é o número de parâmetros estimados no modelo (\\(\\hat{\\beta_{0}}\\) e \\(\\hat{\\beta_{1}}\\): 2 para uma regressão linear simples). David Sam Jayakumar e A. Sulthan (Exact distribution of Hat Values and Identification of Leverage Points, 2014) propuseram a distribuição teóricas exata para os valores da diagonal da matriz de projeção link de acesso ao recurso. 12.7.6.1 DFBeta: A estatística \\(DFBeta\\) indica o quanto cada coeficiente de regressão \\(\\hat{\\beta_{j}}\\) se altera em unidades de desvio padrão quando a i-ésima observação for removida: \\[ DFBeta_{(j,i)}=\\frac{\\hat{\\beta_{j}}-\\hat{\\beta_{j(i)}}}{ \\sqrt{S_{i}^{2}C_{(jj)}}} \\] onde \\(C_{(jj)}\\) é o j-ésimo elemento da diagonal da matriz \\((X^{t}X)^{-1}\\) e: \\[ S_{i}^{2}=\\frac{(n-p-1)QMRes - \\hat{\\varepsilon_{i}} (1-h_{ii}) }{(n-p)} \\] Valores superiores a \\(|DFBeta_{(ji)}|&gt; \\frac{2}{\\sqrt{n}}\\) requerem exame mais detalhado. 12.7.6.2 DFFits: A estatística \\(DFFits\\) indica a influência da i-ésima observação medindo o quanto os valores preditos se modificam, em unidades de desvio padrão, se aquela observação for removida: \\[ DFFits= \\frac{\\hat{Y}-\\hat{Y_{i}} }{\\sqrt{S_{i}^{2} h_{ii}}} \\] Valores superiores a \\(|DFFits|&gt; 2\\sqrt{\\frac{p}{n}}\\) requerem exame mais detalhado. 12.7.6.3 Distância de Cook: A estatítica proposta por Denis R. Cook mede a influência de um determinado dado da amostra no que tange a quanto ele está afetando a linha de regressão, sendo medida pelo quanto a linha de regressão se alteraria caso esse dado fosse removido da da análise: ele exerce um destacado impacto da estimativa dos parâmetros do modelo. A influência na locação (afastamento de alguma observação da vizinhança do resto dos dados) pode ser investigada pelo gráfico feito das distâncias de Cook contra os valores ajustados. Há vários critérios para se definir um valor limite para a estatística de Cook: \\(D_{i}&gt;1\\): Cook e Weisberg, 1982 e Chatterjee, Hadi e Price, 2000; duas vezes a média das distâncias de Cook; \\(\\frac{4}{n}&lt;D_{i}&lt;1\\): Bollen et al, 1990; e, o valor crítico do quantil da distribuição F para uma significância igual a 0.5 com df1=p e f2=n-p.  12.7.7 Independência Quando as observações da amostra são independentes o que se espera é que seus resíduos apresentem-se aleatoriamente dispersos em torno da linha horizontal (zero) quando dispostos na sequência em que foram coletadas. O que se pretende aqui é verificar se há correlação serial entre as observações. A autocorrelação pode ser definida como a correlação entre integrantes de séries de observações ordenadas no tempo (como as séries temporais) ou no espaço (como nos dados de corte transversal) quando então os resíduos de duas observações guardam correlação diferente de zero entre si: \\[ \\begin{aligned} cov(\\hat{\\varepsilon_{i}}, \\hat{\\varepsilon_{j}}|x_{i}, x_{j}) \\neq 0 \\\\ i \\neq j \\end{aligned} \\] A correlação serial pode decorrer: inércia: quando os efeitos na alteração da variável \\(X\\) demoram a se manifestar na variável \\(Y\\) (muito comum em dados econômicos); forma funcional do modelo incorreta; variáveis importantes foram omitidas. A verificação da independência resíduos \\(\\hat{\\varepsilon}\\) pode ser verificada informalmente através de vários modos gráficos dentre os quais destacam-se: resíduos contra o tempo ou ordem no qual as observações foram realizadas; e, observações contra o tempo ou ordem no qual foram realizadas (um gráfico sequencial). O que se espera é que nenhuma relação funcional seja percebida. Há ferramentas estatísticas apropriadas para se analisar dados provenientes de séries. 12.7.8 Normalidade A Normalidade dos resíduos \\(\\hat{\\varepsilon}\\) pode ser verificada informalmente através de vários modos gráficos dentre os quais descam-se: pela comparação de suas frequências às frequências esperadas de uma distribuições Normal: 68%: \\(\\pm 1\\) desvio padrão; 90%: \\(\\pm 1.65\\) desvio padrão; 95%: \\(\\pm 1.96\\) desvio padrão; gráficos de caixas; histogramas; gráficos dos quantis teóricos da distribuição Normal padronizada contra os quantis amostrais dos resíduos (QQ plot); gráfico com envoltória simulada dos resíduos (Brian David Ripley em Modelling Spatial Patterns, 1977). Se os valores de uma amostra provêm de uma distribuição Normal, então os valores das estatísticas de ordem contruídas com os resíduos e os \\(Z_{i}\\) correspondentes obtidos da distribuição Normal padrão são linearmente relacionados e, assim, o gráfico dos valores deve ter o aspecto aproximado de uma reta. Todavia observam-se que alguns aspectos desse gráfico diferentes de uma reta que sugerem ausência de Normalidade têm como provável causa: “S”: indica distribuições com caudas muito curtas, isto é, distribuições cujos valores estão muito próximos da média; “S invertido”: indica distribuições com caudas muito longas e, portanto, presença de muitos valores extremos; e, “J” e “J invertido”: indicam distribuições assimétricas, positivas e negativas, respectivamente. A análise do modelo com respeito à Normalidade de seus resíduos é, em muitos aspectos, mais difícil do que para as outras verificações. A menos que o tamanho da amostra seja muito grande (\\(n \\sim 300\\)) a variação aleatória impõe sérias dificuldades para se estudar a natureza da distribuição de probabilidade da variável em estudo. Outros tipos de desvios podem também afetar a distribuição dos resíduos como quando a função é inadequada ou quando a variância não é constante. Assim, pequenos desvios dos resíduos em relação à distribuição Normal podem ser tolerados pois não causam problemas sérios na estimação do modelo. 12.7.8.1 Testes para Normalidade dos resíduos: Para uma análise formal da Normalidade há vários testes definidos: \\(K^{2}\\) de D’agostino (Ralph D’agostino); Jarque-Bera (Carlos Jarque e Anil K. Bera); Anderson-Darling (Theodore Wilbur Anderson e Donald Alan Darling); Cramer-von Mises (H. Cramer e R.E. von Mises); Lilliefors (Hubert W. Lilliefors); Shapiro-Francia (Samuel Sandford Shapiro e S. Francia); \\(X^{2}\\) de Karl Pearson; Shapiro-Wilk (Samuel Sandford Shapiro e Martin Bradbury Wilk); Kolmogorov-Smirnov (Andrey Kolmogorov e Nikolai Smirnov); e, teste de correlação linear entre os resíduos padronizados ordenados e os quantis teóricos da distribuição Normal padronizada; 12.7.9 Variáveis omitidas do modelo Caso os dados sob análise possuam mais variáveis preditoras é prudente plotar um gráfico dos resíduos contra cada uma delas para que eventuais efeitos na variável resposta sejam descartados. O objetivo desta análise adicional é determinar se há quaisquer outras variáveis que possam contribuir na explicação da variável resposta e assim, o padrão visual dos resíduos não pode diferir do padrão apresentado quando se plotam os resíduos contra a variável incorporada no modelo, não só na aleatoridade de sua dispersão mas também nas frequências ou concentrações mostradas acima ou abaixo da linha base (zero). "],["teste-de-significância-global-do-modelo.html", "12.8 Teste de significância (global) do modelo", " 12.8 Teste de significância (global) do modelo O modelo \\(\\hat{Y} = a + b \\cdot X\\) pode ser decomposto em duas partes: variação explicada: \\(a + b \\cdot X\\) variação residual: \\(\\hat{Y}-Y\\), a diferença entre um valor estimado e o realmente observado. Se a variação explicada for significativamente superior à variação residual, teremos um bom indicativo de existe regressão linear entre as variáveis \\(X\\) e \\(Y\\) e o modelo a está explicando razoavelmente bem. Essa verificação é realizada pela análise de variância. Quadro para a Análise de variância do modelo Fonte Graus Soma Quadrados Fcal Ftab da variação de liberdade de quadrados médios REGRESSÃO k = 1 b ⋅ Sxy \\(QMREG = \\frac{b \\cdot S_{xy}}{1}\\) \\(F_{calc}= \\frac{QMREG}{QMRES}\\) Ftab[1, (n − 2); α] RESÍDUOS n-k-1 = n-2 Syy − b ⋅ Sxy \\(QMRES = \\frac{S_{yy} -b \\cdot S_{xy}}{n-2}\\) - - TOTAL k+(n-k-1) = n-1 Syy - - - Sendo SQTOTAL = SQREG - SQRES, em que: \\[ SQRES = S_{yy} - b\\cdot S_{xy}\\\\ S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i} - \\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n}\\\\ {S}_{yy}=\\sum _{i=1}^{n} y_{i}^{2} - \\frac{(\\sum _{i=1}^{n} y_{i})^{2}}{n} \\] A verificação da existência ou não de regressão linear na população é necessário testar o parâmetro \\(\\beta\\) e, para tanto, propomos as seguintes hipóteses: \\[ \\begin{cases} H_{0}: \\beta = \\beta_{0} \\\\ H_{1}: \\beta \\ne 0 \\end{cases} \\] Usualmente \\(\\beta_{0}=0\\), indicando não haver regressão na população. A estatística calculada (\\({F}_{calc}\\)) será comparada a uma estatística \\(F_{tab}\\) tabelada da Distribuição “F” (Ronald Aylmer Fisher-George Waddel Snedecor). \\(F_{tab}\\) é o quantil de ordem \\(\\alpha\\) da Distribuição “F” (Ronald Aylmer Fisher-George Waddel Snedecor) com graus de liberdade \\(1,(n-2)\\) (numerador e denominador, respectivamente). Rejeita-se a hipótese nula (\\(H_{0}\\)) se: \\[ F_{calc}= \\frac{QMREG}{QMRES} \\ge F_{tab[1,(n-2); \\alpha]} \\] em um teste unilateral à direita: \\((\\alpha)\\in \\text{right tail}\\). Vejam nessa simulação o gráfico da função densidade de probabilidade “F” (Ronald Aylmer Fisher-George Waddel Snedecor) com graus de liberdade no numerador e denominador: \\(1, (n-2)\\) e nível de significância \\((\\alpha)\\in \\text{right tail}\\). SIMULADOR 4 Exemplo 4 Uma indústria farmacêutica vende um remédio para aliviar os sintomas do resfriado. Após dois anos de operação ela coletou as informações trimestrais de vendas desse produto e despesas com sua propaganda. Estime um modelo de regressão linear simples e teste a existência da regressão pela ANOVA a um nível de significância de 5% Quadro de despesas de propaganda (X) e receitas de vendas (Y) Trimestre Despesas (X) Vendas (Y) 1 11 25 2 5 13 3 3 8 4 9 20 5 12 25 6 6 12 7 5 10 8 9 15 Quadro para cálculo das estimativas dos parâmetros do modelo Trimestre Despesas (X) Vendas (Y) X ⋅ Y X2 Y2 1 11 25 275 121 625 2 5 13 65 25 169 3 3 8 24 9 64 4 9 20 180 81 400 5 12 25 300 144 625 6 6 12 72 36 144 7 5 10 50 25 100 8 9 15 135 81 225 Totais 60 128 1101 522 2352 Valor médio 7,50 16,00 - - - Sendo \\(n= 8\\), \\(\\stackrel{-}{y}= 16\\) e \\(\\stackrel{-}{x} = 7,50\\), calculamos: \\[ S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i} - \\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n} = 1101 - \\frac{60 \\cdot 128}{8} = 141 \\\\ S_{xx} = \\sum _{i=1}^{n} x_{i}^{2} - \\frac{(\\sum _{i=1}^{n} x_{i})^{2}}{n} = 522 - \\frac{60^2}{8} = 72\\\\ {S}_{yy} = \\sum _{i=1}^{n}y_{i}^{2} - \\frac{(\\sum _{i=1}^{n} y_{i})^{2}}{n}= 2352 - \\frac{128^2}{8} = 304 \\] As estimativas dos parâmetros do modelo serão: \\[ b = \\frac{S_{xy}}{S_ {xx}} = \\frac{141}{72} = 1,9583\\\\ a = \\stackrel{-}{y} - b\\cdot\\stackrel{-}{x} = 16 - 1,9583 \\cdot 7,50 = 1,3125 \\] O modelo toma a seguinte forma: \\[ \\hat{y} = 1,3125 + 1,9583 \\cdot x \\] Quadro para análise de variância do modelo Fonte da variação Graus de liberdade Soma de quadrados Quadrados médios Fcal Ftab REGRESSÃO k = 1 b.Sxy = 1, 9583.141 = 276, 12 \\(QMREG = \\frac{b.S_{xy}}{1}=276,12\\) \\(F_{calc} = \\frac{QMREG}{QMRES} = 59,50\\) Ftab[1, (n − 2); α] = Ftab[1, 6; 5% = 5, 987 RESÍDUOS n-k-1 = n-2 = 6 Syy − b ⋅ Sxy = 304 − 1, 9583 ⋅ 141 = 27, 87 \\(QMRES = \\frac{S_{yy} -b \\cdot S_{xy}}{n-2} = 4,64\\) — — TOTAL k+(n-k-1) = n-1 = 7 Syy = 304 — — — Conclusão: frente ao resultado da análise dos dados rejeita-se a hipótese sob um nível de significância de 5%. (SIMULADOR 4) "],["teste-de-hipóteses-para-o-coef.-angular-beta.html", "12.9 Teste de hipóteses para o coef. angular \\(\\beta\\)", " 12.9 Teste de hipóteses para o coef. angular \\(\\beta\\) O teste de hipóteses para o coeficiente angular \\(\\beta\\) pode ser proposto da forma que se segue: \\[ \\begin{cases} H_{0}: \\beta = \\beta_{0} \\hspace{0.5cm} \\\\ H_{1}: \\beta \\ne \\beta_{0} \\hspace{0.5cm} \\end{cases} \\] Usualmente fazemos \\(\\beta_{0}=0\\), indicando não haver regressão. Estatística do teste: \\[ t_{calc}=\\frac{b-\\beta_{0}}{s_{b}} \\] Rejeita-se a hipótese nula (\\(H_{0}\\)) se: \\[ {t}_{calc} \\le {t}_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]}\\\\ \\text{ou}\\\\ {t}_{calc} \\ge {t}_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]} \\] em um teste bilateral: \\((\\frac{\\alpha}{2})\\in \\text{left tail}; (\\frac{\\alpha}{2})\\in \\text{right tail}\\). Sendo \\(t_{tab}\\) o quantil associado na distribuição “t” de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido (\\(\\alpha\\)) com \\((n-2)\\) graus de liberdade. O número de graus de liberdade irá determinar qual curva da família dessa distribuição será utilizada, por essa razão, as tabelas apresentam-se na forma de linhas (graus de liberdade) e colunas (nível de significância). Vejam nessa simulaçao o gráfico da função densidade de probabilidade “t” de Student (William Sealy Gosset, 1876-1937) com graus de liberdade: \\((n-2)\\) e nível de significância: \\((\\frac{\\alpha}{2})\\in \\text{left tail}; (\\frac{\\alpha}{2})\\in \\text{right tail}\\). (SIMULADOR 2 COM t) "],["teste-de-hipóteses-para-o-coef.-angular-alpha.html", "12.10 Teste de hipóteses para o coef. angular \\(\\alpha\\)", " 12.10 Teste de hipóteses para o coef. angular \\(\\alpha\\) O teste de hipóteses para o coeficiente linear \\(\\alpha\\) pode ser proposto da forma que se segue: \\[ \\begin{cases} H_{0}: \\alpha = \\alpha_{0} \\\\ H_{1}: \\alpha \\ne \\alpha_{0} \\end{cases} \\] Usualmente \\(\\alpha_{0} =0\\) indicando que a regressão passa pela origem. Estatística do teste: \\[ {t}_{calc}=\\frac{a-{\\alpha }_{0}}{{s}_{a}} \\] Rejeita-se a hipótese nula (\\(H_{0}\\)) se: \\[ t_{calc} \\le {t}_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]} \\\\ \\text{ou}\\\\ t_{calc} \\ge {t}_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]} \\] em um teste bilateral: \\((\\frac{\\alpha}{2})\\in \\text{left tail}; (\\frac{\\alpha}{2})\\in \\text{right tail}\\). Sendo \\(t_{tab}\\) o quantil associado na distribuição “t” de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido (\\(\\alpha\\)) com \\((n-2)\\) graus de liberdade. O número de graus de liberdade irá determinar qual curva da família dessa distribuição será utilizada, por essa razão, as tabelas apresentam-se na forma de linhas (graus de liberdade) e colunas (nível de significância). Vejam nessa simulaçao o gráfico da função densidade de probabilidade “t” de Student (William Sealy Gosset, 1876-1937) com graus de liberdade: \\((n-2)\\) e nível de significância: \\((\\frac{\\alpha}{2})\\in \\text{left tail}; (\\frac{\\alpha}{2})\\in \\text{right tail}\\). (SIMULADOR 2 COM t) "],["coeficiente-de-determinação-r2.html", "12.11 Coeficiente de determinação \\(R^{2}\\)", " 12.11 Coeficiente de determinação \\(R^{2}\\) O coeficiente de determinação amostral (\\(R^{2}\\)) é uma medida estatística que informa o quanto da variação observada na variável \\(Y\\) está sendo explicada no modelo pela relação linear estabelecida com a variável \\(X\\). \\[ R^{2} = \\frac{\\text{variação explicada}}{\\text{variação total}} \\\\ R^{2}=\\frac{b\\cdot Sxy}{{S}_{yy}} \\] Exemplo 5: O faturamento de uma loja durante o período de janeiro a gosto de 2010 é dado pela tabela abaixo (milhares de R$). Construa um modelo, calcule a correlação existente, teste a existência da regressão pela ANOVA, a correlação linear obtida, as estimativas de seus coeficientes \\(a\\) e \\(b\\) de seus coeficientes \\(\\alpha\\) e \\(\\beta\\), a um nível de significância de 5% Quadro do faturamento: meses (X); faturamento (Y) Meses (X) Faturamento (Y) Janeiro 1 20 Fevereiro 2 22 Março 3 23 Abril 4 26 Maio 5 28 Junho 6 29 Julho 7 32 Agosto 8 36 Sendo \\(n= 8\\), \\(\\stackrel{-}{Y}= 27\\) e \\(\\stackrel{-}{x} = 4,5\\), calculamos: \\[ S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i} - \\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n} = 1063 - \\frac{36 \\cdot 216}{8} = 91\\\\ S_{xx} = \\sum _{i=1}^{n} x_{i}^{2} - \\frac{(\\sum _{i=1}^{n} x_{i})^{2}}{n} = 204 - \\frac{36^2}{8} = 42\\\\ {S}_{yy} = \\sum _{i=1}^{n}y_{i}^{2} - \\frac{(\\sum _{i=1}^{n} y_{i})^{2}}{n}= 6034 - \\frac{216^2}{8} = 202 \\] As estimativas dos parâmetros do modelo serão: \\[ b = \\frac{S_{xy}}{S_ {xx}} = \\frac{91}{42} = 2,166\\\\ a = \\stackrel{-}{y} - b\\cdot\\stackrel{-}{x} = 27 - 2,166 \\cdot 4,50 = 17,253\\\\ \\] E o modelo toma a seguinte forma: \\[ \\hat{y} = 17,253 + 2,166 \\cdot x \\] O coeficiente de correlação linear de Pearson é: \\[ r = \\frac{{s}_{xy}}{\\sqrt{{s}_{xx}\\cdot {s}_{yy}}} = \\frac{91}{\\sqrt{42 \\cdot 202}} = 0,9880 \\] Quadro para análise de variância do modelo da variação Graus de liberdade Soma de quadrados Quadrados médios Fcal Ftab REGRESSÃO k = 1 b.Sxy = 2, 166.91 = 197, 106 \\(QMREG = \\frac{b.S_{xy}}{1}=197,106\\) \\(F_{calc}=\\frac{QMREG}{QMRES} = 241,84\\) Ftab[1, (n − 2); α] = Ftab[1, 6; 5%] RESÍDUOS n-k-1 = n-2 = 6 Syy − b.Sxy = 202 − 2, 166.91 = 4, 894 \\(QMRES = \\frac{S_{yy} -b.S{xy}}{n-2} = 0,815\\) – – TOTAL k+(n-k-1) = n-1 = 7 Syy = 202 – – Conclusão: frente ao resultado da análise dos dados rejeitamos a hipótese nula sob um nível de significância de 5%. (SIMULADOR 4) Teste de hipóteses para a correlação linear \\(\\rho\\) : \\[ \\begin{cases} H_{0}: \\rho = \\rho_{0} \\\\ H_{1}: \\rho \\ne 0 \\end{cases} \\] com \\(\\rho_{0} =0\\). Estatística do teste: \\[ t_{calc}=\\frac{r\\cdot\\sqrt{n-2}}{\\sqrt{1-{r}^{2}}} = \\frac{0,9880 \\cdot \\sqrt{6}}{\\sqrt{1-0,9880^2}} = 15,668 \\] Rejeita-se a hipótese nula (\\(H_{0}\\)) se o valor da estatística for tão extremo que se verifique: \\[ t_{calc} \\le {t}_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]}\\\\ \\text{ou}\\\\ t_{calc} \\ge {t}_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]} \\] \\[ t_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]}={t}_{tab[\\frac{0.05}{2};\\left(6\\right)]}=-2,44 \\\\ t_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]}={t}_{tab[1-\\frac{0.05}{2};\\left(6\\right)]}=2,44 \\] Conclusão: frente ao resultado da análise dos dados rejeitamos a hipotese nula sob um nível de significância de 5%. (SIMULADOR 2 COM t) Teste de hipóteses para o coeficiente angular \\(\\beta\\): \\[ \\begin{cases} H_{0}: \\beta = \\beta_{0} \\\\ H_{1}: \\beta \\ne \\beta_{0} \\end{cases} \\] com \\(\\beta_{0} =0\\). Estatística do teste: \\[ t_{calc}=\\frac{b-\\beta_{0}}{s_{b}} \\] com: \\[ s_{b} = \\sqrt{\\frac{{\\hat{\\sigma}}^{2}}{{S}_{xx}}} = \\sqrt{\\frac{\\text{QMRES}}{S_{xx}}} \\] \\[ t_{calc}= 15,5491 \\] Rejeita-se a hipótese nula (\\(H_{0}\\)) se: \\[ t_{calc} \\le {t}_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]} \\\\ \\text{ou}\\\\ t_{calc} \\ge {t}_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]} \\] \\[ t_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]}={t}_{tab[\\frac{0.05}{2};\\left(6\\right)]}=-2,44 \\\\ t_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]}={t}_{tab[1-\\frac{0.05}{2};\\left(6\\right)]}=2,44 \\] Conclusão: frente ao resultado da análise dos dados rejeita-se a hip´tese nula sob um nível de significância de 5%. (SIMULADOR 2 COM t) Teste de hipóteses para o coeficiente linear \\(\\alpha\\): \\[ \\begin{cases} H_{0}: \\alpha = \\alpha_{0} \\\\ H_{1}: \\alpha \\ne \\alpha_{0} \\end{cases} \\] com \\(\\alpha_{0}=0\\). Estatística do teste: \\[ t_{calc}=\\frac{a-{\\alpha }_{0}}{{s}_{a}} \\] com \\[ s_{a} = \\sqrt{\\text{QMRES} \\cdot \\left(\\frac{1}{n}+\\frac{{\\stackrel{-}{x}}^{2}}{{S}_{xx}}\\right)}\\\\ \\] \\[ t_{calc}= 24,5268 \\] Rejeita-se a hipótese nula (\\(H_{0}\\)) se: \\[ t_{calc} \\le {t}_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]} \\\\ \\text{ou}\\\\ t_{calc} \\ge {t}_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]} \\] \\[ t_{tab[\\frac{\\alpha }{2};\\left(n-2\\right)]}={t}_{tab[\\frac{0.05}{2};\\left(6\\right)]}=-2,44 \\\\ t_{tab[1-\\frac{\\alpha }{2};\\left(n-2\\right)]}={t}_{tab[1-\\frac{0.05}{2};\\left(6\\right)]}=2,44 \\] Conclusão: frente ao resultado da análise dos dados rejeita-se a hip´tese nula sob um nível de significância de 5%. (SIMULADOR 2 COM t) O coeficiente de determinação será: \\[ R^{2} = \\frac{\\text{variação explicada}}{\\text{variação total}}\\\\ R^{2}=\\frac{b\\cdot Sxy}{{S}_{yy}} = 0,9758 \\] O coeficiente de determinação amostral (\\(R^{2}\\)) é uma medida estatística que informa, em termos percentuais, o quanto da variação observada na variável \\(Y\\) está sendo explicada no modelo pela relação linear estabelecida com a variável \\(X\\). No exemplo em tela, 97,58%. "],["intervalos-de-confiança-2.html", "12.12 Intervalos de confiança", " 12.12 Intervalos de confiança Um intervalo de confiança (\\(IC\\)) pode ser entendido como uma faixa de valores bastante específica para uma estatística calculada dentro da qual, sob alguma confiança, podemos afirmar se localizar o valor do parâmetro estimado. Essa faixa pode ser fechada ou aberta (delimitada apenas por dois ou apenas um valor, respectivamente): intervalos de confiança bilaterais: intervalos delimitados por dois valores: mínimo e máximo, dentro do qual todos os valores possuem um mesmo nível de confiança de ocorrência; intervalos de confiança unilaterais: intervalos delimitados apenas em um de seus lados, nos quais todos os valores possuem um mesmo nível de confiança (limitados à direita por um valor máximo ou limitados à esquerda por um valor mínimo). A amplitude de um intervalo de confiança é uma função diretamente proporcional a um nível de confiança e à variabilidade da população amostrada (quanto maior a variabilidade e/ou o nível de confiança, maior sua amplitude) e inversamente proporcional ao tamanho amostral (quanto maior o tamanho da amostra, menor sua amplitude. \\[ amplitude=\\text{estimativa amostral} \\pm f(confiança, variabilidade, \\frac{1}{n}) \\] Como raramente se dispõe de informação a respeito da variabilidade da carcaterística estudada na população, esse valor é considerado na expressão acima de modo estimado por uma amostra. Um intervalo de confiança reflete uma estimativa objetiva da (im)precisão acarretada pelo tamanho da amostra e, assim, podemos considerá-lo como uma medida da qualidade da pesquisa. O nível de confiança associado ao intervalo é designado pela quantidade \\((1-\\alpha)\\), sendo \\(\\alpha\\) denominado de nível de significância: uma medida da probabilidade de erro. Dependendo do nível de confiança que escolhemos, os limites do intervalo mudam para uma mesma estimativa amostral. Os níveis de confiança mais utilizados na literatura são os de 90%, 95% e 99%. Assim, \\((1-\\alpha)\\) traduz o grau de confiança que se tem em que uma particular amostra de tamanho \\(n\\) da variável aleatória \\(X\\) dê origem a um intervalo de valores (o intervalo de confiança) que compreenda o verdadeiro valor do parâmetro sobre o qual se estima ou sobre o qual se infere. Vejam a simulação onde contruímos um grande número de intervalos de confiança calculados sob as mesmas condições (mesma população amostrada, mesmo tamanho amostral (n) e nível de significância \\(\\alpha\\)). (SIMULADOR 5) Nela podemos observar que uma determinada proporção desses intervalos (aproximadamente igual ao nível de confiança \\(1-\\alpha\\)), conterá o parâmetro sobre o qual se estima e se deseja inferir. 12.12.1 Intervalos de confiança nos modelos de regressão linear simples Intervalo de confiança para a resposta média do modelo (equivale a dizer a resposta fornecida pelo modelo ajustado para valores observados) Intervalo de predição para novas observações (equivale a dizer a resposta fornecida pelo modelo ajustado para valores não observados) Intervalo de confiança para as estimativas dos parâmetros do modelo (o modelo ajustado apresenta meras estimativas: a e b, dos parâmetros desconhecidos: \\(\\alpha\\) e \\(\\beta\\)). 12.12.1.1 Intervalo de confiança para a resposta média do modelo sob um nível de significância \\(\\alpha\\) \\[ IC=\\hat{y_0} \\pm {t}_{tab\\left[\\frac{\\alpha }{2};\\left(n-2\\right)\\right]}\\cdot \\hat{\\sigma}\\cdot \\sqrt{\\frac{1}{n}+ \\frac{{\\left({x}_{0}-\\stackrel{-}{x}\\right)}^{2}}{S_{xx}}} \\] em que: \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} \\] e \\(\\hat{y}_{0}\\) é o valor médio estimado para um \\(x_{0}\\) pertencente à amostra e \\(t_{tab}\\) é o quantil associado na distribuição “t” de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido com \\((n-2)\\) graus de liberdade. O número de graus de liberdade irá determinar qual curva da família dessa distribuição será utilizada, por essa razão, as tabelas apresentam-se individualizadas por nível de significância e graus de liberdade. (SIMULADOR 2 COM t) 12.12.1.2 Intervalo de predição para novas observações sob um nível de significância \\(\\alpha\\) \\[ IC=\\hat{y_0} \\pm {t}_{tab\\left[\\frac{\\alpha }{2};\\left(n-2\\right)\\right]}\\cdot \\hat{\\sigma }\\cdot \\sqrt{1+\\frac{1}{n}+\\frac{{\\left({x}_{0}-\\stackrel{-}{x}\\right)}^{2}}{S_{xx}}} \\] em que \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} \\] e \\(\\hat{y}_{0}\\) é o valor predito para um \\(x_{0}\\) não pertencente à amostra e \\(t_{tab}\\) é o quantil associado na distribuição “t” de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido com \\((n-2)\\) graus de liberdade. O número de graus de liberdade irá determinar qual curva da família dessa distribuição será utilizada, por essa razão, as tabelas apresentam-se individualizadas por nível de significância e graus de liberdade. (SIMULADOR 2 COM t) 12.12.1.3 Intervalo confiança para a estimativa \\(a\\) do parâmetro \\(\\alpha\\) sob um nível de significância \\(\\alpha\\) \\[ a \\pm {t}_{tab\\left[\\frac{\\alpha }{2};\\left(n-2\\right)\\right]}\\cdot \\hat{\\sigma } \\cdot \\sqrt{ \\left(\\frac{1}{n}+\\frac{\\stackrel{-}{x}^{2}}{Sxx}\\right)} \\] em que \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} \\] e \\(a\\) é a estimativa do parâmetro \\(\\alpha\\) e \\(t_{tab}\\) é o quantil associado na distribuição “t” de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido com \\((n-2)\\) graus de liberdade. O número de graus de liberdade irá determinar qual curva da família dessa distribuição será utilizada, por essa razão, as tabelas apresentam-se individualizadas por nível de significância e graus de liberdade. (SIMULADOR 2 COM t) 12.12.1.4 Intervalo confiança para a estimativa \\(b\\) do parâmetro \\(\\beta\\) sob um nível de significância \\(\\alpha\\) \\[ b \\pm {t}_{tab\\left[\\frac{\\alpha }{2},\\left(n-2\\right)\\right]}\\cdot \\frac{\\hat{\\sigma}}{\\sqrt{ {S_{xx}}}} \\] em que \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} \\] e \\(b\\) é a estimativa do parâmetro \\(\\beta\\) e \\(t_{tab}\\) é o quantil associado na distribuição “t” de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido com \\((n-2)\\) graus de liberdade. O número de graus de liberdade irá determinar qual curva da família dessa distribuição será utilizada, por essa razão, as tabelas apresentam-se individualizadas por nível de significância e graus de liberdade. SIMULADOR 2 Exemplo 6: Um jornal deseja verificar a eficácia de seus anúncios na venda de carros usados e para isso realizou um levantamento de todos os seus anúncios e informações dos resultados obtidos pelas empresas que o contrataram e dele extraiu uma pequena amostra. A tabela abaixo mostra o número de anúncios e o correspondente número de veículos vendidos por 6 companhias que usaram apenas este jornal como veículo de propaganda. Obtenha a equação de regressão linear simples. Qual a estimativa de vendas do modelo para um volume de 36 anúncios? Qual a previsão do número de carros vendidos para um volume de 70 anúncios? Quais os intervalos (estimativa, predição e para os regressores do modelo) sob um nível de significância de 5 Quadro de dados da quantidade de carros vendidos por 6 empresas distintas pela quantidade de anúncios feitos Companhia Anúncios feitos (X) Carros vendidos (Y) A 74 139 B 45 108 C 48 98 D 36 76 E 27 62 F 16 57 Trazendo os resultados já calculados em exemplos anteriores: com \\(n= 6\\), \\(\\stackrel{-}{y}= 90\\) e \\(\\stackrel{-}{x} = 41\\) calcula-se \\[ S_{xy} = \\sum _{i=1}^{n} x_{i}y_{i} - \\frac{\\sum _{i=1}^{n}x_{i}\\cdot\\sum _{i=1}^{n}y_{i}}{n} = 25172 - \\frac{246 \\cdot 540}{6} = 3032 \\\\ S_{xx} = \\sum _{i=1}^{n} x_{i}^{2} -\\frac{(\\sum _{i=1}^{n} x_{i})^{2}}{n} = 12086 - \\frac{246^2}{6} = 2000 \\\\ {S}_{yy} = \\sum _{i=1}^{n}y_{i}^{2} - \\frac{(\\sum _{i=1}^{n} y_{i})^{2}}{n}= 53458 - \\frac{540^2}{6} = 4858 \\] As estimativas dos parâmetros do modelo serão: \\[ b = \\frac{S_{xy}}{S_ {xx}} = \\frac{3032}{2000} = 1,5160 \\\\ a = \\stackrel{-}{y} - b\\cdot\\stackrel{-}{x} = 90 - 1,5160 \\cdot 41 = 27,844 \\] E o modelo toma a seguinte forma: \\[ \\hat{y} = 27,844 + 1,5160 \\cdot x \\] O valor médio estimado para um volume de anúncios de 36 veiculações é de 82 carros vendidos. O intervalo de confiança para a resposta média do modelo: \\(IC[\\mu(x_{0}=36)]\\) sob um nível de significância \\(\\alpha\\) será \\[ \\hat{y_0} \\pm {t}_{tab\\left[\\frac{\\alpha }{2};\\left(n-2\\right)\\right]}\\cdot \\hat{\\sigma}\\cdot \\sqrt{\\frac{1}{n}+ \\frac{{\\left({x}_{0}-\\stackrel{-}{x}\\right)}^{2}}{S_{xx}}} \\] em que \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} = 8,0853 \\] \\(\\hat{y_0}=82\\) é o valor médio estimado para o valor observado \\(x_{0} = 36\\) (um dado pertencente à amostra) e \\(t_{tab}\\) é o quantil associado na distribuição ``t’’ de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido (\\(\\alpha=5\\%\\)) com \\((n-2)=4\\) graus de liberdade (\\(t_{tab} = 2,77\\)). Assim, \\(IC[\\mu(x=36)]_{(\\alpha=5\\%)} = (72,5201 ; 91,4799 )\\) (SIMULADOR 2 COM t) O valor predito para um volume de anúncios de 70 veiculações é de 134 carros vendidos. O intervalo de predição para novas observações \\(IP[Y({x_{0})}]\\) com nível de significância \\(\\alpha\\) será: \\[ \\hat{y_0} \\pm {t}_{tab\\left[\\frac{\\alpha }{2};\\left(n-2\\right)\\right]}\\cdot \\hat{\\sigma }\\cdot \\sqrt{1+\\frac{1}{n}+\\frac{{\\left({x}_{0}-\\stackrel{-}{x}\\right)}^{2}}{S_{xx}}} \\] em que \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} = 8,0853 \\] \\(\\hat{y}_{0}=134\\) é o valor predito para um valor não observado \\(x_{0} = 70\\) e \\(t_{tab}\\) é o quantil associado na distribuição ``t’’ de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido (\\(\\alpha=5\\%\\)) com \\((n-2)=4\\) graus de liberdade (\\(t_{tab} = 2,77\\)). Assim, \\(IP[Y({x_{0})}]_{(\\alpha=5\\%)} = (105,7845 ; 162,2155)\\) "],["simulador-2-com-t.html", "12.13 (SIMULADOR 2 COM t)", " 12.13 (SIMULADOR 2 COM t) Intervalo de confiança para a estimativa \\(a\\) do parâmetro \\(\\alpha\\) do modelo sob um nível de significância \\(\\alpha\\): \\[ a \\pm {t}_{tab\\left[\\frac{\\alpha }{2};\\left(n-2\\right)\\right]}\\cdot \\hat{\\sigma } \\cdot \\sqrt{ \\left(\\frac{1}{n}+\\frac{\\stackrel{-}{x}^{2}}{Sxx}\\right)} \\] em que \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} = 8,0853 \\] \\(a = 27,844\\) é a estimativa do parâmetro \\(\\alpha\\) e \\(t_{tab}\\) é o quantil associado na distribuição ``t’’ de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido (\\(\\alpha=5\\%\\)) com \\((n-2)=4\\) graus de liberdade (\\(t_{tab} = 2,77\\)). Assim, \\(IC(a)_{(\\alpha=5\\%)} = (5,3676 ; 50,3204)\\). (SIMULADOR 2 COM t) Intervalo de confiança para a estimativa \\(b\\) do parâmetro \\(\\beta\\) do modelo sob um nível de significância \\(\\alpha\\): \\[ b \\pm {t}_{tab\\left[\\frac{\\alpha }{2},\\left(n-2\\right)\\right]}\\cdot \\frac{\\hat{\\sigma}}{\\sqrt{ {S_{xx}}}} \\] em que \\[ \\hat{\\sigma}=\\sqrt{QMRES} = \\sqrt{\\frac{SQRES}{(n-2)}} = \\sqrt{\\frac{S_{yy}- b \\cdot S_{xy}}{(n-2)}} = 8,0853 \\] e \\(b=1,5160\\) é a estimativa do parâmetro \\(\\beta\\) e e \\(t_{tab}\\) é o quantil associado na distribuição ``t’’ de Student (William Sealy Gosset, 1876-1937) ao nível de significância pretendido (\\(\\alpha=5\\%\\)) com \\((n-2)=4\\) graus de liberdade (\\(t_{tab} = 2,77\\)). Assim, \\(IC(b)_{(\\alpha=5\\%)} = (1,0152 ; 2,0168 )\\). (SIMULADOR 2 COM t) "],["verificações-gráficas-visuais-das-premissas-do-mmqo.html", "12.14 Verificações gráficas (visuais) das premissas do MMQO", " 12.14 Verificações gráficas (visuais) das premissas do MMQO A análise dos resíduos de um modelo de regressão linear simples é parte fundamental para que se avalie se o modelo produzido representa de forma acurada a realidade estudada. Linearidade no parâmetro: deve-se esperar que a relação entre a variável dependente (Y) e a variável independente (X) possa ser representada por uma função linear}: pela análise dos gráficos dos resíduos padronizados no eixo \\(y\\) pelos valores estimados e pela variável independente no eixo \\(x\\). Em geral, valores próximos à linha horizontal representam observações bem estimadas pelo modelo. Os pontos acima e abaixo são observações superestimadas ou subestimadas pelo modelo. A premissa delinearidade é apoiada pelo padrão de distribuição dos pontos, que deve indicar uma razoável igualdade acima e abaixo da linha. Padronizam-se os resíduos brutos pela Divisão de cada um deles pelo desvio padrão; ou seja: \\(d_{i} = \\frac{e_{i}}{ \\hat{\\sigma}} = \\frac{e_{i}}{ \\sqrt{QMRES}}\\) independência dos resíduos, com valor médio zero e estejam normalmente distribuídos: (\\(\\varepsilon \\sim N(0,\\sigma^{2}\\))}: pela análise do histograma dos resíduos padronizados, com o propósito de se verificar se sua distribuição guarda semelhança com a da curva normal pela comparação das frequências relativas acumuladas dos resíduos padronizados para os intervalos de (-1; +1), (-1,64; +1,64), (-1,96; +1,96) com as probabilidades da distribuição normal nesses mesmos intervalos (68%, 95% e 99%) pela análise do gráfico dos resíduos padronizados ordenados pelos quantis da distribuição normal padronizada, que deve se aproximar da bissetriz do primeiro quadrante a variância residual seja sempre constante (homocedástica) para todas as observações, isto é, \\(VAR(\\varepsilon)=E(\\varepsilon)=\\sigma^2\\) ausência de autocorrelação entre os termos de erros: pela análise do gráfico dos resíduos padronizados pelos valores estimados \\(\\hat{y}\\), que deve apresentar pontos dispostos aleatoriamente sem padrão aparente; mensuração das variáveis: assume-se que as variáveis foram medidas sem erro; correta especificação do modelo: todas as variáveis independentes teoricamente relevantes foram incluídas no modelo e nenhuma irrelevante foi mantida; ausência de multicolinearidade. "],["verificações-adicionais.html", "12.15 Verificações adicionais", " 12.15 Verificações adicionais Análise de pontos com elevada capacidade de alavancar o modelo. A alavancagem mede o quanto uma observação \\(x_{i}\\) contribui para a predição de \\(\\hat{y}_{i}\\) pelo modelo. Um ponto é considerado alavanca (leverage) quando este exerce uma forte influência no seu valor ajustado, sem com isso afetar a estimativa dos parâmetros do modelo. De modo análogo à distância de Cook, há diversos critérios para estabelecer um valor crítico para os hat values: \\(h_{ii}\\): \\(h_{ii}\\) &gt; 2p/n (Hoaglin e Welsch, 1978), \\(h_{ii}\\) &gt; 3p/n. Pontos discrepantes (\\(outliers\\)): A discrepância pode ser medida pela distância residual. Entretanto, os resíduos não são uma medida completa da discrepância. Para tanto basta-se imaginar casos onde onde uma observação possua elevada alavancagem que arraste o modelo inteiro em sua direção, resultando em pequenos resíduos. Uma forma de isolar esses pontos é dividindo seu resíduo por 1-\\(h_{ii}\\), obtendo-se a partir dessa expressão os resíduos \\(studentizados\\). influentes: A estatística distância de Cook mede a influência de um determinado dado da amostra no que tange a quanto ele está afetando a linha de regressão, sendo medida pelo quanto a linha de regressão se alteraria caso esse dado fosse removido da da análise: ele exerce um destacado impacto da estimativa dos parâmetros do modelo. A influência na locação (afastamento de alguma observação da vizinhança do resto dos dados) pode ser investigada pelo gráfico feito das distâncias de Cook contra os valores ajustados. Há vários critérios para se estabelecer um valor limite para a estatística de Cook:\\ \\(D_{i}\\) &gt; 1 (Cook e Weisberg, 1982); duas vezes a média das distâncias de Cook; 4/n &lt; \\(D_{i}\\) &lt; 1 (Bollen et al, 1990); ou, o valor crítico do quantil da distribuição F para uma significância igual a 0.5 com df1=p e f2=n-p. Exemplo 7: Um jornal deseja verificar a eficácia de seus anúncios na venda de carros usados e para isso realizou um levantamento de todos os seus anúncios e informações dos resultados obtidos pelas empresas que o contrataram e dele extraiu uma pequena amostra. A tabela abaixo mostra o número de anúncios e o correspondente número de veículos vendidos por 6 companhias que usaram apenas este jornal como veículo de propaganda. Estime os parâmetros de um modelo de regressão linear simples de \\(X\\) por \\(Y\\) verifique os pressupostos subjacentes ao método utilizado. Faça a análise dos resíduos e identifique possíveis \\(outliers\\) . Quadro de dados da quantidade de carros vendidos por 6 empresas distintas pela quantidade de anúncios feitos Companhia Anúncios feitos (X) Carros vendidos (Y) A 74 139 B 45 108 C 48 98 D 36 76 E 27 62 F 16 57 Trazendo o modelo estimado anteriormente: \\(\\hat{y} = 27,844 + 1,5160 \\cdot x\\) Quadro de valores observados, estimados, resíduos brutos e padronizados Anúncios (X) Carros vendidos (Y) Valores estimados Resíduos brutos Resíduos padronizados 74 139 140,028 -1,028 -0,1271 45 108 96,064 11,936 1,4762 48 98 100,612 -2,612 0,3230 36 76 82,420 -6,420 0,7940 27 62 68,776 -6,776 -0,8380 16 57 52,100 4,900 0,6060 12.15.0.1 Roteiro básico para uma análise de regressão linear simples Definir o problema de pesquisa, selecionar a variável dependente e identificar a variável independente; ou seja, proceder a especificação do modelo. Aqui o pesquisador deve definir qual é a relação esperada entre a variável dependente e a independente; Maximizar o número de observações no sentido de aumentar o poder estatístico, a capacidade de generalização e reduzir toda sorte de problemas associados a estimação de parâmetros populacionais a partir de dados amostrais com \\(n\\) reduzido; Estimar um modelo; Verificar em que medida os dados disponíveis satisfazem os pressupostos da análise de regressão de mínimos quadrados ordinários. Como procedimento padrão, o pesquisador deve reportar as técnicas utilizadas para corrigir eventuais violações (transformações, re-codificações, aumento de \\(n\\), etc.); Interpretar os resultados, caso o modelo seja validado. 12.15.0.2 Homocedasticidade: transformações para estabilização da variância Quando se observa que a distribuição gráfica dos resíduos não se mostra homocedástica, muitas vezes é útil aplicar uma transformação de Box-Cox para estabilizarmos a variância (torná-la constante independentemente do valor do resíduo). Considerando \\(X_{1}, ..., X_{n}\\) os dados originais, a transformação de Box-Cox consiste em encontrar um \\(\\lambda\\) tal que os dados transformados \\(Y_{1}, ..., Y_{n}\\) se aproximem de uma distribuição normal. O modelo passa a assumir a forma: \\(Y^{\\lambda} = X \\cdot \\beta + \\varepsilon\\) com \\(Y_{\\lambda}\\) sendo: \\[ \\frac{Y^{\\lambda} - 1}{\\lambda} \\text{ se } \\lambda \\ne 0 \\\\ ln (Y_{i}) \\text{ se } \\lambda = 0 \\] 12.15.0.3 Transformações para linearização das relações Algumas vezes as relações observadas entre a variável dependente e a independente não se mostram diretamente lineares. Relações não-lineares podem ser linearizadas pela aplicação de transformações aos dados: Função hiperbólica: \\(Y = \\frac{X}{a \\cdot X - b}\\), pela forma transformada: \\(\\frac{1}{Y} = a - \\frac{b}{X}\\) Função exponencial: \\(Y = a \\cdot e^{b \\cdot X}\\), pela forma transformada: $Ln (Y) = Ln (a) + b X $ Função potência: \\(Y = a \\cdot X ^{b}\\), pela forma transformada: \\(Ln (Y) = Ln (a) + b \\cdot Ln (X)\\) 12.15.0.4 Tabelas 12.15.0.5 Resolução do sistema de equações matriciais Seja a matriz \\(Y\\) das observações realizadas na variável dependente \\(Y_{i}\\) (dimensão \\(n \\times 1\\)): \\[ Y = \\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{pmatrix} \\] Seja a matriz \\(X\\) das observações realizadas na variável independente \\(X_{i}\\) (dimensão \\(n \\times 2\\)): \\[ X = \\begin{pmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} \\end{pmatrix} \\] Seja a matriz \\(\\beta\\) dos parâmetros a serem estimados (dimensão: \\(2 \\times 1\\)): \\[ \\beta = \\begin{pmatrix} \\hat{\\beta}_{0} \\\\ \\hat{\\beta}_{1} \\end{pmatrix} \\] Seja a matriz \\(e\\) dos termos aleatórios (dimensão: \\(n \\times 1\\)), não correlacionados, com média zero e variância constante: \\[ e = \\begin{pmatrix} e_{1} \\\\ e_{2} \\\\ \\vdots \\\\ e_{n} \\end{pmatrix} \\] Então podemos escrever o seguinte sistema matricial: \\[ Y = X \\cdot \\hat{\\beta} + e \\] A minimização da soma dos quadrados dos resíduos pode ser realizada fazendo-se: \\[ \\Sigma (e_{i})^{2} = e^T \\cdot e \\] O sistema acima tomará a forma: \\[ e = Y - X \\cdot \\hat{\\beta} \\] \\[ e^T \\cdot e = (Y - X \\cdot \\hat{\\beta})^T \\cdot (Y - X \\cdot \\hat{\\beta}) \\] Expandindo: \\[ Y^T \\cdot Y - 2 \\cdot \\hat{\\beta}^T \\cdot X^T \\cdot Y + \\hat{\\beta}^T \\cdot X^T \\cdot X \\cdot \\hat{\\beta} \\] Minimizando os resíduos, obtemos a equação normal: \\[ (X^T \\cdot X) \\cdot \\hat{\\beta} = X^T \\cdot Y \\] Multiplicando ambos os lados por \\((X^T \\cdot X)^{-1}\\): \\[ (X^T \\cdot X)^{-1} \\cdot (X^T \\cdot X) \\cdot \\hat{\\beta} = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y \\] Por fim, considerando-se que \\((X^T \\cdot X)^{-1} \\cdot (X^T \\cdot X) = I\\), obtemos a solução para \\(\\hat{\\beta}\\): \\[ \\hat{\\beta} = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y \\] library(reticulate) #use_condaenv(&quot;r-python&quot;) #use_python(&#39;/opt/homebrew/Caskroom/miniconda/base/envs/r-python/python3.9&#39;) library(reticulate) Sys.setenv(RETICULATE_CONDA = &quot;/opt/homebrew/Caskroom/miniconda/base/bin/conda&quot;) use_condaenv(&quot;r-python&quot;) "],["introdução-à-modelagem-de-processos-estocásticos.html", "Capítulo 13 Introdução à modelagem de processos estocásticos", " Capítulo 13 Introdução à modelagem de processos estocásticos A expressão processo estocástico pode ser interpretada de duas formas diferentes, todavia interligadas, dependendo do contexto. Como fenômeno real que ocorre no mundo físico ou social como o tempo de chegada de clientes em um banco, as variações no preço de uma ação ou a propagação de uma epidemia. Esses fenômenos apresentam variabilidade inerente e são influenciados por fatores que não podem ser totalmente controlados ou previstos. Como um modelo (abstração do fenômeno): usado para descrever, aproximar e analisar esse fenômeno aleatório. Um processo de Poisson pode ser utilizado para modelar a chegada de clientes em um restaurante. Nesse caso, ele é definido como uma família de variáveis aleatórias indexadas por um conjunto de parâmetros (tempo, no exemplo). "],["modelos-determinísticos-e-estocásticos.html", "13.1 Modelos determinísticos e estocásticos", " 13.1 Modelos determinísticos e estocásticos Os modelos estocásticos contrastam com os modelos determinísticos. Enquanto os modelos determinísticos são definidos por equações que descrevem exatamente como o sistema evolui ao longo do tempo, os modelos estocásticos envolvem pelo menos algum grau de aleatoriedade. Assim, diferentes execuções de um processo estocástico produzem resultados variados, conhecidos como realizações do processo. Modelos determinísticos são geralmente mais fáceis de analisar, mas os estocásticos costumam ser mais realistas. Por exemplo, ao modelar a sobrevivência de uma espécie rara, um modelo determinístico prevê extinção ou sobrevivência com certeza. Já um modelo estocástico atribui uma probabilidade de extinção, permitindo estudar como diferentes estratégias de manejo influenciam esse resultado. "],["dedução-e-indução.html", "13.2 Dedução e indução", " 13.2 Dedução e indução Uma dicotomia relativamente recente (Hammersly e Handscom em Monte Carlo Methods, 1964) contrasta o matemático teórico com o matemático experimental (semelhantes às usadas comumente para físicos teóricos e experimentais) Elas são independentes do fato de os objetivos serem puros ou aplicados. Não pressupõem que o teórico esteja sentado em uma sala vazia diante de uma folha de papel em branco, enquanto o experimentalista manipule equipamentos caros em um laboratório. Embora certos experimentos matemáticos complexos exijam computadores eletrônicos, outros requerem apenas papel e lápis. A diferença essencial é que os teóricos deduzem conclusões a partir de postulados, enquanto os experimentalistas inferem conclusões com base em observações. Trata-se da diferença entre dedução e indução. "],["processos-estocásticos-temporais-espaciais-e-espaçotemporais.html", "13.3 Processos estocásticos temporais, espaciais e espaçotemporais", " 13.3 Processos estocásticos temporais, espaciais e espaçotemporais Como um modelo, um processo estocástico é uma família de variáveis aleatórias \\(\\{X_\\theta\\}\\), indexada por um parâmetro \\(\\theta\\), onde \\(\\theta\\) pertence a algum conjunto de índices \\(\\Theta\\) e a natureza do processo estocástico pode ser temporal, espacial ou espaçotemporal. 13.3.1 Processos Estocásticos Temporais Em um processo estocástico temporal o parâmetro de índice (\\(\\Theta\\)) representa o tempo: \\(\\{X_t : t \\in T \\}\\). Se \\(\\Theta\\) for um conjunto de números inteiros, representando pontos específicos no tempo como no \\(1^{o}s\\) ou no \\(3^{o}s\\), teremos um processo estocástico em tempo discreto e a notação mais utilizada é: \\[ \\{X_t : t \\in \\mathbb{Z}_{+}\\} \\] Se \\(\\Theta\\) for a reta real (ou algum intervalo da reta real) como um intervalo contínuo de tempo como em \\(0.1s\\) ou em \\(3s\\), teremos um processo estocástico em tempo contínuo e a notação mais utilizada é: \\[ \\{X_t : t \\in [0, \\infty] \\} \\] 13.3.2 Processos Estocásticos Espaciais Em um processo espacial, o conjunto de índices \\(\\Theta\\) não representa mais o tempo, mas sim localizações no espaço e passa a ser representado por um vetor, descrevendo posições em um espaço de uma ou mais dimensões. Esse tipo de processo é frequentemente representado por: \\[ \\{X_{\\mathbf{s}} : \\mathbf{s} \\in D \\} \\] em que: \\(X_{s}\\): Variável aleatória associada ao ponto espacial representado por \\(s\\). \\(s\\): Vetor de coordenadas espaciais, por exemplo, \\((u, v)\\) no caso de um plano ou \\((u, v, z)\\) no espaço. \\(D\\): Domínio espacial, que pode ser um subconjunto de \\(\\mathbb{R}^2\\) (no plano) ou \\(\\mathbb{R}^3\\) (no espaço tridimensional). Se o domínio espacial \\(D\\) for discreto, representando pontos específicos no espaço (ex.: coordenadas de uma grade regular em um mapa), teremos: \\[ \\{X_{\\mathbf{s}} : \\mathbf{s} \\in \\mathbb{Z}^2 \\} \\] Se o domínio espacial \\(D\\) for contínuo, representando qualquer ponto em uma região do espaço (ex.: temperatura em qualquer ponto de uma superfície contínua), teremos: \\[ \\{X_{\\mathbf{s}} : \\mathbf{s} \\in \\mathbb{R}^2 \\} \\] 13.3.3 Processos Estocásticos Espaçotemporais Em alguns casos, podemos ter processos que evoluem no tempo e no espaço simultaneamente. Estes são chamados de processos espaço-temporais, frequentemente representados por:: \\[ \\{X_{t, \\mathbf{s}} : t \\in T, \\mathbf{s} \\in D \\} \\] em que: \\(X_{t, \\mathbf{s}}\\): Variável aleatória associada ao instante \\(t\\) na posição espacial \\(\\mathbf{s}\\). \\(t\\): Índice temporal. \\(\\mathbf{s}\\): Índice espacial (vetor de coordenadas no espaço). \\(T\\): Intervalo de tempo de interesse. \\(D\\): Região espacial de interesse. O domínio espacial (\\(D\\)) pode ser discreto (ex.: pontos específicos em uma grade regular, \\(\\mathbb{Z}^2\\)) ou contínuo (ex.: qualquer ponto em uma superfície, \\(\\mathbb{R}^2\\)). Da mesma forma, o domínio temporal (\\(T\\)) pode ser discreto (ex.: momentos específicos, \\(\\mathbb{Z}\\)) ou contínuo (ex.: qualquer instante ao longo de um intervalo, \\(\\mathbb{R}\\)). "],["processo-de-poisson.html", "13.4 Processo de Poisson", " 13.4 Processo de Poisson 13.4.1 Natureza Alguns experimentos aleatórios envolvem, essencialmente, contagens observadas em um certo intervalo de tempo como, por exemplo: 1- clientes que entram num supermercado por dia, 2- chamadas que uma central telefônica recebe por hora, 3. pacotes que passam por um roteador por minuto, 4. chamadas ao SIATE por semana, 5. trens que chegam a uma estação a cada 12 horas. Ao se aumentar o intervalo, aumenta-se a probabilidade de se observar uma certa contagem. Definição 1: um processo de contagem é um processo estocástico temporal representado por: \\[ \\{N_t : t \\in [0, \\infty) \\} \\] em que: \\(N_t\\) representa o número de eventos ocorridos no intervalo [0, t]. Esse processo deve satisfazer as seguintes propriedades: \\(N_0 = 0\\) (no exato momento do início do processo a contagem é zero) \\(N_t \\in \\{0, 1, 2, \\ldots \\}\\) (\\(ie\\), são contagens) Se \\(s &lt; t\\), então \\(N_s \\leq N_t\\) (a contagem no intervalo [0,s] não é superior à contagem no intervalo [0,t], para \\(s &lt; t\\)) Para \\(s &lt; t\\), a diferença \\(N_t - N_s\\) representa o número de eventos ocorridos no intervalo \\((s, t]\\) \\(N_t\\) é uma função não descrescente Definição 2: um processo de contagem tem incrementos independentes se o número de eventos que ocorrem em intervalos de tempo disjuntos são independentes. Para \\(0 \\leq t_1 &lt; t_2 &lt; \\ldots &lt; t_n\\), temos que as variáveis aleatórias: \\[ N(t_2) - N(t_1), N(t_3) - N(t_2), \\ldots, N(t_n) - N(t_{n-1}) \\] são variáveis independentes. Definição 3: um processo de contagem tem incrementos estacionários se a distribuição de probabilidade do número de eventos que ocorrem em qualquer intervalo é a mesma e depende apenas da duração do intervalo (a probabilidade de chegar um cliente entre 9 e 10 h é a mesma de chegar um cliente entre 15 e 16 h: \\(P(N([9,10])=P(N([15,16])\\)): falta de memória Figure 13.1: O processo de Poisson: (1) tem contagem zero em t=0; (2) se s&lt;t então N(s)&lt;N(t); (4) N(t) é não descrescente e (4) os incrementos são independentes Um processo de contagem \\(\\{N_t : t \\geq 0 \\}\\) é dito ser um processo de Poisson com taxa \\(\\lambda &gt; 0\\) se: \\(N(0) = 0\\). o processo tem incrementos independentes e estacionários. o número de eventos em qualquer intervalo de comprimento \\(t\\) tem distribuição de Poisson com média \\(\\lambda t\\). Ou seja, para \\(s, t \\geq 0\\) \\[ P(N(t + s) - N(s) = n) = \\frac{e^{-\\lambda t} (\\lambda t)^n}{n!} \\] em que \\(P(N(t + s) - N(s) = n)\\) é a probabilidade de serem observados \\(n\\) eventos no intervalo de tempo \\((t+s)-t=t\\). Desse modo, para um intervalo de tempo \\(t\\), a probabilidade de serem observados \\(n\\) eventos é \\[ P(N(t) = n) = \\frac{e^{-\\lambda t} (\\lambda t)^n}{n!} \\] Exemplo 1: Fregueses chegam a uma certa loja de acordo com um processo de Poisson com taxa \\(\\lambda = 4\\) fregueses por hora. Admita que a loja abra às 9h e que que os fregueses não deixam a loja. Quais são as probabilidades de que: 1. um freguês chegue até às 9:30h 2. um total de 5 fregueses estejam na loja até às 11:30h? A loja abre às 9 h então até as 9 h 30 min o primeiro intervalo de tempo será \\(t_1 = 0.5\\) horas e até as 11 h 30 min o segundo intervalode tempo será \\(t_2 = 2.5\\). A taxa média de chegadas por hora é \\(\\lambda = 4\\). O que se pede é a \\(P[N(t_1)=1,N(t_2)=5]\\). Essa probabilidade é a mesma que \\(P[N(t_1)=1 \\cap N(t_2-t_1)=4]\\) e, sendo os intervalos de tempo disjuntos, as probabilidades \\(P[N(t_1)=1]\\) e \\(P[N(t_2 - t_1)=4]\\) são independentes e podemos escrever: \\[\\begin{align} P[N(t_1)=1 \\cap N(t_2-t_1)=4] &amp; = P[N(t_1)=1] \\times P[N(t_2-t_1)=4] \\\\ &amp; = \\frac{\\varepsilon^{-\\lambda t_1} (\\lambda t_1)^{n_1}}{n_1!} \\times \\frac{\\varepsilon^{-\\lambda (t_2-t_1)}[\\lambda (t_2-t_1)]^{n_2}}{n_2!}\\\\ &amp; = \\frac{\\varepsilon^{-(4\\times 0.5)} (4 \\times 0.5)^{1}}{1!} \\times \\frac{\\varepsilon^{-(4 \\times 2)} (4 \\times 2)^{4}}{4!}\\\\ &amp; = 2\\varepsilon^{-2} \\times 170.67\\varepsilon^{-8}\\\\ &amp; = 0.2707 \\times 0.0573 \\approx 0.0155 \\end{align}\\] Um valor bastante baixo posto a taxa média \\(\\lambda\\) de 4 clientes por hora indicar que se esperam 2 em meia hora (apenas 1 chegou) e 8 em duas horas (apenas 4 chegaram). Ambos os eventos: chegar 1 pessoa em meia hora e 4 pessoas em duas horas são raros. Exemplo 2: Suponha que pacotes SMTP chegam a um servidor de e-mails de acordo com um processo de Poisson com frequência \\(\\lambda = 2\\) pacotes por segundo. Seja \\(N(t)\\) o número de mensagens que chegam até o tempo \\(t\\). Quais são as probabilidades de que: 1. \\(P(N(1) = 2)\\): 2 pacotes em um intervalo de 1 segundo 2. \\(P(N(1) = 2 \\cap N(3) = 6)\\): 2 pacotes em um intervalo de 1 segundo e 6 pacotes em um intervalo de 3 segundos 3. \\(P(N(1) = 2 | N(3) = 6)\\) 4. \\(P(N(3) = 6 | N(1) = 2)\\) A primeira probabilidade é imediata: \\[\\begin{align} P(N(1) = 2) &amp; = \\frac{e^{-2 \\times 1} (2 \\times 1)^2}{2!}\\\\ &amp; = \\frac{e^{-2} \\cdot 4}{2} \\\\ &amp; = 2 e^{-2} \\approx 0.27 \\end{align}\\] A segunda,recorrendo à mesma definição do exemplo anterior, será dada por: \\[\\begin{align} P[N(1)=2 \\cap N(3)=6] &amp; = P[N(1)=2] \\times P[N(2)=4] \\\\ &amp; = \\frac{\\varepsilon^{-(2\\times 1)} (2 \\times 1)^{2}}{2!} \\times \\frac{\\varepsilon^{-(2 \\times 2)} (2 \\times 2 )^{4}}{4!}\\\\ &amp; = 0.2707 \\times 0.1952 \\approx 0.052 \\end{align}\\] A terceira, recorrendo à probabilidade de dois eventos condicionado \\(P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\), será dada por: \\[\\begin{align} P(N(1) = 2 | N(3) = 6) &amp; = \\frac{P(N(1) = 2 \\cap N(3) = 6)}{P(N(3) = 6)}\\\\ &amp; = \\frac{0.052}{P(N(3) = 6)}\\\\ &amp; = \\frac{0.052}{0.1606} \\approx 0.32 \\\\ \\end{align}\\] A quarta podemos entender que a probabilidade de N(3)=6 dado N(1) = 2, ilustrada a seguir: Portanto, calcular \\(P(N(3) = 6 | N(1) = 2)\\) equivale a calcular a probabilidade de \\(P(N(2)=4)\\) (os incrementos são independentes em um processo de Poisson), dada por \\(\\frac{\\varepsilon^{-(2\\times 2)} (2 \\times 2)^{4}}{4!} \\approx 0.19\\) . 13.4.2 Processo de Poisson com classificação de eventos Seja \\(\\{N(t), t \\geq 0\\}\\) um processo de Poisson com taxa \\(\\lambda\\), que descreve o número de eventos ocorridos em um intervalo de tempo \\([0, t]\\). Admita que cada evento nesse processo possa ser classificado em dois tipos distintos (Tipo I e Tipo II): Cada evento tem probabilidade \\(p\\) de ser classificado como Tipo I. Cada evento tem probabilidade \\(1-p\\) de ser classificado como Tipo II. Se a classificação de cada evento é independente das demais classificações e da ocorrência dos eventos, demonstra-se (Sheldon Ross, in Introduction to Probability Models, \\(6^{a}\\) ed., Cap. 5) que \\(\\{N_1(t), t \\geq 0\\}\\): Número de eventos do Tipo I no intervalo \\([0, t]\\). \\(\\{N_2(t), t \\geq 0\\}\\): Número de eventos do Tipo II no intervalo \\([0, t]\\), e \\(N(t) = N_1(t) + N_2(t)\\). Os processos \\(\\{N_1(t), t \\geq 0\\}\\) e \\(\\{N_2(t), t \\geq 0\\}\\) têm as seguintes propriedades: 1. Ambos são processos de Poisson com taxas: - \\(\\lambda_1 = \\lambda p\\) para eventos do Tipo I. - \\(\\lambda_2 = \\lambda (1-p)\\) para eventos do Tipo II. 2. Os dois processos são independentes. \\[ N_1(t) \\sim \\text{Poisson}(\\lambda p), \\quad N_2(t) \\sim \\text{Poisson}(\\lambda (1-p)) \\] De modo geral, considere um processo de Poisson com taxa \\(\\lambda\\). Cada evento pode ser classificado em n tipos diferentes com probabilidades: \\[ p_1, p_2, \\ldots, p_n, \\quad p_1 + p_2 + \\cdots + p_n = 1 \\] Seja \\(N_i(t)\\) o número de eventos do tipo \\(i\\) até o tempo \\(t\\).Cada subprocesso segue uma distribuição de Poisson com taxa: \\[ N_i(t) \\sim \\text{Poisson}(p_i \\lambda t) \\] Os subprocessos \\(N_1(t), N_2(t), \\ldots, N_n(t)\\) são independentes entre si. A ocorrência de um evento de um tipo não afeta a probabilidade de ocorrência de eventos de outro tipo. Cada tipo de evento mantém suas próprias propriedades estatísticas e comportamentos, preservando a estrutura probabilística do processo original. A proposição afirma que podemos decompor um processo de Poisson subprocessos independentes, onde cada subprocesso segue uma distribuição de Poisson ajustada pela probabilidade de classificação de cada evento. Esse resultado é frequentemente utilizado em aplicações práticas, como: filas com diferentes tipos de clientes e sistemas de telecomunicações com pacotes de dados de diferentes tipos. Exemplo 3: Clientes entram em uma loja de acordo com um processo de Poisson com taxa λ = 10 por hora. De forma independente, cada cliente compra alguma coisa com probabilidade 0.3 ou sai da loja sem comprar nada com probabilidade 0.7. Calcule a probabilidade de que durante a primeira hora 9 pessoas entrem na loja e, dentre essas 9 pessoas, 3 comprem alguma coisa e 6 não. Considerando que \\(N_0(t)\\) e \\(N_1(t)\\) são processos de Poisson independentes com taxas \\((1 − p)\\lambda\\) e \\(p \\lambda\\), em que \\(N_0(t)\\) o número de clientes que não compram nada até o tempo \\(t\\) e \\(N_1(t)\\) o número de clientes que compram até o tempo \\(t\\), a probabilidade pedida é \\[ P(N_0(t=1)=6, N_1(t=1)=3)\\] em que \\(N_0(t=1) \\sim Poisson(\\lambda_0=7)\\) e \\(N_1(t=1) \\sim Poisson(\\lambda_1=3)\\). \\[\\begin{align} P(N_0(t=1)=6 \\land N_1(t=1)=3) &amp; = P(N_0(t=1)=6) \\times P(N_1(t=1)=3)\\\\ &amp; = \\frac{e^{-\\lambda_0 t} (\\lambda_0 t)^n_0}{n_0!} \\times \\frac{e^{-\\lambda_1 t} (\\lambda_1 t)^n_1}{n_1!} \\\\ &amp; = \\frac{e^{-7 \\times 1}\\dot(7 \\times 1)^6}{6!} \\times \\frac{e^{-3 \\times 1}\\dot(3 \\times 1)^3}{3!} \\\\ &amp; \\approx 0.1251 \\times 0.2666 \\\\ &amp; \\approx 0.0333 \\end{align}\\] 13.4.3 Processos de Poisson não homogêneos 13.4.4 Tempo de espera em um processo de Poisson Seja \\(\\{N_t : t \\geq 0 \\}\\) um processo de Poisson com taxa \\(\\lambda\\): denota-se por \\(T_{n}\\) o tempo entre a \\((n − 1)\\) e a \\(n-ésima\\) ocorrência de eventos, sendo \\(T_{1}\\) o tempo até a primeira ocorrência a sequência \\(T_{n}, n=1,2,...\\) é a chamada sequência de tempos entre ocorrências (ou entre chagadas) Proposição: \\(T_{1},T_{2},...\\) são variáveis aleatórias com distribuição exponencial de parâmetro \\(\\frac{1}{\\lambda}\\). Figure 13.2: Os tempos de espera entre cada observação não são constantes. import numpy as np import matplotlib matplotlib.use(&#39;Agg&#39;) import matplotlib.pyplot as plt from scipy.stats import expon #A função **`generate_poisson_events`** simula um **Processo de Poisson**, gerando um número aleatório de eventos com taxa média `rate` em um intervalo de duração `time_duration`, retornando o número total de eventos, os tempos ordenados de ocorrência e os intervalos entre eventos consecutivos. def generate_poisson_events(rate, time_duration): num_events = np.random.poisson(rate * time_duration) event_times = np.sort(np.random.uniform(0, time_duration, num_events)) inter_arrival_times = np.diff(event_times) return num_events, event_times, inter_arrival_times # A função **`plot_non_sequential_poisson`** visualiza um **Processo de Poisson**, exibindo em dois gráficos o tempo cumulativo dos eventos (em uma curva de passos) e o histograma dos intervalos entre eventos, destacando a distribuição exponencial dos tempos de espera, com taxa média `rate` e duração total `time_duration`. def plot_non_sequential_poisson(num_events, event_times, inter_arrival_times, rate, time_duration): fig, axs = plt.subplots(1, 2, figsize=(14, 5)) fig.suptitle(f&#39;Simulação de um Processo de Poisson (λ = {rate}, Duração = {time_duration} segundos)\\n&#39;, fontsize=14) # Gráfico 1: Tempo dos Eventos axs[0].step(event_times, np.arange(1, num_events + 1), where=&#39;post&#39;, color=&#39;blue&#39;) axs[0].set_xlabel(&#39;Tempo (s)&#39;) axs[0].set_ylabel(&#39;Número de Eventos&#39;) axs[0].set_title(f&#39;Evolução Incremental dos Eventos no Tempo\\nN(t) é não descrescente\\nTotal: {num_events} eventos\\n&#39;, fontsize=12) axs[0].grid(True) # Gráfico 2: Histograma dos Tempos de Espera axs[1].hist(inter_arrival_times, bins=20, density=True, color=&#39;green&#39;, alpha=0.5, label=&#39;Dados Empíricos&#39;) axs[1].set_xlabel(&#39;Tempo de Espera entre Eventos (s)&#39;) axs[1].set_ylabel(&#39;Densidade&#39;) axs[1].set_title( f&#39;Distribuição dos Tempos de Espera entre Eventos\\nE(T): {1/rate:.2f}, Sd(T): {np.std(inter_arrival_times):.2f}&#39;, fontsize=12 ) axs[1].grid(True, alpha=0.5) # Sobreposição da curva exponencial teórica lambda_param = rate x = np.linspace(0, max(inter_arrival_times), 100) y = expon.pdf(x, scale=1/lambda_param) axs[1].plot(x, y, &#39;r-&#39;, lw=2, label=f&#39;Modelo Exponencial (λ={lambda_param:.2f})&#39;) # Legenda axs[1].legend() # Ajustar espaçamento entre gráficos plt.subplots_adjust(left=0.08, right=0.95, top=0.85, bottom=0.1, wspace=0.3) # Ajustar layout final plt.tight_layout() plt.show() # A função **`plot_sequential_poisson`** visualiza múltiplos **Processos de Poisson** com diferentes taxas `rate`, exibindo em dois gráficos a evolução cumulativa dos eventos no tempo e os histogramas dos intervalos entre eventos, destacando a distribuição exponencial dos tempos de espera para cada taxa ao longo de uma duração definida `time_duration`. def plot_sequential_poisson(num_events_list, event_times_list, inter_arrival_times_list, rate, time_duration): fig, axs = plt.subplots(1, 2, figsize=(14, 5)) fig.suptitle(f&#39;Simulação de Processos de Poisson com diferentes λ (Duração = {time_duration} segundos)\\n&#39;, fontsize=14) # Gráfico 1: Tempo dos Eventos axs[0].set_xlabel(&#39;Tempo (s)&#39;) axs[0].set_ylabel(&#39;Número de Eventos&#39;) axs[0].set_title(f&#39;Evolução Incremental dos Eventos no Tempo \\nN(t) é não decrescente&#39;, fontsize=12) axs[0].grid(True) # Gráfico 2: Histograma dos Tempos de Espera axs[1].set_xlabel(&#39;Tempo de Espera entre Eventos (s)&#39;) axs[1].set_ylabel(&#39;Densidade&#39;) axs[1].set_title(f&#39;Distribuição dos Tempos de Espera: T(n) ~ Exp\\n E(T)=1/λ, Sd(T)=sqrt(1/λ²)&#39;, fontsize=12) axs[1].grid(True, alpha=0.5) color_palette = plt.get_cmap(&#39;tab20&#39;) colors = [color_palette(i) for i in range(len(rate))] for n, individual_rate in enumerate(rate): num_events = num_events_list[n] event_times = event_times_list[n] inter_arrival_times = inter_arrival_times_list[n] # Gráfico 1: Curva de passos para os tempos de chegada axs[0].step(event_times, np.arange(1, num_events + 1), where=&#39;post&#39;, color=colors[n], label=f&#39;λ = {individual_rate}, Total: {num_events}&#39;) # Gráfico 2: Histograma dos tempos entre eventos axs[1].hist(inter_arrival_times, bins=20, density=True, color=colors[n], alpha=0.5, label=f&#39;λ = {individual_rate}, E(T): {1/individual_rate:.2f}, Sd(T): {np.std(inter_arrival_times):.2f}&#39;) # Sobreposição da curva exponencial teórica x = np.linspace(0, max(inter_arrival_times), 100) y = expon.pdf(x, scale=1/individual_rate) axs[1].plot(x, y, color=colors[n], lw=2, linestyle=&#39;--&#39;, label=f&#39;Modelo Exp (λ={individual_rate:.2f})&#39;) axs[0].legend(loc=&#39;upper left&#39;, fontsize=9) axs[1].legend(loc=&#39;upper right&#39;, fontsize=9) # Ajustar espaçamento entre gráficos plt.subplots_adjust(left=0.08, right=0.95, top=0.85, bottom=0.1, wspace=0.3) # Ajustar layout final plt.tight_layout() plt.show() # A função **`poisson_simulation`** simula um ou vários **Processos de Poisson**, dependendo se `rate` é um valor único (int) ou uma lista de taxas, gerando tempos de ocorrência e intervalos entre eventos; além disso, visualiza os resultados por meio de gráficos que exibem a evolução temporal dos eventos e a distribuição dos tempos de espera ao longo de um intervalo definido por `time_duration`. def poisson_simulation(rate, time_duration, show_visualization=True): if isinstance(rate, int): num_events, event_times, inter_arrival_times = generate_poisson_events(rate, time_duration) if show_visualization: fig = plt.figure(figsize=(14, 6)) # Aumentar tamanho da figura plot_non_sequential_poisson(num_events, event_times, inter_arrival_times, rate, time_duration) fig.set_tight_layout(True) # Ajuste automático do layout else: return num_events, event_times, inter_arrival_times elif isinstance(rate, list): num_events_list = [] event_times_list = [] inter_arrival_times_list = [] for individual_rate in rate: num_events, event_times, inter_arrival_times = generate_poisson_events(individual_rate, time_duration) num_events_list.append(num_events) event_times_list.append(event_times) inter_arrival_times_list.append(inter_arrival_times) if show_visualization: fig = plt.figure(figsize=(18, 8)) # Maior espaço para gráficos sequenciais plot_sequential_poisson(num_events_list, event_times_list, inter_arrival_times_list, rate, time_duration) fig.set_tight_layout(True) # Ajuste automático do layout else: return num_events_list, event_times_list, inter_arrival_times_list Nas simulações a seguir confirma-se empiricamente que a evolução incremental da contagem de eventos ao longo do tempo (a variável aleatória \\(N(t)\\)) é crescente e a distribuição dos tempos de espera (a variável aleatória \\(T(n)\\) entre eventos segue um modelo exponencial com parametro 1/λ (código adaptado de link). poisson_simulation(rate=5, time_duration=100) poisson_simulation(rate=[2, 4, 6, 10], time_duration=100) plt.close(&#39;all&#39;) Exemplo 3: Em um sistema de atendimento telefônico, as chamadas chegam de acordo com um processo de Poisson com taxa média de chegada de λ=3 chamadas por minuto. Responda: 1) Qual é a probabilidade de que ocorram exatamente 2 chamadas em um intervalo de 1 minuto? 2) Qual a probabilidade do tempo entre chamadas ser menor que 20 segundos? 3) Sabendo que o sistema comporta sem interrupção o atendimento de até 6 chamadas por minuto, qual é a probabilidade do sistema apresentar falhas? 4) Sabendo que o sistema comporta sem interrupção o atendimento com um intervalo mínimo de 15 segundos entre elas, qual é a probabilidade do sistema apresentar falhas? A probabilidade de ocorrerem exatamente \\(k = 2\\) chamadas em 1 minuto é dada pela fórmula da distribuição de Poisson: \\[ P(N(t) = k) = \\frac{(\\lambda t)^k e^{-\\lambda t}}{k!}\\\\ P(N(1) = 2) = \\frac{(3 \\cdot 1)^2 e^{-3}}{2!}\\\\ P(N(1) = 2) = \\frac{9 e^{-3}}{2}\\\\ P(N(1) = 2) \\approx \\frac{9 \\cdot 0.0498}{2} = 0.2241 \\] A probabilidade de tempo entre chamadas ser menor que 20 segundos é \\[ P(T \\leq t) = 1 - e^{-\\lambda t}\\\\ P(T \\leq 1/3) = 1 - e^{-3 \\cdot \\frac{1}{3}}\\\\ P(T \\leq 1/3) \\approx 1 - 0.3679 = 0.6321 \\] Sabendo que o sistema comporta sem interrupção o atendimento de até 6 chamadas por minuto, a interrupção ocorrerá quando há mais de 6 chamadas no intervalo de 1 minuto, e a probabilidade disso ocorrer é \\[ P(N(1) &gt; 6) = 1 - P(N \\leq 6)\\\\ P(N(1) &gt; 6 ) = 1- \\sum_{k=0}^{6} \\frac{3^k e^{-3}}{k!}\\\\ P(N(1) &gt; 6 \\approx 1- 0.967\\\\ P(N(1) &gt; 6) \\approx 0.033\\\\ \\] Sabendo que o sistema comporta sem interrupção o atendimento com um intervalo mínimo de 15 segundos entre chamadas consecutivas, a interrupção ocorrerá quando o sistema receber cahamadas com um intervalo menor que 15 s e essa probabilidade será \\[ P(T \\leq t) = 1 - e^{-\\lambda t}\\\\ P(T \\leq 0.25) = 1 - e^{-3 \\cdot 0.25}\\\\ P(T \\leq 0.25) = 1 - e^{-0.75}\\\\ P(T \\leq 0.25) \\approx 1 - 0.4724 \\\\ P(T \\leq 0.25) \\approx 0.5276 \\] Esses resultados ilustram que: - É improvável que o sistema falhe devido a mais de 6 chamadas em 1 minuto. - Há uma alta probabilidade de falha devido a intervalos muito curtos entre chamadas consecutivas. 13.4.5 Distribuição condicional dos tempos de chegada Considere \\(N(t), t \\geq 0\\) com taxa \\(\\lambda\\). Sabendo-se a priori que um evento ocorreu exatamente no intervalo \\((0, t]\\), qual a distribuição do tempo até a ocorrência desse evento? Ou seja, desejamos estimar a probabilidade do evento ter ocorrido em intervalo de tempo \\(s\\) dado que ocorreu um evento no intervalo \\(t\\), sendo \\(s \\leq t\\) \\[ P(T_1 &lt; s \\mid N(t) = 1), \\, s \\leq t. \\] Pela hipótese de incrementos independentes e estacionários demonstra-se (Sheldon Ross, in Introduction to Probability Models, \\(6^{a}\\) ed., Cap. 5) que a distribuição é uniforme no intervalo \\((0, t]\\) e pode-se verificar que \\[ P(T_1 &lt; s \\mid N(t) = 1) = \\frac{s}{t}, \\, s \\leq t. \\] Exemplo 4: Em um sistema de atendimento telefônico, as chamadas chegam de acordo com um processo de Poisson com taxa média de chegada de λ=3 chamadas por minuto. Dado que ocorreram 2 chamadas em 1 minuto, qual é a probabilidade de que o tempo de espera para a primeira chamada seja inferior a 20 segundos? A probabilidade de que o tempo de espera para a primeira chamada seja inferior a \\(s=\\frac{1}{3} min\\) dado que ocorreram 2 chamadas em \\(t=1min\\) segue uma distribuição uniforme \\[ P(T_1 &lt; s \\mid N(t) = 1) = \\frac{s}{t}, \\, s \\leq t.\\\\ P(T_1 &lt; \\frac{1}{3} \\mid N(t=1) = 1) = \\frac{\\frac{1}{3}}{1}\\\\ P(T_1 &lt; \\frac{1}{3} \\mid N(t=1) = 1) \\approx 0.0333 \\] Dado que ocorreram 2 chamadas em 1 minuto, a probabilidade de que a primeira chamada ocorra nos primeiros 20 segundos é de 1/3 (aproximadamente 33,33%). "],["simulações-monte-carlo.html", "13.5 Simulações Monte Carlo", " 13.5 Simulações Monte Carlo 13.5.1 Introdução O Método de Monte Carlo, desenvolvido na década de 1940 por Stanislaw Ulam durante seu trabalho no Los Alamos National Laboratory, surgiu como uma ferramenta para simular processos probabilísticos (uma ideia semelhante já havia sido utilizada por Enrico Fermi no estudo da difusão de nêutrons, mas nunca foi publicada). John von Neumann aprimorou o método, implementando-o no ENIAC, o primeiro computador programável da história. É uma técnica matemática computacional amplamente utilizada para modelar e analisar sistemas complexos, frequentemente caracterizados por elevadas variabilidade e incerteza, nos quais os cálculos analíticos diretos seriam inviáveis devido ao grande número de variáveis ou à complexidade matemática envolvida. Baseiam-se na amostragem (ou geração) de valores aleatórios (estritamente falando, pseudoaleatórios), obtidos por algoritmos determinísticos - dentro de intervalos plausíveis para variáveis de entrada ie., suas distribuições de probabilidade específicas. As distribuições de probabilidade podem ser determinadas por: dados de séries temporais; estimativas de especialistas; conhecimento prévio. Cada simulação realizada gera um resultado e, ao serem repetidas um grande número de vezes, esses resultados agregados permitem a aproximação dos parâmetros da distribuição de probabilidade do fenômeno. A geração de valores aleatórios confiáveis é essencial para que os resultados sejam estatisticamente robustos, garantindo que as simulações ofereçam estimativas precisas e representativas. Aplicadas em áreas como finanças, engenharia, física e ciência de dados, permitindo a previsão de cenários, a análise de riscos e a otimização de decisões sob incerteza. A CDF desempenha um papel importante nas Simulações Monte Carlo. A função de distribuição cumulativa (CDF) dá a probabilidade de que a variável aleatória seja menor ou igual a um valor específico. 13.5.2 Fundamentação A Lei dos Grandes Números afirma que, à medida que o número de amostras aumenta, a média das observações converge para o valor esperado: \\[ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\to \\mathbb{E}[X] \\text{ quando } n \\to \\infty \\] O Teorema do Limite Central afirma que, para um número suficientemente grande de amostras independentes e identicamente distribuídas (i.i.d), a soma das variáveis converge em distribuição a uma distribuição Normal: \\[ \\frac{\\sum_{i=1}^{n} X_i - n \\mu}{\\sqrt{n} \\sigma} \\xrightarrow{d} N(0,1) \\] em que: \\(\\mu\\): média da variável aleatória \\(\\sigma\\): desvio padrão Esses teoremas garantem que métodos de Monte Carlo produzam aproximações confiáveis conforme se aumente o número de simulações. 13.5.3 Números Aleatórios e Pseudoaleatórios Os números verdadeiramente aleatórios são gerados a partir de processos físicos imprevisíveis, como ruído térmico ou decaimento radioativo. Esses métodos são difíceis de implementar computacionalmente devido ao custo e à dificuldade de captura dos fenômenos físicos. Os números pseudoaleatórios são gerados por algoritmos determinísticos que produzem sequências que se assemelham à aleatoriedade. Um dos métodos mais comuns é o Gerador Linear Congruente (Linear Congruential Generator – LCG), cuja fórmula matemática é dada por: \\[ X_{i+1} = (a X_i + c) \\mod m \\] em que: \\(X_i\\): Valor atual da sequência \\(a\\): Multiplicador \\(c\\): Incremento \\(m\\): Módulo (resto) \\(X_0\\): Semente inicial O módulo (\\(m\\)) é o valor pelo qual a expressão \\((a X_i + c)\\) é dividida, e o resto dessa divisão é o próximo número gerado na sequência. Considere como exemplo o LCG com parâmetros \\(a=9\\), \\(c=1\\), \\(m=17\\) e semente \\(X_0=7\\). O primeiro número aleatório (\\(X_0=7\\)) será \\[\\begin{align} X_{0} &amp; = 7 \\\\ X_{1} &amp; = (a X_0 + c) \\mod m X_{1} &amp; = (9*7+1) \\mod 17 \\\\ X_{1} &amp; = 64 \\mod 17 \\\\ X_{1} &amp; = 13 \\\\ \\end{align}\\] uma vez que \\(\\frac{64}{17} \\approx 3,76\\) e tomando-se \\(64-(3*17)=13\\). Assim \\(X_{1} = 13\\). A sequência é então normalizada para o intervalo \\([0, 1)\\) fazendo-se \\[ U_i = \\frac{X_{i+1}}{m} \\] No exemplo anterior teríamos: \\[\\begin{align} U_i &amp; = \\frac{X_{i+1}}{m}\\\\ U_0 &amp; = \\frac{X_{1}}{m}\\\\ U_0 &amp; = \\frac{13}{17}\\\\ U_0 &amp; = 0,76471\\\\ \\end{align}\\] Essa normalização é essencial para aplicações que requerem números pseudoaleatórios no intervalo \\([0, 1)\\) como as CDF. O método LCG é determinístico, ou seja, dada uma mesma semente inicial (\\(X_0\\)), ele produzirá sempre a mesma sequência. Isso pode ser problemático em contextos onde a imprevisibilidade é desejada. O LCG a seguir foi adaptado de link): from typing import Iterator, List # Função correta do Gerador Linear Congruente (LCG) def linear_congruential_generator(m: int, a: int, c: int, seed: int) -&gt; Iterator[int]: &quot;&quot;&quot; Implementa o Gerador Linear Congruente (LCG) para gerar números pseudoaleatórios. Parâmetros: - m: Módulo (define o intervalo dos números gerados). - a: Multiplicador (controla a dispersão dos números). - c: Incremento. - seed: Valor para iniciar a sequência. Retorna: - Um iterador que gera números inteiros pseudoaleatórios entre 0 e m-1. &quot;&quot;&quot; x = seed # Inicializa a sequência com a semente fornecida while True: yield x # Retorna o valor atual de x x = (a * x + c) % m # Calcula o próximo valor na sequência usando a fórmula do LCG # Função para gerar uma lista de números pseudoaleatórios def generate_lcg_samples(n_samples: int, m: int, a: int, c: int, seed: int) -&gt; List[int]: &quot;&quot;&quot; Gera uma lista de `n_samples` números pseudoaleatórios não normalizados usando o método do Gerador Linear Congruente (LCG). Parâmetros: - n_samples: Número de amostras a serem geradas. - m: Módulo do LCG. - a: Multiplicador do LCG. - c: Incremento do LCG. - seed: Semente inicial. Retorna: - Uma lista de números inteiros pseudoaleatórios entre 0 e m-1. &quot;&quot;&quot; gen = linear_congruential_generator(m, a, c, seed) # Inicializa o gerador corretamente return [next(gen) for _ in range(n_samples)] # Coleta n_samples números Gerando números pseudoaleatórios com valores \\(m = 16\\), \\(a = 11\\), \\(c = 0\\), \\(seed=1\\): # Parâmetros do LCG com ciclo visível m = 16 # Módulo pequeno para demonstrar a repetibilidade a = 11 # Multiplicador c = 0 # Incremento seed = 1 # Semente inicial X0 n_samples = 200 # Número de amostras para observar o ciclo completo # Gerar números com o LCG (não normalizados) lcg_samples = generate_lcg_samples(n_samples, m, a, c, seed) # Exibir os números gerados # print(lcg_samples) # Função para gerar uma sequência de números pseudoaleatórios normalizados no intervalo [0,1) def lcg_normalized(n_samples: int, m: int, a: int, c: int, seed: int = 1) -&gt; list[float]: &quot;&quot;&quot; Gera uma sequência de números pseudoaleatórios normalizados no intervalo [0,1) usando o Gerador Linear Congruente (LCG). Parâmetros: - n_samples: Número de amostras a serem geradas. - m: Módulo do LCG. - a: Multiplicador do LCG. - c: Incremento do LCG. - seed: Valor para iniciar a sequência. Retorna: - Uma lista de números float no intervalo [0,1). &quot;&quot;&quot; gen = linear_congruential_generator(m, a, c, seed) # Inicializa o gerador LCG sequence = [] # Lista para armazenar os números normalizados # Gera os números pseudoaleatórios normalizados for _ in range(n_samples): rand: float = next(gen) / m # Normaliza o número gerado para o intervalo [0, 1) sequence.append(rand) # Adiciona o número normalizado à sequência return sequence # Retorna a lista completa de números normalizados Gerando números pseudoaleatórios com valores \\(m = 16\\), \\(a = 11\\), \\(c = 0\\), \\(seed=1\\): # Parâmetros do LCG com ciclo visível m = 16 # Módulo pequeno para demonstrar a repetibilidade a = 11 # Multiplicador c = 0 # Incremento seed = 1 # Semente inicial X0 n_samples = 100 # Número de amostras para observar o ciclo completo # Gerar números com o LCG (normalizados) lcg_samples_norm = lcg_normalized(n_samples, m, a, c, seed) # Exibir os números gerados #print(lcg_samples_norm) A escolha arbitrária de \\(m,a,c\\) resulta em valores em um ciclo de repetibilidade previsível conforme os gráficos a seguir mostram: import numpy as np import matplotlib.pyplot as plt # Criar índice para os pontos gerados indices = np.arange(n_samples) # Criar subplots lado a lado fig, axes = plt.subplots(1, 2, figsize=(14, 5)) # Gráfico 1: Dispersão para observar padrão periódico axes[0].scatter(indices, lcg_samples_norm, s=50, alpha=0.7, color=&#39;blue&#39;) # Corrigido: índices no eixo X axes[0].set_title(&#39;Distribuição de Números Pseudoaleatórios \\n(Ciclo Repetitivo)&#39;) axes[0].set_xlabel(&#39;Índice da Sequência&#39;) axes[0].set_ylabel(&#39;Números gerados (0 a 1)&#39;) axes[0].grid(True) # Gráfico 2: Barras para visualizar periodicidade axes[1].bar(indices, lcg_samples_norm, color=&#39;gray&#39;, edgecolor=&#39;black&#39;) axes[1].set_title(&#39;Gráfico de Barras \\n(Ciclo Evidenciado)&#39;) axes[1].set_xlabel(&#39;Índice da Sequência&#39;) axes[1].set_ylabel(&#39;Números gerados (0 a 1)&#39;) axes[1].grid(True) # Ajustar layout para melhor visualização plt.tight_layout() plt.show() De acordo com Donald Knuth (The Art Of Computer Programming), um gerador pseudoaleatório linear produz uma sequência aperiódica se as seguintes condições forem satisfeitas: Se \\(p\\) é um número primo que divide \\(m\\), então \\(p\\) deve dividir \\(c\\). Se \\(m\\) é múltiplo de 4, então \\(a-1\\) também deve ser múltiplo de 4. O único número inteiro que divide exatamente \\(a\\) e \\(m\\) deve ser 1 (ou seja, \\(a\\) e \\(m\\) devem ser coprimos). Essas condições garantem um período máximo e uma boa distribuição dos números gerados. Considerando os parâmetros escolhidos: \\(m = 2^{31} = 2\\,147\\,483\\,648\\) \\(a = 594\\,156\\,893\\) \\(c = 0\\) verificamos as três condições: Se \\(p\\) é um número primo que divide \\(m\\), então \\(p\\) deve dividir \\(c\\). Como \\(m = 2^{31}\\), o único primo que o divide é \\(p = 2\\). Como \\(c = 0\\), \\(p\\) divide \\(c\\), satisfazendo a condição. Se \\(m\\) é múltiplo de 4, então \\(a-1\\) também deve ser múltiplo de 4. Como \\(m = 2^{31}\\) é múltiplo de 4, verificamos se \\(a - 1\\) também é múltiplo de 4: \\[ a - 1 = 594\\,156\\,893 - 1 = 594\\,156\\,892 \\] Como \\(594\\,156\\,892 \\div 4\\) é inteiro, a condição é satisfeita. O único número inteiro que divide exatamente \\(a\\) e \\(m\\) deve ser 1 (ou seja, \\(a\\) e \\(m\\) devem ser coprimos). Calculamos o máximo divisor comum \\(\\gcd(a, m)\\), obtendo: \\[ \\gcd(594\\,156\\,893, 2^{31}) = 1 \\] isso confirma que \\(a\\) e \\(m\\) são coprimos, atendendo à terceira condição. Portanto, os parâmetros escolhidos atendem às condições de Knuth, garantindo que o gerador linear congruencial produzirá uma sequência aperiódica. # Definição dos parâmetros em acordo com Knuth m: int = 2_147_483_648 # Define o módulo (2^31), garantindo um grande período de números únicos a: int = 594_156_893 # Define o multiplicador, cuidadosamente escolhido para evitar ciclos curtos c: int = 0 # Incremento igual a zero, caracterizando um gerador multiplicativo seed: int = 1 # Define o número inicial n_samples: int = 200 # Número de amostras para observar o ciclo completo # Gerar números com o LCG (normalizados) lcg_samples_norm = lcg_normalized(n_samples, m, a, c, seed) A escolha desses parãmetros resulta em valores em um ciclo de repetibilidade muito menos previsível conforme os gráficos a seguir mostram: import numpy as np import matplotlib.pyplot as plt # Criar índice para os pontos gerados indices = np.arange(n_samples) # Criar subplots lado a lado fig, axes = plt.subplots(1, 2, figsize=(14, 5)) # Gráfico 1: Dispersão para observar padrão periódico axes[0].scatter(indices, lcg_samples_norm, s=50, alpha=0.7, color=&#39;blue&#39;) # Corrigido: índices no eixo X axes[0].set_title(&#39;Distribuição de Números Pseudoaleatórios \\n(Ciclo Repetitivo)&#39;) axes[0].set_xlabel(&#39;Índice da Sequência&#39;) axes[0].set_ylabel(&#39;Números gerados (0 a 1)&#39;) axes[0].grid(True) # Gráfico 2: Barras para visualizar periodicidade axes[1].bar(indices, lcg_samples_norm, color=&#39;gray&#39;, edgecolor=&#39;black&#39;) axes[1].set_title(&#39;Gráfico de Barras \\n(Ciclo Evidenciado)&#39;) axes[1].set_xlabel(&#39;Índice da Sequência&#39;) axes[1].set_ylabel(&#39;Números gerados (0 a 1)&#39;) axes[1].grid(True) # Ajustar layout para melhor visualização plt.tight_layout() plt.show() Números aleatórios numa faixa específica [a,b] Para gerar números aleatórios em uma faixa diferente de 0 e 1 usando um gerador linear congruente, você deve ajustar os parâmetros de normalização. Especificamente, para obter números em um intervalo \\([a, b]\\), você deve multiplicar o número gerado (que está inicialmente entre 0 e 1) pela amplitude do intervalo \\(b - a\\) e, em seguida, somar o valor mínimo do intervalo a. # Gerar números pseudoaleatórios normalizados no intervalo [0,1) m: int = 2_147_483_648 # Define o módulo (2^31), garantindo um grande período de números únicos a: int = 594_156_893 # Define o multiplicador, cuidadosamente escolhido para evitar ciclos curtos c: int = 0 # Incremento igual a zero, caracterizando um gerador multiplicativo seed: int = 1 # Define o número inicial n_samples: int = 200 # Número de amostras para observar o ciclo completo # Gerar números com o LCG (normalizados) lcg_samples_norm = lcg_normalized(n_samples, m, a, c, seed) # Faixa desejada a_range = 1 # Novo limite inferior b_range = 3 # Novo limite superior # Transformar os números normalizados para a nova faixa [a_range, b_range] lcg_samples_ab = [a_range + x * (b_range - a_range) for x in lcg_samples_norm] # Criar índices para os pontos gerados indices = np.arange(len(lcg_samples_norm)) indices_b = np.arange(len(lcg_samples_ab)) # Criar subplots lado a lado fig, axes = plt.subplots(1, 2, figsize=(14, 5)) # Gráfico 1: Dispersão dos números em [0,1] axes[0].scatter(indices, lcg_samples_norm, s=50, alpha=0.7, color=&#39;blue&#39;) axes[0].set_title(&#39;Distribuição de Números Pseudoaleatórios em [0,1]&#39;) axes[0].set_xlabel(&#39;Índice da Sequência&#39;) axes[0].set_ylabel(&#39;Números gerados em [0,1]&#39;) axes[0].grid(True) # Gráfico 2: Dispersão dos números em [a_range,b_range] axes[1].scatter(indices_b, lcg_samples_ab, s=50, alpha=0.7, color=&#39;blue&#39;) axes[1].set_title(&#39;Distribuição de Números Pseudoaleatórios em [1,3]&#39;) axes[1].set_xlabel(&#39;Índice da Sequência&#39;) axes[1].set_ylabel(&#39;Números gerados em [1,3]&#39;) axes[1].grid(True) # Ajustar layout plt.tight_layout() plt.show() 13.5.4 Geração de amostras aleatórias de distribuições de probabilidade Os números pseudoaleatórios gerados pelo LCG (\\(U_{i})\\) seguem uma distribuição uniforme no intervalo \\([0,1]\\). No entanto, muitos problemas exigem amostras de distribuições diferentes, como a Normal, Exponencial ou Poisson. 13.5.4.1 Método da Inversa da Função de distribuição Acumulada (CDF) A geração de números aleatórios de uma distribuição de probabilidade qualquer pode ser feita por meio da inversa de sua função de distribuição acumulada (CDF). Se \\(F(x)\\) é a CDF de uma variável aleatória \\(X\\), então: \\[ X = F^{-1}(U) \\] em que: \\(U \\sim U(0,1)\\): é um número pseudoaleatório que segue uma distribuição uniforme \\(F^{-1}\\): função inversa da CDF da distribuição da variável aleatória \\(X\\) Exemplo1: variável aleatória \\(X\\) que siga uma distribuição uniforme com parâmetros \\(a,b\\). Sua CDF é \\[ F(x)=\\frac{x-a}{b-a} \\] Invertendo-se a CDF tem-se \\[ x=F^{-1}(p) = a + p.(b-1) \\]   em que \\(p\\) é o valor da probabilidade no intervalo [0,1]. Como \\(U \\sim U(0,1)\\) é um número pseudoaleatório uniforme \\[ F^{-1}(U) = a + U(b-a) \\] Se admitirmos valores \\(a=2,b=3\\), um valor aleatório \\(x\\) da variável aleatória \\(X\\) gerado a partir do número aleatório \\(U=0.56\\) será \\(x=2+0.56*(3-2)=2.56\\). Exemplo 2: variável aleatória \\(X\\) que siga a distribuição exponencial com parâmetro \\(\\lambda\\). Sua CDF é \\[ F(x) = 1 - e^{-\\lambda x} \\] Invertendo-se a CDF tem-se \\[ x=F^{-1}(p) = -\\frac{1}{\\lambda} \\ln(1 - p). \\] em que \\(p\\) é o valor da probabilidade no intervalo [0,1]. Como \\(U \\sim U(0,1)\\) é um número pseudoaleatório uniforme \\[ F^{-1}(U) = -\\frac{1}{\\lambda} \\ln(1 - U). \\] Se admitirmos um valor de \\(\\lambda=3\\), um valor aleatório \\(x\\) da variável aleatória \\(X\\) gerado a partir do número aleatório \\(U=0.56\\) será \\(x= -\\frac{1}{3} \\ln(1 - 0.56)=0.2729\\). Exemplo 3: variável aleatória \\(X\\) que siga uma distribuição Normal com parâmetros \\(\\mu, \\sigma\\). Sua CDF é \\[ F(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^{x} e^{-\\frac{(t - \\mu)^2}{2 \\sigma^2}} \\, dt\\\\ \\] Essa função não possui uma inversa fechada em termos de funções elementares. Portanto, sua inversa é frequentemente aproximada numericamente ou expressa em termos da função de erro inversa (\\(\\operatorname{erf}^{-1}\\)). 13.5.4.2 Quando \\(F^{-1}(X)\\) não possui uma inversa fechada Método da Aceitação-Rejeição: usado para gerar amostras de uma distribuição-alvo quando a inversa da CDF não está disponível, utilizando uma distribuição auxiliar e uma função de aceitação para gerar amostras. Método de Newton-Raphson: método numérico iterativo usado para para encontrar aproximações da inversa da CDF resolvendo a equação \\(F(X)=U\\). 13.5.5 Exemplo 1 (Goodwin e Wright, 2009) Neste exemplo, analisamos as entradas (cash inflows) e saídas (cash outflows) de caixa de um sistema para se estimar Lucro = Cash inflows - Cash outflows. Entradas de Caixa (Cash Inflows) Cash inflows ($) Probabilidade CDF 50,000 0.30 0.3 60,000 0.40 0.7 70,000 0.30 1.0 Probabilidade de 30% para entrada de caixa de 50,000 Probabilidade de 40% para entrada de caixa de 60,000 Probabilidade de 30% para entrada de caixa de 70,000 Saídas de Caixa (Cash Outflows) Cash outflows ($) Probability (%) CDF 50,000 0.45 0.45 70,000 0.55 1.00 Probabilidade de 45% para saída de caixa de 50,000. Probabilidade de 55% para saída de caixa de 70,000. Admitindo-se que aos valores cash inflow sejam uma variável aleatória \\(X\\) com função distribuição de probabilidade: \\[ P(X = x) = \\begin{cases} 0.30, &amp; x = 50,000 \\\\ 0.40, &amp; x = 60,000 \\\\ 0.30, &amp; x = 70,000 \\\\ 0, &amp; \\text{outros valores} \\end{cases} \\] e que que os valores cash outflow sejam uma variável aleatória \\(Y\\) com função distribuição de probabilidade: \\[ P(Y = y) = \\begin{cases} 0.45, &amp; y = 50,000 \\\\ 0.55, &amp; y = 70,000 \\\\ 0, &amp; \\text{outros valores} \\end{cases} \\] números aleatórios \\(U,V\\) podem ser gerados e mapeados para valores de cash inflow e cash outflow a partir desses distribuições, e o lucro calculado para cada par de valores. Etapas da simulação Passo 1: Gerar valores aleatórios uniformes \\(U\\)** no intervalo \\([0,1]\\) para representar Cash Inflows e \\(V\\)** no intervalo \\([0,1]\\) Cash Outflows. Passo 2: Mapeamento para Cash Inflows Para cada número aleatório \\(U\\): - Se \\(0 \\leq U &lt; 0.30\\) → Cash Inflow = 50,000 - Se \\(0.30 \\leq U &lt; 0.70\\) → Cash Inflow = 60,000 - Se \\(0.70 \\leq U \\leq 0.99\\) → Cash Inflow = 70,000 Passo 3: Mapeamento para Cash Outflows Para cada número aleatório \\(V\\): - Se \\(0 \\leq V &lt; 0.45\\) → Cash Outflow = 50,000 - Se \\(0.45 \\leq V \\leq 0.99\\) → Cash Outflow = 70,000 Passo 4: Cálculo do Lucro Para cada par de valores simulados: \\[ \\text{Lucro} = \\text{Cash Inflow} - \\text{Cash Outflow} \\] Passo 5: Análise dos Resultados Calcule a média dos lucros obtidos. Visualize os resultados por meio de gráficos ou tabelas. # Simulação de Monte Carlo para Fluxo de Caixa Usando U e V # Parâmetros de Cash Inflows cash_inflows &lt;- c(50000, 60000, 70000) prob_inflows &lt;- c(0.30, 0.40, 0.30) cdf_inflows &lt;- cumsum(prob_inflows) # Parâmetros de Cash Outflows cash_outflows &lt;- c(50000, 70000) prob_outflows &lt;- c(0.45, 0.55) cdf_outflows &lt;- cumsum(prob_outflows) # Número de simulações n_simulations &lt;- 1000 # Geração de números aleatórios independentes para U (Inflows) e V (Outflows) set.seed(123) # Garante reprodutibilidade U &lt;- runif(n_simulations) # Para Cash Inflows V &lt;- runif(n_simulations) # Para Cash Outflows # Vetores para armazenar resultados simulated_inflows &lt;- numeric(n_simulations) simulated_outflows &lt;- numeric(n_simulations) simulated_profit &lt;- numeric(n_simulations) # Mapeamento de U para Cash Inflows for (i in 1:n_simulations) { if (U[i] &lt; cdf_inflows[1]) { simulated_inflows[i] &lt;- cash_inflows[1] } else if (U[i] &lt; cdf_inflows[2]) { simulated_inflows[i] &lt;- cash_inflows[2] } else { simulated_inflows[i] &lt;- cash_inflows[3] } } # Mapeamento de V para Cash Outflows for (i in 1:n_simulations) { if (V[i] &lt; cdf_outflows[1]) { simulated_outflows[i] &lt;- cash_outflows[1] } else { simulated_outflows[i] &lt;- cash_outflows[2] } } # Cálculo do Lucro simulated_profit &lt;- simulated_inflows - simulated_outflows # Tabela de resultados agregados profit_table &lt;- as.data.frame(table(simulated_profit)) profit_table$Probability &lt;- as.numeric(profit_table$Freq) / n_simulations # Gráfico com Probabilidades no Topo das Barras library(ggplot2) ggplot(profit_table, aes(x = as.numeric(as.character(simulated_profit)), y = Probability)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + geom_text( aes(label = sprintf(&quot;%.2f&quot;, Probability)), vjust = -0.5, size = 4 ) + labs( title = &quot;Distribuição do Lucro Simulado&quot;, x = &quot;Lucro ($)&quot;, y = &quot;Probabilidade&quot; ) + theme_minimal() 13.5.6 Exemplo 2: The Elite Pottery Company (Goodwin e Wright, 2009) Neste exemplo, analisamos os custos (Variable Costs), vendas (Sales) e custos fixos (Fixed Costs) de um sistema para estimar: \\(Lucro = (Sales - Variable \\, Costs) \\times Sales - Fixed \\, Costs\\) Custos Variáveis (Variable Costs) Variable Costs ($) Probabilidade CDF 13 0.30 0.30 8 0.40 0.70 18 0.30 1.00 Probabilidade de 30% para custo variável de 13 Probabilidade de 40% para custo variável de 8 Probabilidade de 30% para custo variável de 18 Vendas (Sales) Sales Probabilidade CDF 22,000 0.30 0.30 10,000 0.40 0.70 30,000 0.30 1.00 Probabilidade de 30% para vendas de 22,000 Probabilidade de 40% para vendas de 10,000 Probabilidade de 30% para vendas de 30,000 Custos Fixos (Fixed Costs) Fixed Costs ($) Probabilidade CDF 175,000 0.30 0.30 100,000 0.40 0.70 300,000 0.30 1.00 Probabilidade de 30% para custo fixo de 175,000 Probabilidade de 40% para custo fixo de 100,000 Probabilidade de 30% para custo fixo de 300,000 Admitindo-se que os valores Variable Costs sejam uma variável aleatória \\(X\\) com função distribuição de probabilidade: \\[ P(X = x) = \\begin{cases} 0.30, &amp; x = 13 \\\\ 0.40, &amp; x = 8 \\\\ 0.30, &amp; x = 18 \\\\ 0, &amp; \\text{outros valores} \\end{cases} \\] E que os valores Sales sejam uma variável aleatória \\(Y\\) com função distribuição de probabilidade: \\[ P(Y = y) = \\begin{cases} 0.30, &amp; y = 22,000 \\\\ 0.40, &amp; y = 10,000 \\\\ 0.30, &amp; y = 30,000 \\\\ 0, &amp; \\text{outros valores} \\end{cases} \\] E que os valores Fixed Costs sejam uma variável aleatória \\(Z\\) com função distribuição de probabilidade: \\[ P(Z = z) = \\begin{cases} 0.30, &amp; z = 175,000 \\\\ 0.40, &amp; z = 100,000 \\\\ 0.30, &amp; z = 300,000 \\\\ 0, &amp; \\text{outros valores} \\end{cases} \\] Números aleatórios \\(U, V, W\\) podem ser gerados e mapeados para valores de Variable Costs, Sales e Fixed Costs a partir dessas distribuições, e o lucro calculado para cada conjunto de valores. Etapas da Simulação Passo 1: Gerar valores aleatórios uniformes \\(U\\) no intervalo \\([0,1]\\) para representar Variable Costs. \\(V\\) no intervalo \\([0,1]\\) para representar Sales. \\(W\\) no intervalo \\([0,1]\\) para representar Fixed Costs. Passo 2: Mapeamento para Variable Costs Para cada número aleatório \\(U\\): - Se \\(0 \\leq U &lt; 0.30\\) → Variable Cost = 13 - Se \\(0.30 \\leq U &lt; 0.70\\) → Variable Cost = 8 - Se \\(0.70 \\leq U \\leq 0.99\\) → Variable Cost = 18 Passo 3: Mapeamento para Sales Para cada número aleatório \\(V\\): - Se \\(0 \\leq V &lt; 0.30\\) → Sales = 22,000 - Se \\(0.30 \\leq V &lt; 0.70\\) → Sales = 10,000 - Se \\(0.70 \\leq V \\leq 0.99\\) → Sales = 30,000 Passo 4: Mapeamento para Fixed Costs Para cada número aleatório \\(W\\): - Se \\(0 \\leq W &lt; 0.30\\) → Fixed Costs = 175,000 - Se \\(0.30 \\leq W &lt; 0.70\\) → Fixed Costs = 100,000 - Se \\(0.70 \\leq W \\leq 0.99\\) → Fixed Costs = 300,000 Passo 5: Cálculo do Lucro Para cada conjunto de valores simulados: \\[ \\text{Lucro} = \\text{(Sales - Variable Cost)} \\times \\text{Sales - Fixed Cost} \\] Passo 6: Análise dos Resultados Calcule a média dos lucros obtidos. Visualize os resultados por meio de gráficos ou tabelas. # Parâmetros variable_costs &lt;- c(13, 8, 18) sales &lt;- c(22000, 10000, 30000) fixed_costs &lt;- c(175000, 100000, 300000) prob_variable &lt;- c(0.30, 0.40, 0.30) prob_sales &lt;- c(0.30, 0.40, 0.30) prob_fixed &lt;- c(0.30, 0.40, 0.30) cdf_variable &lt;- cumsum(prob_variable) cdf_sales &lt;- cumsum(prob_sales) cdf_fixed &lt;- cumsum(prob_fixed) # Simulação n_simulations &lt;- 1000 set.seed(123) U &lt;- runif(n_simulations) V &lt;- runif(n_simulations) W &lt;- runif(n_simulations) variable_sim &lt;- variable_costs[findInterval(U, cdf_variable) + 1] sales_sim &lt;- sales[findInterval(V, cdf_sales) + 1] fixed_sim &lt;- fixed_costs[findInterval(W, cdf_fixed) + 1] profit &lt;- (25 - variable_sim) * sales_sim - fixed_sim # Tabela de resultados agregados com probabilidade profit_table &lt;- as.data.frame(table(profit)) profit_table$Probability &lt;- as.numeric(profit_table$Freq) / n_simulations # Gráfico com Probabilidades no Topo das Barras library(ggplot2) ggplot(profit_table, aes(x = as.numeric(as.character(profit)), y = Probability)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + geom_text( aes(label = sprintf(&quot;%.2f&quot;, Probability)), vjust = -0.5, size = 4 ) + labs( title = &quot;Distribuição do Lucro Simulado&quot;, x = &quot;Lucro ($)&quot;, y = &quot;Probabilidade&quot; ) + theme_minimal() 13.5.7 Exemplo 3: Integração Numérica Usando o Método de Monte Carlo A integração numérica pelo Método de Monte Carlo é uma técnica amplamente utilizada para aproximar valores de integrais definidas, especialmente quando as funções envolvidas são complexas ou não possuem primitivas analíticas. A ideia central do método é substituir o problema do cálculo integral por um problema de probabilidade. Etapas Etapa 1: Pontos aleatórios são gerados uniformemente dentro de um retângulo que cobre a região de integração. Etapa 2: Cada ponto é testado para verificar se está abaixo ou acima da curva definida pela função \\(f(x)\\). Etapa 3: A razão entre os pontos que caem abaixo da curva e o número total de pontos é usada para estimar a área sob a curva, que corresponde ao valor da integral. A estimativa da integral é calculada por meio da seguinte fórmula: \\[ I \\approx \\frac{\\text{Pontos Abaixo da Curva}}{\\text{Total de Pontos}} \\times (b - a) \\times f_{max} \\] em que: - \\((b - a)\\): representa o intervalo de integração. - \\(f_{max}\\): é o valor máximo da função no intervalo considerado. - \\(n\\): número de pontos aleatórios gerados. Esse método é especialmente útil quando as funções são complexas ou multidimensionais, tornando os métodos tradicionais de integração inviáveis. No código a seguir, apresentamos uma implementação generalizada para integrar qualquer função definida em um intervalo arbitrário, com representação visual dos pontos gerados. Aplicação para a determinação da integral da função \\(f(x) = sin(x)\\) nos limites \\([0 \\pi]\\): \\[ I = \\int_0^{\\pi} \\sin(x) \\, dx \\] Sabemos que a primitiva de \\(\\sin(x)\\) é dada por: \\[ \\int \\sin(x) \\, dx = -\\cos(x) + C \\\\ I = \\left[-\\cos(x)\\right]_0^{\\pi}\\\\ I = -\\cos(\\pi) + \\cos(0)\\\\ \\cos(\\pi) = -1 \\quad \\text{e} \\quad \\cos(0) = 1\\\\ I = -(-1) + 1\\\\ I = 1 + 1 = 2\\\\ \\] O valor exato da integral de \\(f(x) = \\sin(x)\\) no intervalo \\([0, \\pi]\\) é: \\[ I = 2 \\] Esse resultado analítico servirá como referência para compararmos com os valores obtidos numericamente pelo método de Monte Carlo. # Função a ser integrada f &lt;- function(x) { sin(x) } # Parâmetros a &lt;- 0 b &lt;- pi f_max &lt;- 1 n &lt;- 10000 # Gerar gráfico com valor da integral exibido graf &lt;- monte_carlo_integration(f, a, b, f_max, n) graf "],["orientações-gerais.html", "Capítulo 14 Orientações Gerais", " Capítulo 14 Orientações Gerais "],["informações-administrativas.html", "14.1 Informações administrativas", " 14.1 Informações administrativas 14.1.1 Regimento geral da UEL O Regimento Geral da Universidade Estadual de Londrina está disponível nesse link Figure 14.1: Regimento Geral da UEL Figure 14.2: Artigos 60 e 71 do Regimento Geral da UEL 14.1.2 Amparos e apoios na UEL Figure 14.3: Amparos e apoios na UEL 14.1.3 Tutoriais para os estudantes da graduação da UEL Tutoriais para os estudantes da graduação da Universidade Estadual de Londrina estão disponíveis nesse link Figure 14.4: Tutoriais para os estudantes da graduação da UEL "],["programas-de-atividade-acadêmica.html", "14.2 Programas de atividade acadêmica", " 14.2 Programas de atividade acadêmica 14.2.1 Geografia: 1STA004 - Estatística Aplicada à Geografia 14.2.1.1 Horários: Turma 1000: quartas-feiras (19 h 15 min - 20 h 55 min / 21 h 10 min - 22 h 50 min) 14.2.1.2 Locais: CCE Turma 1000: sala 705 (CCE) 14.2.1.3 Ementa conforme projeto pedagógico do curso, Resolução CEPE/CA 026/2019: Introdução à Estatística. Amostragem. Tabelas. Gráficos. Distribuição de Frequências. Medidas de Posição. Medidas de Dispersão. Números Índices. Correlação. Regressão Linear Simples. Séries Temporais. 14.2.1.4 Conteúdo programático: Introdução à pesquisa científica, análise exploratória e descritiva de dados e assuntos correlacionados: noções sobre a produção de conhecimento por meio da pesquisa científica, conceitos de população e parâmetros, amostra e estatísticas, tipos de variáveis, indexação e somatório de dados, combinações, conectivos lógicos, conjuntos, diagramas de Venn, operações com conjuntos, apresentação dos dados brutos e em rol, apresentações gráficas elementares, sínteses numéricas de posição (média, moda), sínteses numéricas de dispersão (máximo, mínimo, amplitude, variância e coeficiente de variação), medidas separatrizes (percentis, decis, quartis), mediana, apresentação tabular de dados, média, moda e variância para dados agrupados em tabelas de frequências, apresentações gráficas para variáveis qualitativas (barras, setores) e quantitativas (histograma, box-plot). Introdução ao planejamento de pesquisas e levantamentos amostrais: tipos de levantamentos amostrais (probabilísticos e não probabilísticos), levantamento amostral aleatório simples e sistematizado, levantamento amostral aleatório estratificado e por conglomerados, dimensionamento de amostras para inferências sobre médias e proporções (distribuição das médias e proporções amostrais). Introdução às técnicas de regressão: análise de correlação linear e regressão linear simples. Modelo clássico de séries temporais. 14.2.1.5 Bibliografia básica: A. MARTINS, Gilberto e DOMINGUES, Osmar (2017). Estatística Geral e Aplicada. 6ª ed. Rio de Janeiro: Atlas. MORETTIN, Pedro A. e O. BUSSAB, Wilton (2017). Estatística básica. 9ª ed. Rio de Janeiro: Editora Saraiva. TRIOLA, Mario F. (2024). Introdução à Estatística. 14ª ed. Rio de Janeiro: LTC. 14.2.1.6 Procedimentos de ensino O processo de ensino será composto por um conjunto de atividades presenciais expositivas e práticas com a resolução de exercícios em sala de aula a ter início no dia 26/03/2025, prosseguindo nos dias e horários determinados no Sistema UEL. Todo material utilizado (textos, slides, listas de exercícios) será disponibilizado na sala de aula virtual a ser criada na Plataforma Google Classroom, de adesão compulsória por parte do discente, por meio de convite enviado ao seu email registrado no Sistema UEL. Toda comunicação de natureza pedagógica entre discentes e o docente dar-se-á por meio de postagens na plataforma Google Classroom para que todos os alunos possam ter acesso e se beneficiar do conteúdo. O atendimento presencial e em caráter pessoal, para elucidação de eventuais dúvidas de discentes, deverá ser previamente agendado com o docente. As datas das atividades, avaliações e do exame final, indicadas no cronograma abaixo, poderão ser alteradas tanto em razão de situações não previstas pelo atual calendário acadêmico quanto do bom progresso das atividades didáticas, mediante comunicação aos alunos com razoável antecipação. 14.2.1.7 Formas e critérios de avaliação: Durante o semestre serão realizadas as seguintes atividades avaliativas: - duas (2) provas presenciais escritas (P1 e P2) referentes ao conteúdo das aulas e valendo de zero (0) a dez (10) pontos cada uma e com peso 5 para fins da determinação da média final (MF); - duas (2) atividades no formato de listas de exercícios (L1 e L2) disponibilizadas na plataforma Google Classroom, valendo de zero (0) a dez (10) pontos. As atividades L1 e L2 poderão ser compostas por mais de uma lista sendo, nesse caso, atribuída a média aritmética com peso 1 das notas de todas as listas que compõem cada atividade conforme a expressão: \\[ L_1 = \\frac{L_{1,1} + · · · + L_{1,n_1}}{n_1} \\] e \\[ L_2 = \\frac{L_{2,1} + · · · + L_{2,n_2}}{n_2} \\] A média final (MF) será calculada pela seguinte expressão: \\[ MF = \\frac{(5 × P_1) + (1 × L_1) + (5 × P_2) + (1 × L_2)}{12} \\] 14.2.2 Química: 2STA032 - Estatística 14.2.2.1 Horários: Turma 1000: segundas-feiras (8 h 15 min - 9 h 55 min) e quartas-feiras (10 h 15 min - 11 h 55 min) 14.2.2.2 Locais: CCE Turma 1000: sala 7 (CCE) 14.2.2.3 Ementa conforme projeto pedagógico do curso na Resolução CEPE/CA 18/2024: 14.2.2.4 Conteúdo programático: 14.2.2.5 Bibliografia básica: 14.2.2.6 Procedimentos de ensino 14.2.2.7 Formas e critérios de avaliação: 14.2.3 Farmácia: 2STA010 - Elementos de bioestatística 14.2.3.1 Horários: Turmas 0001, 0002 e 0003: quintas-feiras (10 h 15 min - 11 h 55 min) 14.2.3.2 Locais: Turmas 0001, 0002 e 0003: sala 113 (CCLH) 14.2.3.3 Ementa conforme projeto pedagógico do curso na Resolução CEPE/CA 004/2022: 14.2.3.4 Conteúdo programático: Introdução à pesquisa científica, análise exploratória e descritiva de dados e assuntos correlacionados: noções sobre a produção de conhecimento por meio da pesquisa científica, conceitos de população e parâmetros, amostra e estatísticas, tipos de variáveis, indexação e somatório de dados, combinações, conectivos lógicos, conjuntos, diagramas de Venn, operações com conjuntos, apresentação dos dados brutos e em rol, apresentações gráficas elementares, sínteses numéricas de posição (média, moda), sínteses numéricas de dispersão (máximo, mínimo, amplitude, variância e coeficiente de variação), medidas separatrizes (percentis, decis, quartis), mediana, apresentação tabular de dados, média, moda e variância para dados agrupados em tabelas de frequências, apresentações gráficas para variáveis qualitativas (barras, setores) e quantitativas (histograma, box-plot). Introdução ao planejamento de pesquisas e levantamentos amostrais: tipos de levantamentos amostrais (probabilísticos e não probabilísticos), levantamento amostral aleatório simples e sistematizado, levantamento amostral aleatório estratificado e por conglomerados, dimensionamento de amostras para inferências sobre médias e proporções. Introdução às estatísticas epidemiológicas: conceitos e tipos de estudos (caso-controle e coorte), tabelas de associação, incidência, prevalência, risco atribuível, razão de risco, fração etiológica, odds ratio. Introdução às técnicas de regressão: correlação linear. 14.2.3.5 Bibliografia básica: CALLEGARI-JACQUES, Sidia M. (2003). Bioestatística: princípios e aplicações. Porto Alegre: ArtMed. DÍAZ, F. R. e LÓPEZ, F. J. B. (2007). Bioestatística. São Paulo: Thomson Learning. MORETTIN, Pedro A. e O. BUSSAB, Wilton (2017). Estatística básica. 9ª ed. Rio de Janeiro: Editora Saraiva. 14.2.3.6 Procedimentos de ensino O processo de ensino será composto por um conjunto de atividades presenciais expositivas e práticas com a resolução de exercícios em sala de aula a ter início no dia 27/03/2025, prosseguindo nos dias e horários determinados no Sistema UEL. Todo material utilizado (textos, slides, listas de exercícios) será disponibilizado na sala de aula virtual a ser criada na Plataforma Google Classroom, de adesão compulsória por parte do discente, por meio de convite enviado ao seu email registrado no Sistema UEL. Toda comunicação de natureza pedagógica entre discentes e o docente dar-se-á por meio de postagens na plataforma Google Classroom para que todos os alunos possam ter acesso e se beneficiar do conteúdo. O atendimento presencial e em caráter pessoal, para elucidação de eventuais dúvidas de discentes, deverá ser previamente agendado com o docente. As datas das atividades, avaliações e do exame final, indicadas no cronograma abaixo, poderão ser alteradas tanto em razão de situações não previstas pelo atual calendário acadêmico quanto do bom progresso das atividades didáticas, mediante comunicação aos alunos com razoável antecipação. 14.2.3.7 Formas e critérios de avaliação: Durante o semestre serão realizadas as seguintes atividades avaliativas: - duas (2) provas presenciais escritas (P1 e P2) referentes ao conteúdo das aulas e valendo de zero (0) a dez (10) pontos cada uma e com peso 5 para fins da determinação da média final (MF); - duas (2) atividades no formato de listas de exercícios (L1 e L2) disponibilizadas na plataforma Google Classroom, valendo de zero (0) a dez (10) pontos. As atividades L1 e L2 poderão ser compostas por mais de uma lista sendo, nesse caso, atribuída a média aritmética com peso 1 das notas de todas as listas que compõem cada atividade conforme a expressão: \\[ L_1 = \\frac{L_{1,1} + · · · + L_{1,n_1}}{n_1} \\] e \\[ L_2 = \\frac{L_{2,1} + · · · + L_{2,n_2}}{n_2} \\] A média final (MF) será calculada pela seguinte expressão: \\[ MF = \\frac{(5 × P_1) + (1 × L_1) + (5 × P_2) + (1 × L_2)}{12} \\] 14.2.4 Computação: 2STA030 - Estatística 14.2.4.1 Horários: Turma 1000: sextas-feiras (14 h 00 min - 17 h 35 min) 14.2.4.2 Locais: CCE Turma 1000: sala 303A (CCE) 14.2.4.3 Ementa conforme projeto pedagógico do curso na Resolução CEPE/CA 109/2022: Análise exploratória de dados. Probabilidades. Variáveis Aleatórias Discretas e Contínuas. Estimação de parâmetro. Teste de Hipóteses. Uso de programa estatístico. 14.2.4.4 Conteúdo programático: Estatística descritiva: introdução à pesquisa científica, análise exploratória e descritiva de dados e assuntos correlacionados: noções sobre a produção de conhecimento por meio da pesquisa científica, conceitos de população e parâmetros, amostra e estatísticas, tipos de variáveis, indexação e somatório de dados, conectivos lógicos, conjuntos, diagramas de Venn, operações com conjunto, apresentações gráficas elementares, sínteses numéricas para dados individualizados e agrupados, apresentação tabular de dados, apresentações gráficas. Probabilidade: aspectos históricos, experimentos aleatórios e determinísticos, espaços e eventos, conceitos, probabilidade da adição de eventos (disjuntos e não disjuntos), probabilidade condicional de eventos e independência de eventos, regra de Bayes, simulações usando a linguagem R, variáveis aleatórias discretas e contínuas (função distribuição e de densidade de probabilidade, esperança e variância), modelos teóricos discretos e contínuos de probabilidade. Estatística inferencial: distribuições amostrais, estimativas intervalares e testes de hipóteses. Engenharia de variáveis: visualização, eliminação, imputação, codificação e transformação de dados em variáveis, introdução à programação em R especificamente relacionado aos tópicos acima (Google Colab). 14.2.4.5 Bibliografia básica: DEVORE, Jay L. (2018). Probabilidade e estatística para engenharia e ciências. 3ª ed. Porto Alegre: +A Educação - Cengage Learning Brasil. MORETTIN, Pedro A. e O. BUSSAB, Wilton (2017). Estatística básica. 9ª ed. Rio de Janeiro: Editora Saraiva. ROSS, S. M. (2010). Probabilidade: um Curso Moderno com Aplicações. 8ª ed. São Paulo: Book 14.2.4.6 Procedimentos de ensino O processo de ensino será composto por um conjunto de atividades presenciais teóricas expositivas e resolução de exercícios em sala de aula a partir do dia 28/03/2025, prosseguindo nos dias e horários determinados no Sistema UEL. Todo material utilizado (textos, slides, listas de exercícios) será disponibilizado na sala de aula virtual a ser criada na Plataforma Google Classroom, de adesão compulsória por parte do discente, por meio de convite enviado ao seu email registrado no Sistema UEL. Toda comunicação de natureza pedagógica entre discentes e o docente dar-se-á por meio de postagens na plataforma Google Classroom para que todos os alunos possam ter acesso e se beneficiar do conteúdo. O atendimento presencial e em caráter pessoal, para elucidação de eventuais dúvidas de discentes, deverá ser previamente agendado com o docente. As datas das atividades, avaliações e do exame final, indicadas no cronograma abaixo, poderão ser alteradas tanto em razão de situações não previstas pelo atual calendário acadêmico quanto do bom progresso das atividades didáticas, mediante comunicação aos alunos com razoável antecipação. 14.2.4.7 Formas e critérios de avaliação: Durante o semestre serão realizadas as seguintes atividades avaliativas: - um (1) prova presencial escrita (P) referente ao conteúdo das aulas e valendo de zero (0) a dez (10) pontos e com peso 4 para fins da determinação da média final (M F ); - uma (1) apresentação em sala na forma de seminário (S) por equipes de alunos com temas a serem determinados, referentes ao conteúdo das aulas, valendo de zero (0) a dez (10) pontos e com peso 5 para fins da determinação da média final (MF); - uma (1) atividade no formato de listas de exercícios (L) disponibilizadas na plataforma Google Classroom, referente ao conteúdo das aulas, valendo de zero (0) a dez (10) pontos e com peso 1 para fins da determinação da média final (MF). A atividade L poderá ser composta por mais de uma lista sendo, nesse caso, atribuída a média aritmética com peso 1 das notas de todas as listas que compõem essa atividade: \\[ L = \\frac{L_1 + · · · + L_n}{n} \\] A média final (MF) será calculada pela seguinte expressão que atribui peso 4 para a prova escrita presencial, peso 5 para o seminário e peso 1 para a lista de exercícios: \\[ MF = \\frac{4(P) + 5(S) + 1(L)}{10} \\] 14.2.5 Engenharia Civil: 2STA016 - Estatística e probabilidades (2025/02) 14.2.5.1 Horários: Turma 1000: segundas e sextas-feiras (10 h 15 min - 11 h 55 min) Turma 2000: segundas e sextas-feiras (8 h 20 min - 10 h 00 min)  Turma 3000: segundas e quartas-feiras (19 h 15 min - 20 h 55 min)  14.2.5.2 Locais: CTU Turma 1000: segundas-feiras: sala 1012 e sextas-feiras: sala 1015 Turma 2000: segundas-feiras: sala 1015 e sextas-feiras: sala 1014 Turma 3000: segundas-feiras e quartas-feiras: sala 1015 14.2.5.3 Ementa contida na Resolução Resolução CEPE/CA 073/2022: Técnicas de amostragem. Medidas de posição e dispersão. Introdução à probabilidade. Variáveis aleatórias discretas, contínuas e suas principais distribuições de probabilidade. Inferência sobre médias, variâncias e proporções. Noções de planejamento de experimentos aplicados à Engenharia. 14.2.5.4 Conteúdo programático: Introdução à pesquisa científica, análise exploratória e descritiva de dados e assuntos correlacionados: noções sobre a produção de conhecimento por meio da pesquisa científica, conceitos de população e parâmetros, amostra e estatísticas, tipos de variáveis, indexação e somatório de dados, combinações, conectivos lógicos, conjuntos, diagramas de Venn, operações com conjuntos, coleta de dados das alturas dos alunos da turma para análise exploratória de dados, apresentação dos dados brutos e em rol, apresentações gráficas elementares, sínteses numéricas de posição (média, moda), sínteses numéricas de dispersão (máximo, mínimo, amplitude, variância e coeficiente de variação), medidas separatrizes (percentis, decis, quartis), mediana, apresentação tabular de dados, média, moda e variância para dados agrupados em tabelas de frequências, apresentações gráficas para variáveis qualitativas (barras, setores) e quantitativas (histograma box-plot) Introdução ao cálculo de probabilidades, variáveis aleatórias e distribuições teóricas de probabilidade: conceitos essenciais de experimentos aleatórios e experimentos determinísticos, a variável aleatória, o conjunto de possíveis resultados do experimento aleatório (espaço amostral e seus elementos), eventos simples e compostos (representações com diagramas de Venn), conceitos de probabilidade: (1) clássico (a priori) ; (2) frequentista (a posteriori); (3) conceito axiomático: a probabilidade como uma função, probabilidade da união de eventos, probabilidade de eventos condicionados e independentes, introdução ao teorema de Bayes, funções de distribuição e densidade de probabilidade, modelo teórico discreto de Bernoulli, binomial e de Poisson, modelo teórico Normal. Introdução às distribuições amostrais e testes de hipóteses paramétricos: distribuição das médias amostrais e a construção de intervalos de confiança, distribuição das proporções amostrais e a construção de intervalos de confiança, distribuição das variâncias amostrais e a construção de intervalos de confiança, teste de hipóteses sobre a média de uma população, teste de hipóteses sobre a proporção de uma população, teste de hipóteses sobre a variância de uma população. Introdução ao planejamento de pesquisas e levantamentos amostrais: tipos de levantamentos amostrais (probabilísticos e não probabilísticos), levantamento amostral aleatório simples e sistematizado, levantamento amostral aleatório estratificado e por conglomerados, dimensionamento de amostras para inferências sobre médias e proporções.\\ 14.2.5.5 Bibliografia básica: BARBETTA, Pedro A., REIS, Marcelo M. e BORNIA, Antonio C. (2024). Estatística para Cursos de Engenharia, Computação e Ciência de Dados. 4ª ed. Rio de Janeiro: LTC. DEVORE, Jay L. (2018). Probabilidade e estatística para engenharia e ciências. 3ª ed. Porto Alegre: +A Educação - Cengage Learning Brasil. MORETTIN, Pedro A. e O. BUSSAB, Wilton (2017). Estatística básica. 9ª ed. Rio de Janeiro: Editora Saraiva. ROSS, S. M. (2010). Probabilidade: um Curso Moderno com Aplicações. 8ª ed. São Paulo: Bookman. 14.2.5.6 Procedimentos de ensino O processo de ensino será composto por um conjunto de atividades presenciais teóricas expositivas e resolução de exercícios em sala de aula a partir do dia 11/08/2025, nos dias e horários determinados no Sistema UEL. Todo material utilizado (textos, slides, listas de exercícios) será disponibilizado na sala de aula virtual a ser criada na Plataforma Google Classroom, de adesão compulsória por parte do discente, por meio de convite enviado ao seu email registrado no Sistema UEL. Toda comunicação entre discentes e o docente dar-se-á por meio de postagens na plataforma Google Classroom. As datas das duas provas escritas presenciais e do exame final indicadas no cronograma a seguir poderão ser alteradas tanto em razão de situações não previstas no atual calendário acadêmico quanto do bom progresso das atividades didáticas, mediante comunicação aos alunos com devida antecipação. 14.2.5.7 Formas e critérios de avaliação: Durante o semestre serão realizadas as seguintes atividades avaliativas: três (3) (P1, P2, P3) referentes ao conteúdo das aulas e valendo de zero (0) a dez (10) pontos cada uma; uma (1) (A) no formato de listas de exercícios (\\(L_1, L_2, \\cdots, L_n\\)), disponibilizadas na plataforma , valendo de zero (0) a dez (10) pontos cada uma, sendo seu valor final atribuído pela seguinte expressão (média aritmética simples): \\[ A=\\frac{ (L_1+L_2+\\dots+L_n)}{n} \\] A (\\(MF\\)) será calculada pela seguinte expressão (média aritmética simples): \\[ MF= \\frac{1*P_1 + 1*P_2 + 1*P_3 + 1*A}{4} \\] Ao final da disciplina haverá exame final conforme estabelecido no Regimento da UEL (Art. 59). 14.2.5.8 Bibliografia complementar: MAGALHÃES, M. N. e LIMA, C. Pedroso de (2015). Noções de Probabilidade e Estatística. 7ª ed. São Paulo: Edusp. MEYER, Paul L. (2010). Probabilidade: Aplicações à Estatística. 2ª ed. Rio de Janeiro: LTC. MONTGOMERY, Douglas C. e RUNGER, George C. (2021). Estatística Aplicada e Probabilidade para Engenheiros. 7ª ed. Rio de Janeiro: LTC. MORETTIN, L. G. (2010). Estatística básica: probabilidade e inferência. São Paulo: Pearson Prentice Hall. WALPOLE, R. E. (2009). Probabilidade e Estatística para Engenharia e Ciências. São Paulo: Pearson Prentice Hall. 14.2.6 Ciência de dados e Inteligência Artifical: 2STA011 - Probabilidade 14.2.6.1 Horários: segundas e quartas-feiras (21 h 10 min - 22 h 50 min) 14.2.6.2 Locais: CCE Sala 03 e XX 14.2.6.3 Ementa contida na Resolução CEPE/CA 060/2022: Probabilidade e propriedades. Probabilidade condicional e independência. Variáveis aleatórias discretas e principais modelos de distribuição discretas. Variáveis aleatórias contínuas e principais modelos de distribuições contínuas. Processo de Poisson. Cadeias de Markov. Simulação de Monte Carlo. Uso de programa estatístico. 14.2.6.4 Conteúdo programático: Módulo 1: Probabilidade -Introdução à Probabilidade: aspectos históricos, experimentos aleatórios e determinísticos, espaços e eventos, conceitos. -Probabilidade da adição de eventos (disjuntos e não disjuntos). -Probabilidade Condicional de eventos e independência de eventos. -Teorema de Bayes. -Simulações usando a linguagem R. -Variáveis aleatórias discretas e contínuas (função distribuição e de densidade de probabilidade, esperança e variância). -Principais modelos teóricos discretos e contínuos de probabilidade. Módulo 2: Processos estocásticos -Aplicações do processo de Poisson, de simulações de Monte Carlo e cadeias de Markov. 14.2.6.5 Bibliografia básica: ROSS, Sheldon. Probabilidade: um curso moderno com aplicações. 8 ed. Porto Alegre: Bookman, 2010. 606 p. MEYER, Paul L. Probabilidade: Aplicações à Estatística. 2 ed. Rio de Janeiro: LTC, 2010. 426 p. 14.2.6.6 Procedimentos de ensino O processo de ensino será composto por um conjunto de atividades presenciais teóricas expositivas e resolução de exercícios em sala de aula a partir do dia 14/10/2024, nos dias e horários determinados no Sistema UEL. Todo material utilizado (textos, slides, listas de exercícios) será disponibilizado na sala de aula virtual a ser criada na Plataforma Google Classroom, de adesão compulsória por parte do discente, por meio de convite enviado ao seu email registrado no Sistema UEL. Toda comunicação entre discentes e o docente se dará por meio de postagens na plataforma Google Classroom. As datas das prova escrita presencial, do seminário e do exame final indicadas no cronograma a seguir poderão ser alteradas tanto em razão de situações não previstas no atual calendário acadêmico quanto do bom progresso das atividades didáticas, mediante comunicação aos alunos com devida antecipação. 14.2.6.7 Formas e critérios de avaliação: Durante o semestre serão realizadas as seguintes atividades avaliativas: - uma (1) prova escrita presencial (P) referente ao conteúdo das aulas do módulo 1 valendo de zero (0) a dez (10) pontos; - uma (1) apresentação em sala na forma de seminário (S) com tema a ser determinado e referente ao conteúdo das aulas do módulo 2, valendo de zero (0) a dez (10) pontos; - uma (1) atividade no formato de listas de exercícios (L) disponibilizadas na plataforma Google Classroom, referente ao conteúdo das aulas do módulo 1 e valendo de zero (0) a dez (10) pontos. A atividade L poderá ser composta por mais de uma lista de exercícios sendo, nesse caso, atribuída a média aritmética das notas de todas as listas que compõem essa atividade: \\[ L=\\frac{ (L_{1,1}+\\dots+L_{1,n})}{n} \\] A média final (\\(MF\\)) será calculada pela seguinte expressão que atribui peso 4 para a prova escrita presencial, peso 5 para o seminário e peso 1 para a lista de exercícios: \\[ MF= \\frac{4(P) + 5(S) + 1(L)}{10} \\] Ao final da disciplina haverá exame final conforme estabelecido no Regimento da UEL (Art. 59) . "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
